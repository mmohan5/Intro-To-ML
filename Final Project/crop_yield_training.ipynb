{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"crop_yield.csv\")\n",
    "\n",
    "# Remove rows with Cotton and Soybean crops and the Region column\n",
    "data = data[~data['Crop'].isin(['Cotton', 'Soybean'])]\n",
    "data = data.drop(columns=['Region'])\n",
    "\n",
    "# Save the new dataset\n",
    "data.to_csv(\"final_yield_set.csv\", index=False)\n",
    "\n",
    "# Split features and targets\n",
    "X = data.drop(columns=['Days_to_Harvest', 'Yield_tons_per_hectare'])\n",
    "y = data[['Days_to_Harvest', 'Yield_tons_per_hectare']]\n",
    "\n",
    "# Encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "categorical_features = ['Soil_Type', 'Crop', 'Weather_Condition']\n",
    "encoded_categorical = encoder.fit_transform(X[categorical_features])\n",
    "\n",
    "# Convert boolean features to integers\n",
    "X['Fertilizer_Used'] = X['Fertilizer_Used'].astype(int)\n",
    "X['Irrigation_Used'] = X['Irrigation_Used'].astype(int)\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['Rainfall_mm', 'Temperature_Celsius', 'Fertilizer_Used', 'Irrigation_Used']\n",
    "normalized_numerical = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "# Combine processed features\n",
    "X_processed = np.hstack([normalized_numerical, encoded_categorical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Neural Network (FCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/700, Training Loss: 5803.6196, Validation Loss: 5779.6406\n",
      "Epoch 2/700, Training Loss: 5784.7852, Validation Loss: 5738.1553\n",
      "Epoch 3/700, Training Loss: 5743.2715, Validation Loss: 5617.7153\n",
      "Epoch 4/700, Training Loss: 5622.7500, Validation Loss: 5319.5039\n",
      "Epoch 5/700, Training Loss: 5324.3438, Validation Loss: 4674.1665\n",
      "Epoch 6/700, Training Loss: 4678.6045, Validation Loss: 3478.9539\n",
      "Epoch 7/700, Training Loss: 3482.5815, Validation Loss: 1713.7843\n",
      "Epoch 8/700, Training Loss: 1715.8907, Validation Loss: 618.3808\n",
      "Epoch 9/700, Training Loss: 617.6699, Validation Loss: 2928.6741\n",
      "Epoch 10/700, Training Loss: 2924.8274, Validation Loss: 1109.8668\n",
      "Epoch 11/700, Training Loss: 1107.3033, Validation Loss: 394.4665\n",
      "Epoch 12/700, Training Loss: 393.5162, Validation Loss: 780.6130\n",
      "Epoch 13/700, Training Loss: 780.8320, Validation Loss: 1271.7244\n",
      "Epoch 14/700, Training Loss: 1272.5891, Validation Loss: 1535.5160\n",
      "Epoch 15/700, Training Loss: 1536.6130, Validation Loss: 1542.1158\n",
      "Epoch 16/700, Training Loss: 1543.1135, Validation Loss: 1329.2314\n",
      "Epoch 17/700, Training Loss: 1329.8381, Validation Loss: 969.4608\n",
      "Epoch 18/700, Training Loss: 969.3926, Validation Loss: 605.6512\n",
      "Epoch 19/700, Training Loss: 604.6494, Validation Loss: 473.2060\n",
      "Epoch 20/700, Training Loss: 471.1088, Validation Loss: 698.7149\n",
      "Epoch 21/700, Training Loss: 695.6296, Validation Loss: 880.2460\n",
      "Epoch 22/700, Training Loss: 876.7341, Validation Loss: 685.3676\n",
      "Epoch 23/700, Training Loss: 682.2596, Validation Loss: 436.0368\n",
      "Epoch 24/700, Training Loss: 433.9660, Validation Loss: 407.5355\n",
      "Epoch 25/700, Training Loss: 406.6412, Validation Loss: 523.5081\n",
      "Epoch 26/700, Training Loss: 523.5499, Validation Loss: 622.8406\n",
      "Epoch 27/700, Training Loss: 623.4518, Validation Loss: 629.9163\n",
      "Epoch 28/700, Training Loss: 630.7462, Validation Loss: 549.4282\n",
      "Epoch 29/700, Training Loss: 550.1591, Validation Loss: 434.0857\n",
      "Epoch 30/700, Training Loss: 434.4116, Validation Loss: 365.5366\n",
      "Epoch 31/700, Training Loss: 365.1927, Validation Loss: 396.5818\n",
      "Epoch 32/700, Training Loss: 395.4396, Validation Loss: 479.0735\n",
      "Epoch 33/700, Training Loss: 477.2953, Validation Loss: 494.0959\n",
      "Epoch 34/700, Training Loss: 492.1453, Validation Loss: 423.5509\n",
      "Epoch 35/700, Training Loss: 421.9109, Validation Loss: 361.8423\n",
      "Epoch 36/700, Training Loss: 360.7358, Validation Loss: 363.3923\n",
      "Epoch 37/700, Training Loss: 362.7666, Validation Loss: 400.0947\n",
      "Epoch 38/700, Training Loss: 399.7527, Validation Loss: 423.2930\n",
      "Epoch 39/700, Training Loss: 422.9996, Validation Loss: 410.9979\n",
      "Epoch 40/700, Training Loss: 410.5295, Validation Loss: 376.1616\n",
      "Epoch 41/700, Training Loss: 375.3318, Validation Loss: 352.0821\n",
      "Epoch 42/700, Training Loss: 350.7743, Validation Loss: 362.0511\n",
      "Epoch 43/700, Training Loss: 360.2672, Validation Loss: 390.3169\n",
      "Epoch 44/700, Training Loss: 388.2150, Validation Loss: 396.9925\n",
      "Epoch 45/700, Training Loss: 394.8535, Validation Loss: 373.0434\n",
      "Epoch 46/700, Training Loss: 371.1510, Validation Loss: 349.2335\n",
      "Epoch 47/700, Training Loss: 347.7531, Validation Loss: 348.1635\n",
      "Epoch 48/700, Training Loss: 347.1060, Validation Loss: 361.4161\n",
      "Epoch 49/700, Training Loss: 360.6764, Validation Loss: 369.1364\n",
      "Epoch 50/700, Training Loss: 368.5511, Validation Loss: 362.7108\n",
      "Epoch 51/700, Training Loss: 362.1047, Validation Loss: 350.0217\n",
      "Epoch 52/700, Training Loss: 349.2442, Validation Loss: 345.1221\n",
      "Epoch 53/700, Training Loss: 344.0843, Validation Loss: 352.0493\n",
      "Epoch 54/700, Training Loss: 350.7590, Validation Loss: 359.5471\n",
      "Epoch 55/700, Training Loss: 358.1161, Validation Loss: 356.4973\n",
      "Epoch 56/700, Training Loss: 355.0912, Validation Loss: 347.2771\n",
      "Epoch 57/700, Training Loss: 346.0301, Validation Loss: 343.3606\n",
      "Epoch 58/700, Training Loss: 342.3178, Validation Loss: 347.1405\n",
      "Epoch 59/700, Training Loss: 346.2580, Validation Loss: 351.5075\n",
      "Epoch 60/700, Training Loss: 350.6876, Validation Loss: 350.3671\n",
      "Epoch 61/700, Training Loss: 349.4987, Validation Loss: 345.1536\n",
      "Epoch 62/700, Training Loss: 344.1458, Validation Loss: 342.0139\n",
      "Epoch 63/700, Training Loss: 340.8217, Validation Loss: 343.9794\n",
      "Epoch 64/700, Training Loss: 342.6196, Validation Loss: 347.2852\n",
      "Epoch 65/700, Training Loss: 345.8326, Validation Loss: 346.9716\n",
      "Epoch 66/700, Training Loss: 345.5299, Validation Loss: 343.8341\n",
      "Epoch 67/700, Training Loss: 342.4922, Validation Loss: 342.2683\n",
      "Epoch 68/700, Training Loss: 341.0674, Validation Loss: 343.5674\n",
      "Epoch 69/700, Training Loss: 342.4935, Validation Loss: 344.9743\n",
      "Epoch 70/700, Training Loss: 343.9727, Validation Loss: 344.1261\n",
      "Epoch 71/700, Training Loss: 343.1267, Validation Loss: 342.0089\n",
      "Epoch 72/700, Training Loss: 340.9520, Validation Loss: 341.2347\n",
      "Epoch 73/700, Training Loss: 340.0924, Validation Loss: 342.3918\n",
      "Epoch 74/700, Training Loss: 341.1785, Validation Loss: 343.3953\n",
      "Epoch 75/700, Training Loss: 342.1616, Validation Loss: 342.6903\n",
      "Epoch 76/700, Training Loss: 341.4981, Validation Loss: 341.3597\n",
      "Epoch 77/700, Training Loss: 340.2530, Validation Loss: 341.1112\n",
      "Epoch 78/700, Training Loss: 340.0980, Validation Loss: 341.8021\n",
      "Epoch 79/700, Training Loss: 340.8548, Validation Loss: 342.0710\n",
      "Epoch 80/700, Training Loss: 341.1417, Validation Loss: 341.4533\n",
      "Epoch 81/700, Training Loss: 340.4930, Validation Loss: 340.8465\n",
      "Epoch 82/700, Training Loss: 339.8229, Validation Loss: 341.0220\n",
      "Epoch 83/700, Training Loss: 339.9307, Validation Loss: 341.5035\n",
      "Epoch 84/700, Training Loss: 340.3677, Validation Loss: 341.4120\n",
      "Epoch 85/700, Training Loss: 340.2709, Validation Loss: 340.8337\n",
      "Epoch 86/700, Training Loss: 339.7231, Validation Loss: 340.5499\n",
      "Epoch 87/700, Training Loss: 339.4869, Validation Loss: 340.7822\n",
      "Epoch 88/700, Training Loss: 339.7599, Validation Loss: 340.9951\n",
      "Epoch 89/700, Training Loss: 339.9889, Validation Loss: 340.8140\n",
      "Epoch 90/700, Training Loss: 339.7943, Validation Loss: 340.5239\n",
      "Epoch 91/700, Training Loss: 339.4697, Validation Loss: 340.5255\n",
      "Epoch 92/700, Training Loss: 339.4332, Validation Loss: 340.7066\n",
      "Epoch 93/700, Training Loss: 339.5909, Validation Loss: 340.6838\n",
      "Epoch 94/700, Training Loss: 339.5702, Validation Loss: 340.4358\n",
      "Epoch 95/700, Training Loss: 339.3479, Validation Loss: 340.2818\n",
      "Epoch 96/700, Training Loss: 339.2307, Validation Loss: 340.3400\n",
      "Epoch 97/700, Training Loss: 339.3201, Validation Loss: 340.3960\n",
      "Epoch 98/700, Training Loss: 339.3893, Validation Loss: 340.2991\n",
      "Epoch 99/700, Training Loss: 339.2840, Validation Loss: 340.1827\n",
      "Epoch 100/700, Training Loss: 339.1438, Validation Loss: 340.2050\n",
      "Epoch 101/700, Training Loss: 339.1393, Validation Loss: 340.2856\n",
      "Epoch 102/700, Training Loss: 339.2027, Validation Loss: 340.2566\n",
      "Epoch 103/700, Training Loss: 339.1728, Validation Loss: 340.1355\n",
      "Epoch 104/700, Training Loss: 339.0653, Validation Loss: 340.0668\n",
      "Epoch 105/700, Training Loss: 339.0157, Validation Loss: 340.0796\n",
      "Epoch 106/700, Training Loss: 339.0421, Validation Loss: 340.0774\n",
      "Epoch 107/700, Training Loss: 339.0407, Validation Loss: 340.0246\n",
      "Epoch 108/700, Training Loss: 338.9756, Validation Loss: 339.9933\n",
      "Epoch 109/700, Training Loss: 338.9245, Validation Loss: 340.0199\n",
      "Epoch 110/700, Training Loss: 338.9330, Validation Loss: 340.0383\n",
      "Epoch 111/700, Training Loss: 338.9425, Validation Loss: 339.9944\n",
      "Epoch 112/700, Training Loss: 338.9018, Validation Loss: 339.9303\n",
      "Epoch 113/700, Training Loss: 338.8495, Validation Loss: 339.9032\n",
      "Epoch 114/700, Training Loss: 338.8361, Validation Loss: 339.8986\n",
      "Epoch 115/700, Training Loss: 338.8401, Validation Loss: 339.8759\n",
      "Epoch 116/700, Training Loss: 338.8177, Validation Loss: 339.8441\n",
      "Epoch 117/700, Training Loss: 338.7791, Validation Loss: 339.8348\n",
      "Epoch 118/700, Training Loss: 338.7608, Validation Loss: 339.8391\n",
      "Epoch 119/700, Training Loss: 338.7597, Validation Loss: 339.8224\n",
      "Epoch 120/700, Training Loss: 338.7446, Validation Loss: 339.7845\n",
      "Epoch 121/700, Training Loss: 338.7144, Validation Loss: 339.7554\n",
      "Epoch 122/700, Training Loss: 338.6950, Validation Loss: 339.7437\n",
      "Epoch 123/700, Training Loss: 338.6900, Validation Loss: 339.7315\n",
      "Epoch 124/700, Training Loss: 338.6783, Validation Loss: 339.7132\n",
      "Epoch 125/700, Training Loss: 338.6541, Validation Loss: 339.7027\n",
      "Epoch 126/700, Training Loss: 338.6344, Validation Loss: 339.7023\n",
      "Epoch 127/700, Training Loss: 338.6261, Validation Loss: 339.6960\n",
      "Epoch 128/700, Training Loss: 338.6162, Validation Loss: 339.6763\n",
      "Epoch 129/700, Training Loss: 338.5981, Validation Loss: 339.6551\n",
      "Epoch 130/700, Training Loss: 338.5817, Validation Loss: 339.6413\n",
      "Epoch 131/700, Training Loss: 338.5726, Validation Loss: 339.6298\n",
      "Epoch 132/700, Training Loss: 338.5628, Validation Loss: 339.6161\n",
      "Epoch 133/700, Training Loss: 338.5471, Validation Loss: 339.6055\n",
      "Epoch 134/700, Training Loss: 338.5321, Validation Loss: 339.5998\n",
      "Epoch 135/700, Training Loss: 338.5226, Validation Loss: 339.5918\n",
      "Epoch 136/700, Training Loss: 338.5134, Validation Loss: 339.5768\n",
      "Epoch 137/700, Training Loss: 338.5003, Validation Loss: 339.5600\n",
      "Epoch 138/700, Training Loss: 338.4874, Validation Loss: 339.5473\n",
      "Epoch 139/700, Training Loss: 338.4781, Validation Loss: 339.5374\n",
      "Epoch 140/700, Training Loss: 338.4693, Validation Loss: 339.5281\n",
      "Epoch 141/700, Training Loss: 338.4580, Validation Loss: 339.5209\n",
      "Epoch 142/700, Training Loss: 338.4468, Validation Loss: 339.5160\n",
      "Epoch 143/700, Training Loss: 338.4380, Validation Loss: 339.5097\n",
      "Epoch 144/700, Training Loss: 338.4294, Validation Loss: 339.4994\n",
      "Epoch 145/700, Training Loss: 338.4192, Validation Loss: 339.4881\n",
      "Epoch 146/700, Training Loss: 338.4092, Validation Loss: 339.4786\n",
      "Epoch 147/700, Training Loss: 338.4011, Validation Loss: 339.4709\n",
      "Epoch 148/700, Training Loss: 338.3932, Validation Loss: 339.4637\n",
      "Epoch 149/700, Training Loss: 338.3842, Validation Loss: 339.4578\n",
      "Epoch 150/700, Training Loss: 338.3755, Validation Loss: 339.4526\n",
      "Epoch 151/700, Training Loss: 338.3678, Validation Loss: 339.4460\n",
      "Epoch 152/700, Training Loss: 338.3603, Validation Loss: 339.4372\n",
      "Epoch 153/700, Training Loss: 338.3521, Validation Loss: 339.4278\n",
      "Epoch 154/700, Training Loss: 338.3443, Validation Loss: 339.4194\n",
      "Epoch 155/700, Training Loss: 338.3373, Validation Loss: 339.4121\n",
      "Epoch 156/700, Training Loss: 338.3303, Validation Loss: 339.4056\n",
      "Epoch 157/700, Training Loss: 338.3229, Validation Loss: 339.4002\n",
      "Epoch 158/700, Training Loss: 338.3159, Validation Loss: 339.3951\n",
      "Epoch 159/700, Training Loss: 338.3094, Validation Loss: 339.3891\n",
      "Epoch 160/700, Training Loss: 338.3029, Validation Loss: 339.3820\n",
      "Epoch 161/700, Training Loss: 338.2963, Validation Loss: 339.3750\n",
      "Epoch 162/700, Training Loss: 338.2899, Validation Loss: 339.3687\n",
      "Epoch 163/700, Training Loss: 338.2839, Validation Loss: 339.3632\n",
      "Epoch 164/700, Training Loss: 338.2779, Validation Loss: 339.3583\n",
      "Epoch 165/700, Training Loss: 338.2718, Validation Loss: 339.3540\n",
      "Epoch 166/700, Training Loss: 338.2661, Validation Loss: 339.3493\n",
      "Epoch 167/700, Training Loss: 338.2605, Validation Loss: 339.3438\n",
      "Epoch 168/700, Training Loss: 338.2549, Validation Loss: 339.3377\n",
      "Epoch 169/700, Training Loss: 338.2495, Validation Loss: 339.3318\n",
      "Epoch 170/700, Training Loss: 338.2442, Validation Loss: 339.3263\n",
      "Epoch 171/700, Training Loss: 338.2390, Validation Loss: 339.3215\n",
      "Epoch 172/700, Training Loss: 338.2339, Validation Loss: 339.3172\n",
      "Epoch 173/700, Training Loss: 338.2288, Validation Loss: 339.3130\n",
      "Epoch 174/700, Training Loss: 338.2239, Validation Loss: 339.3086\n",
      "Epoch 175/700, Training Loss: 338.2192, Validation Loss: 339.3036\n",
      "Epoch 176/700, Training Loss: 338.2144, Validation Loss: 339.2986\n",
      "Epoch 177/700, Training Loss: 338.2098, Validation Loss: 339.2939\n",
      "Epoch 178/700, Training Loss: 338.2054, Validation Loss: 339.2897\n",
      "Epoch 179/700, Training Loss: 338.2009, Validation Loss: 339.2858\n",
      "Epoch 180/700, Training Loss: 338.1965, Validation Loss: 339.2822\n",
      "Epoch 181/700, Training Loss: 338.1923, Validation Loss: 339.2785\n",
      "Epoch 182/700, Training Loss: 338.1881, Validation Loss: 339.2745\n",
      "Epoch 183/700, Training Loss: 338.1840, Validation Loss: 339.2702\n",
      "Epoch 184/700, Training Loss: 338.1799, Validation Loss: 339.2661\n",
      "Epoch 185/700, Training Loss: 338.1760, Validation Loss: 339.2622\n",
      "Epoch 186/700, Training Loss: 338.1721, Validation Loss: 339.2587\n",
      "Epoch 187/700, Training Loss: 338.1682, Validation Loss: 339.2553\n",
      "Epoch 188/700, Training Loss: 338.1645, Validation Loss: 339.2520\n",
      "Epoch 189/700, Training Loss: 338.1608, Validation Loss: 339.2485\n",
      "Epoch 190/700, Training Loss: 338.1572, Validation Loss: 339.2447\n",
      "Epoch 191/700, Training Loss: 338.1536, Validation Loss: 339.2410\n",
      "Epoch 192/700, Training Loss: 338.1501, Validation Loss: 339.2374\n",
      "Epoch 193/700, Training Loss: 338.1467, Validation Loss: 339.2341\n",
      "Epoch 194/700, Training Loss: 338.1433, Validation Loss: 339.2310\n",
      "Epoch 195/700, Training Loss: 338.1400, Validation Loss: 339.2279\n",
      "Epoch 196/700, Training Loss: 338.1367, Validation Loss: 339.2249\n",
      "Epoch 197/700, Training Loss: 338.1335, Validation Loss: 339.2216\n",
      "Epoch 198/700, Training Loss: 338.1303, Validation Loss: 339.2183\n",
      "Epoch 199/700, Training Loss: 338.1272, Validation Loss: 339.2151\n",
      "Epoch 200/700, Training Loss: 338.1241, Validation Loss: 339.2122\n",
      "Epoch 201/700, Training Loss: 338.1211, Validation Loss: 339.2093\n",
      "Epoch 202/700, Training Loss: 338.1181, Validation Loss: 339.2065\n",
      "Epoch 203/700, Training Loss: 338.1152, Validation Loss: 339.2037\n",
      "Epoch 204/700, Training Loss: 338.1124, Validation Loss: 339.2008\n",
      "Epoch 205/700, Training Loss: 338.1095, Validation Loss: 339.1978\n",
      "Epoch 206/700, Training Loss: 338.1067, Validation Loss: 339.1949\n",
      "Epoch 207/700, Training Loss: 338.1040, Validation Loss: 339.1920\n",
      "Epoch 208/700, Training Loss: 338.1013, Validation Loss: 339.1894\n",
      "Epoch 209/700, Training Loss: 338.0986, Validation Loss: 339.1868\n",
      "Epoch 210/700, Training Loss: 338.0959, Validation Loss: 339.1842\n",
      "Epoch 211/700, Training Loss: 338.0934, Validation Loss: 339.1815\n",
      "Epoch 212/700, Training Loss: 338.0908, Validation Loss: 339.1789\n",
      "Epoch 213/700, Training Loss: 338.0883, Validation Loss: 339.1762\n",
      "Epoch 214/700, Training Loss: 338.0858, Validation Loss: 339.1736\n",
      "Epoch 215/700, Training Loss: 338.0833, Validation Loss: 339.1712\n",
      "Epoch 216/700, Training Loss: 338.0809, Validation Loss: 339.1688\n",
      "Epoch 217/700, Training Loss: 338.0786, Validation Loss: 339.1664\n",
      "Epoch 218/700, Training Loss: 338.0762, Validation Loss: 339.1640\n",
      "Epoch 219/700, Training Loss: 338.0739, Validation Loss: 339.1617\n",
      "Epoch 220/700, Training Loss: 338.0716, Validation Loss: 339.1592\n",
      "Epoch 221/700, Training Loss: 338.0694, Validation Loss: 339.1569\n",
      "Epoch 222/700, Training Loss: 338.0671, Validation Loss: 339.1546\n",
      "Epoch 223/700, Training Loss: 338.0649, Validation Loss: 339.1523\n",
      "Epoch 224/700, Training Loss: 338.0628, Validation Loss: 339.1501\n",
      "Epoch 225/700, Training Loss: 338.0606, Validation Loss: 339.1479\n",
      "Epoch 226/700, Training Loss: 338.0585, Validation Loss: 339.1457\n",
      "Epoch 227/700, Training Loss: 338.0564, Validation Loss: 339.1435\n",
      "Epoch 228/700, Training Loss: 338.0544, Validation Loss: 339.1413\n",
      "Epoch 229/700, Training Loss: 338.0523, Validation Loss: 339.1392\n",
      "Epoch 230/700, Training Loss: 338.0504, Validation Loss: 339.1372\n",
      "Epoch 231/700, Training Loss: 338.0484, Validation Loss: 339.1351\n",
      "Epoch 232/700, Training Loss: 338.0464, Validation Loss: 339.1331\n",
      "Epoch 233/700, Training Loss: 338.0445, Validation Loss: 339.1310\n",
      "Epoch 234/700, Training Loss: 338.0426, Validation Loss: 339.1290\n",
      "Epoch 235/700, Training Loss: 338.0407, Validation Loss: 339.1270\n",
      "Epoch 236/700, Training Loss: 338.0389, Validation Loss: 339.1251\n",
      "Epoch 237/700, Training Loss: 338.0370, Validation Loss: 339.1232\n",
      "Epoch 238/700, Training Loss: 338.0352, Validation Loss: 339.1213\n",
      "Epoch 239/700, Training Loss: 338.0334, Validation Loss: 339.1194\n",
      "Epoch 240/700, Training Loss: 338.0317, Validation Loss: 339.1175\n",
      "Epoch 241/700, Training Loss: 338.0299, Validation Loss: 339.1157\n",
      "Epoch 242/700, Training Loss: 338.0282, Validation Loss: 339.1139\n",
      "Epoch 243/700, Training Loss: 338.0265, Validation Loss: 339.1121\n",
      "Epoch 244/700, Training Loss: 338.0248, Validation Loss: 339.1104\n",
      "Epoch 245/700, Training Loss: 338.0232, Validation Loss: 339.1086\n",
      "Epoch 246/700, Training Loss: 338.0216, Validation Loss: 339.1069\n",
      "Epoch 247/700, Training Loss: 338.0200, Validation Loss: 339.1051\n",
      "Epoch 248/700, Training Loss: 338.0183, Validation Loss: 339.1034\n",
      "Epoch 249/700, Training Loss: 338.0168, Validation Loss: 339.1017\n",
      "Epoch 250/700, Training Loss: 338.0152, Validation Loss: 339.1001\n",
      "Epoch 251/700, Training Loss: 338.0137, Validation Loss: 339.0985\n",
      "Epoch 252/700, Training Loss: 338.0121, Validation Loss: 339.0969\n",
      "Epoch 253/700, Training Loss: 338.0106, Validation Loss: 339.0952\n",
      "Epoch 254/700, Training Loss: 338.0091, Validation Loss: 339.0936\n",
      "Epoch 255/700, Training Loss: 338.0077, Validation Loss: 339.0920\n",
      "Epoch 256/700, Training Loss: 338.0062, Validation Loss: 339.0905\n",
      "Epoch 257/700, Training Loss: 338.0048, Validation Loss: 339.0890\n",
      "Epoch 258/700, Training Loss: 338.0034, Validation Loss: 339.0875\n",
      "Epoch 259/700, Training Loss: 338.0019, Validation Loss: 339.0860\n",
      "Epoch 260/700, Training Loss: 338.0005, Validation Loss: 339.0845\n",
      "Epoch 261/700, Training Loss: 337.9992, Validation Loss: 339.0830\n",
      "Epoch 262/700, Training Loss: 337.9978, Validation Loss: 339.0815\n",
      "Epoch 263/700, Training Loss: 337.9965, Validation Loss: 339.0801\n",
      "Epoch 264/700, Training Loss: 337.9951, Validation Loss: 339.0787\n",
      "Epoch 265/700, Training Loss: 337.9938, Validation Loss: 339.0773\n",
      "Epoch 266/700, Training Loss: 337.9926, Validation Loss: 339.0759\n",
      "Epoch 267/700, Training Loss: 337.9912, Validation Loss: 339.0745\n",
      "Epoch 268/700, Training Loss: 337.9900, Validation Loss: 339.0731\n",
      "Epoch 269/700, Training Loss: 337.9887, Validation Loss: 339.0717\n",
      "Epoch 270/700, Training Loss: 337.9875, Validation Loss: 339.0704\n",
      "Epoch 271/700, Training Loss: 337.9863, Validation Loss: 339.0692\n",
      "Epoch 272/700, Training Loss: 337.9850, Validation Loss: 339.0679\n",
      "Epoch 273/700, Training Loss: 337.9838, Validation Loss: 339.0665\n",
      "Epoch 274/700, Training Loss: 337.9826, Validation Loss: 339.0653\n",
      "Epoch 275/700, Training Loss: 337.9814, Validation Loss: 339.0640\n",
      "Epoch 276/700, Training Loss: 337.9803, Validation Loss: 339.0627\n",
      "Epoch 277/700, Training Loss: 337.9791, Validation Loss: 339.0615\n",
      "Epoch 278/700, Training Loss: 337.9780, Validation Loss: 339.0602\n",
      "Epoch 279/700, Training Loss: 337.9768, Validation Loss: 339.0591\n",
      "Epoch 280/700, Training Loss: 337.9757, Validation Loss: 339.0578\n",
      "Epoch 281/700, Training Loss: 337.9746, Validation Loss: 339.0566\n",
      "Epoch 282/700, Training Loss: 337.9735, Validation Loss: 339.0555\n",
      "Epoch 283/700, Training Loss: 337.9724, Validation Loss: 339.0543\n",
      "Epoch 284/700, Training Loss: 337.9713, Validation Loss: 339.0532\n",
      "Epoch 285/700, Training Loss: 337.9702, Validation Loss: 339.0520\n",
      "Epoch 286/700, Training Loss: 337.9692, Validation Loss: 339.0509\n",
      "Epoch 287/700, Training Loss: 337.9681, Validation Loss: 339.0498\n",
      "Epoch 288/700, Training Loss: 337.9671, Validation Loss: 339.0487\n",
      "Epoch 289/700, Training Loss: 337.9661, Validation Loss: 339.0476\n",
      "Epoch 290/700, Training Loss: 337.9651, Validation Loss: 339.0465\n",
      "Epoch 291/700, Training Loss: 337.9641, Validation Loss: 339.0454\n",
      "Epoch 292/700, Training Loss: 337.9630, Validation Loss: 339.0443\n",
      "Epoch 293/700, Training Loss: 337.9621, Validation Loss: 339.0433\n",
      "Epoch 294/700, Training Loss: 337.9611, Validation Loss: 339.0422\n",
      "Epoch 295/700, Training Loss: 337.9601, Validation Loss: 339.0412\n",
      "Epoch 296/700, Training Loss: 337.9591, Validation Loss: 339.0402\n",
      "Epoch 297/700, Training Loss: 337.9582, Validation Loss: 339.0392\n",
      "Epoch 298/700, Training Loss: 337.9573, Validation Loss: 339.0382\n",
      "Epoch 299/700, Training Loss: 337.9563, Validation Loss: 339.0372\n",
      "Epoch 300/700, Training Loss: 337.9554, Validation Loss: 339.0362\n",
      "Epoch 301/700, Training Loss: 337.9545, Validation Loss: 339.0353\n",
      "Epoch 302/700, Training Loss: 337.9536, Validation Loss: 339.0343\n",
      "Epoch 303/700, Training Loss: 337.9527, Validation Loss: 339.0334\n",
      "Epoch 304/700, Training Loss: 337.9518, Validation Loss: 339.0324\n",
      "Epoch 305/700, Training Loss: 337.9509, Validation Loss: 339.0315\n",
      "Epoch 306/700, Training Loss: 337.9500, Validation Loss: 339.0305\n",
      "Epoch 307/700, Training Loss: 337.9491, Validation Loss: 339.0296\n",
      "Epoch 308/700, Training Loss: 337.9482, Validation Loss: 339.0287\n",
      "Epoch 309/700, Training Loss: 337.9474, Validation Loss: 339.0278\n",
      "Epoch 310/700, Training Loss: 337.9465, Validation Loss: 339.0269\n",
      "Epoch 311/700, Training Loss: 337.9456, Validation Loss: 339.0260\n",
      "Epoch 312/700, Training Loss: 337.9449, Validation Loss: 339.0251\n",
      "Epoch 313/700, Training Loss: 337.9440, Validation Loss: 339.0242\n",
      "Epoch 314/700, Training Loss: 337.9432, Validation Loss: 339.0233\n",
      "Epoch 315/700, Training Loss: 337.9424, Validation Loss: 339.0225\n",
      "Epoch 316/700, Training Loss: 337.9416, Validation Loss: 339.0216\n",
      "Epoch 317/700, Training Loss: 337.9408, Validation Loss: 339.0208\n",
      "Epoch 318/700, Training Loss: 337.9400, Validation Loss: 339.0200\n",
      "Epoch 319/700, Training Loss: 337.9392, Validation Loss: 339.0192\n",
      "Epoch 320/700, Training Loss: 337.9384, Validation Loss: 339.0183\n",
      "Epoch 321/700, Training Loss: 337.9376, Validation Loss: 339.0175\n",
      "Epoch 322/700, Training Loss: 337.9368, Validation Loss: 339.0167\n",
      "Epoch 323/700, Training Loss: 337.9360, Validation Loss: 339.0159\n",
      "Epoch 324/700, Training Loss: 337.9353, Validation Loss: 339.0151\n",
      "Epoch 325/700, Training Loss: 337.9345, Validation Loss: 339.0143\n",
      "Epoch 326/700, Training Loss: 337.9337, Validation Loss: 339.0135\n",
      "Epoch 327/700, Training Loss: 337.9330, Validation Loss: 339.0128\n",
      "Epoch 328/700, Training Loss: 337.9322, Validation Loss: 339.0120\n",
      "Epoch 329/700, Training Loss: 337.9315, Validation Loss: 339.0113\n",
      "Epoch 330/700, Training Loss: 337.9307, Validation Loss: 339.0106\n",
      "Epoch 331/700, Training Loss: 337.9300, Validation Loss: 339.0099\n",
      "Epoch 332/700, Training Loss: 337.9293, Validation Loss: 339.0092\n",
      "Epoch 333/700, Training Loss: 337.9285, Validation Loss: 339.0085\n",
      "Epoch 334/700, Training Loss: 337.9278, Validation Loss: 339.0077\n",
      "Epoch 335/700, Training Loss: 337.9271, Validation Loss: 339.0070\n",
      "Epoch 336/700, Training Loss: 337.9264, Validation Loss: 339.0062\n",
      "Epoch 337/700, Training Loss: 337.9257, Validation Loss: 339.0055\n",
      "Epoch 338/700, Training Loss: 337.9250, Validation Loss: 339.0048\n",
      "Epoch 339/700, Training Loss: 337.9243, Validation Loss: 339.0042\n",
      "Epoch 340/700, Training Loss: 337.9236, Validation Loss: 339.0034\n",
      "Epoch 341/700, Training Loss: 337.9229, Validation Loss: 339.0027\n",
      "Epoch 342/700, Training Loss: 337.9222, Validation Loss: 339.0020\n",
      "Epoch 343/700, Training Loss: 337.9215, Validation Loss: 339.0014\n",
      "Epoch 344/700, Training Loss: 337.9208, Validation Loss: 339.0008\n",
      "Epoch 345/700, Training Loss: 337.9201, Validation Loss: 339.0001\n",
      "Epoch 346/700, Training Loss: 337.9195, Validation Loss: 338.9994\n",
      "Epoch 347/700, Training Loss: 337.9188, Validation Loss: 338.9987\n",
      "Epoch 348/700, Training Loss: 337.9181, Validation Loss: 338.9980\n",
      "Epoch 349/700, Training Loss: 337.9174, Validation Loss: 338.9973\n",
      "Epoch 350/700, Training Loss: 337.9167, Validation Loss: 338.9966\n",
      "Epoch 351/700, Training Loss: 337.9161, Validation Loss: 338.9960\n",
      "Epoch 352/700, Training Loss: 337.9154, Validation Loss: 338.9953\n",
      "Epoch 353/700, Training Loss: 337.9148, Validation Loss: 338.9945\n",
      "Epoch 354/700, Training Loss: 337.9141, Validation Loss: 338.9938\n",
      "Epoch 355/700, Training Loss: 337.9135, Validation Loss: 338.9931\n",
      "Epoch 356/700, Training Loss: 337.9128, Validation Loss: 338.9926\n",
      "Epoch 357/700, Training Loss: 337.9121, Validation Loss: 338.9920\n",
      "Epoch 358/700, Training Loss: 337.9114, Validation Loss: 338.9915\n",
      "Epoch 359/700, Training Loss: 337.9108, Validation Loss: 338.9909\n",
      "Epoch 360/700, Training Loss: 337.9101, Validation Loss: 338.9904\n",
      "Epoch 361/700, Training Loss: 337.9094, Validation Loss: 338.9896\n",
      "Epoch 362/700, Training Loss: 337.9087, Validation Loss: 338.9889\n",
      "Epoch 363/700, Training Loss: 337.9081, Validation Loss: 338.9883\n",
      "Epoch 364/700, Training Loss: 337.9074, Validation Loss: 338.9877\n",
      "Epoch 365/700, Training Loss: 337.9067, Validation Loss: 338.9873\n",
      "Epoch 366/700, Training Loss: 337.9061, Validation Loss: 338.9869\n",
      "Epoch 367/700, Training Loss: 337.9054, Validation Loss: 338.9862\n",
      "Epoch 368/700, Training Loss: 337.9048, Validation Loss: 338.9854\n",
      "Epoch 369/700, Training Loss: 337.9041, Validation Loss: 338.9845\n",
      "Epoch 370/700, Training Loss: 337.9034, Validation Loss: 338.9839\n",
      "Epoch 371/700, Training Loss: 337.9027, Validation Loss: 338.9832\n",
      "Epoch 372/700, Training Loss: 337.9020, Validation Loss: 338.9825\n",
      "Epoch 373/700, Training Loss: 337.9013, Validation Loss: 338.9818\n",
      "Epoch 374/700, Training Loss: 337.9007, Validation Loss: 338.9810\n",
      "Epoch 375/700, Training Loss: 337.9000, Validation Loss: 338.9803\n",
      "Epoch 376/700, Training Loss: 337.8993, Validation Loss: 338.9796\n",
      "Epoch 377/700, Training Loss: 337.8985, Validation Loss: 338.9789\n",
      "Epoch 378/700, Training Loss: 337.8978, Validation Loss: 338.9781\n",
      "Epoch 379/700, Training Loss: 337.8971, Validation Loss: 338.9773\n",
      "Epoch 380/700, Training Loss: 337.8963, Validation Loss: 338.9764\n",
      "Epoch 381/700, Training Loss: 337.8956, Validation Loss: 338.9756\n",
      "Epoch 382/700, Training Loss: 337.8948, Validation Loss: 338.9749\n",
      "Epoch 383/700, Training Loss: 337.8940, Validation Loss: 338.9742\n",
      "Epoch 384/700, Training Loss: 337.8932, Validation Loss: 338.9735\n",
      "Epoch 385/700, Training Loss: 337.8924, Validation Loss: 338.9729\n",
      "Epoch 386/700, Training Loss: 337.8916, Validation Loss: 338.9723\n",
      "Epoch 387/700, Training Loss: 337.8907, Validation Loss: 338.9714\n",
      "Epoch 388/700, Training Loss: 337.8899, Validation Loss: 338.9704\n",
      "Epoch 389/700, Training Loss: 337.8889, Validation Loss: 338.9694\n",
      "Epoch 390/700, Training Loss: 337.8880, Validation Loss: 338.9684\n",
      "Epoch 391/700, Training Loss: 337.8870, Validation Loss: 338.9676\n",
      "Epoch 392/700, Training Loss: 337.8859, Validation Loss: 338.9667\n",
      "Epoch 393/700, Training Loss: 337.8847, Validation Loss: 338.9653\n",
      "Epoch 394/700, Training Loss: 337.8833, Validation Loss: 338.9638\n",
      "Epoch 395/700, Training Loss: 337.8817, Validation Loss: 338.9623\n",
      "Epoch 396/700, Training Loss: 337.8798, Validation Loss: 338.9606\n",
      "Epoch 397/700, Training Loss: 337.8773, Validation Loss: 338.9576\n",
      "Epoch 398/700, Training Loss: 337.8741, Validation Loss: 338.9533\n",
      "Epoch 399/700, Training Loss: 337.8701, Validation Loss: 338.9474\n",
      "Epoch 400/700, Training Loss: 337.8648, Validation Loss: 338.9408\n",
      "Epoch 401/700, Training Loss: 337.8592, Validation Loss: 338.9360\n",
      "Epoch 402/700, Training Loss: 337.8544, Validation Loss: 338.9296\n",
      "Epoch 403/700, Training Loss: 337.8484, Validation Loss: 338.9225\n",
      "Epoch 404/700, Training Loss: 337.8412, Validation Loss: 338.9148\n",
      "Epoch 405/700, Training Loss: 337.8326, Validation Loss: 338.9059\n",
      "Epoch 406/700, Training Loss: 337.8223, Validation Loss: 338.8946\n",
      "Epoch 407/700, Training Loss: 337.8100, Validation Loss: 338.8803\n",
      "Epoch 408/700, Training Loss: 337.7953, Validation Loss: 338.8636\n",
      "Epoch 409/700, Training Loss: 337.7778, Validation Loss: 338.8437\n",
      "Epoch 410/700, Training Loss: 337.7568, Validation Loss: 338.8190\n",
      "Epoch 411/700, Training Loss: 337.7317, Validation Loss: 338.7886\n",
      "Epoch 412/700, Training Loss: 337.7012, Validation Loss: 338.7507\n",
      "Epoch 413/700, Training Loss: 337.6633, Validation Loss: 338.7037\n",
      "Epoch 414/700, Training Loss: 337.6155, Validation Loss: 338.6503\n",
      "Epoch 415/700, Training Loss: 337.5620, Validation Loss: 338.5905\n",
      "Epoch 416/700, Training Loss: 337.5012, Validation Loss: 338.5189\n",
      "Epoch 417/700, Training Loss: 337.4303, Validation Loss: 338.4369\n",
      "Epoch 418/700, Training Loss: 337.3495, Validation Loss: 338.3536\n",
      "Epoch 419/700, Training Loss: 337.2652, Validation Loss: 338.2727\n",
      "Epoch 420/700, Training Loss: 337.1855, Validation Loss: 338.2050\n",
      "Epoch 421/700, Training Loss: 337.1078, Validation Loss: 338.1252\n",
      "Epoch 422/700, Training Loss: 337.0429, Validation Loss: 338.1206\n",
      "Epoch 423/700, Training Loss: 337.0189, Validation Loss: 337.9922\n",
      "Epoch 424/700, Training Loss: 336.9087, Validation Loss: 337.9091\n",
      "Epoch 425/700, Training Loss: 336.8107, Validation Loss: 337.8601\n",
      "Epoch 426/700, Training Loss: 336.7665, Validation Loss: 337.8677\n",
      "Epoch 427/700, Training Loss: 336.7886, Validation Loss: 337.9601\n",
      "Epoch 428/700, Training Loss: 336.8623, Validation Loss: 337.8863\n",
      "Epoch 429/700, Training Loss: 336.8091, Validation Loss: 337.7965\n",
      "Epoch 430/700, Training Loss: 336.7050, Validation Loss: 337.7843\n",
      "Epoch 431/700, Training Loss: 336.6947, Validation Loss: 337.8044\n",
      "Epoch 432/700, Training Loss: 336.7288, Validation Loss: 337.7784\n",
      "Epoch 433/700, Training Loss: 336.6790, Validation Loss: 337.7252\n",
      "Epoch 434/700, Training Loss: 336.6346, Validation Loss: 337.7512\n",
      "Epoch 435/700, Training Loss: 336.6774, Validation Loss: 337.7388\n",
      "Epoch 436/700, Training Loss: 336.6435, Validation Loss: 337.7058\n",
      "Epoch 437/700, Training Loss: 336.6146, Validation Loss: 337.7217\n",
      "Epoch 438/700, Training Loss: 336.6481, Validation Loss: 337.7089\n",
      "Epoch 439/700, Training Loss: 336.6208, Validation Loss: 337.6954\n",
      "Epoch 440/700, Training Loss: 336.6074, Validation Loss: 337.7081\n",
      "Epoch 441/700, Training Loss: 336.6341, Validation Loss: 337.6896\n",
      "Epoch 442/700, Training Loss: 336.6068, Validation Loss: 337.6947\n",
      "Epoch 443/700, Training Loss: 336.6110, Validation Loss: 337.6958\n",
      "Epoch 444/700, Training Loss: 336.6222, Validation Loss: 337.6821\n",
      "Epoch 445/700, Training Loss: 336.6004, Validation Loss: 337.6985\n",
      "Epoch 446/700, Training Loss: 336.6141, Validation Loss: 337.6836\n",
      "Epoch 447/700, Training Loss: 336.6081, Validation Loss: 337.6832\n",
      "Epoch 448/700, Training Loss: 336.6006, Validation Loss: 337.6965\n",
      "Epoch 449/700, Training Loss: 336.6082, Validation Loss: 337.6770\n",
      "Epoch 450/700, Training Loss: 336.5988, Validation Loss: 337.6779\n",
      "Epoch 451/700, Training Loss: 336.5978, Validation Loss: 337.6889\n",
      "Epoch 452/700, Training Loss: 336.6003, Validation Loss: 337.6707\n",
      "Epoch 453/700, Training Loss: 336.5909, Validation Loss: 337.6699\n",
      "Epoch 454/700, Training Loss: 336.5943, Validation Loss: 337.6773\n",
      "Epoch 455/700, Training Loss: 336.5914, Validation Loss: 337.6682\n",
      "Epoch 456/700, Training Loss: 336.5858, Validation Loss: 337.6651\n",
      "Epoch 457/700, Training Loss: 336.5901, Validation Loss: 337.6671\n",
      "Epoch 458/700, Training Loss: 336.5836, Validation Loss: 337.6691\n",
      "Epoch 459/700, Training Loss: 336.5837, Validation Loss: 337.6617\n",
      "Epoch 460/700, Training Loss: 336.5841, Validation Loss: 337.6613\n",
      "Epoch 461/700, Training Loss: 336.5797, Validation Loss: 337.6673\n",
      "Epoch 462/700, Training Loss: 336.5815, Validation Loss: 337.6595\n",
      "Epoch 463/700, Training Loss: 336.5796, Validation Loss: 337.6578\n",
      "Epoch 464/700, Training Loss: 336.5777, Validation Loss: 337.6633\n",
      "Epoch 465/700, Training Loss: 336.5789, Validation Loss: 337.6573\n",
      "Epoch 466/700, Training Loss: 336.5765, Validation Loss: 337.6556\n",
      "Epoch 467/700, Training Loss: 336.5762, Validation Loss: 337.6594\n",
      "Epoch 468/700, Training Loss: 336.5763, Validation Loss: 337.6555\n",
      "Epoch 469/700, Training Loss: 336.5740, Validation Loss: 337.6543\n",
      "Epoch 470/700, Training Loss: 336.5747, Validation Loss: 337.6562\n",
      "Epoch 471/700, Training Loss: 336.5733, Validation Loss: 337.6545\n",
      "Epoch 472/700, Training Loss: 336.5722, Validation Loss: 337.6526\n",
      "Epoch 473/700, Training Loss: 336.5724, Validation Loss: 337.6540\n",
      "Epoch 474/700, Training Loss: 336.5707, Validation Loss: 337.6537\n",
      "Epoch 475/700, Training Loss: 336.5704, Validation Loss: 337.6505\n",
      "Epoch 476/700, Training Loss: 336.5699, Validation Loss: 337.6515\n",
      "Epoch 477/700, Training Loss: 336.5686, Validation Loss: 337.6520\n",
      "Epoch 478/700, Training Loss: 336.5683, Validation Loss: 337.6483\n",
      "Epoch 479/700, Training Loss: 336.5676, Validation Loss: 337.6486\n",
      "Epoch 480/700, Training Loss: 336.5666, Validation Loss: 337.6502\n",
      "Epoch 481/700, Training Loss: 336.5664, Validation Loss: 337.6468\n",
      "Epoch 482/700, Training Loss: 336.5655, Validation Loss: 337.6465\n",
      "Epoch 483/700, Training Loss: 336.5648, Validation Loss: 337.6486\n",
      "Epoch 484/700, Training Loss: 336.5645, Validation Loss: 337.6460\n",
      "Epoch 485/700, Training Loss: 336.5636, Validation Loss: 337.6451\n",
      "Epoch 486/700, Training Loss: 336.5632, Validation Loss: 337.6468\n",
      "Epoch 487/700, Training Loss: 336.5627, Validation Loss: 337.6451\n",
      "Epoch 488/700, Training Loss: 336.5620, Validation Loss: 337.6438\n",
      "Epoch 489/700, Training Loss: 336.5615, Validation Loss: 337.6450\n",
      "Epoch 490/700, Training Loss: 336.5610, Validation Loss: 337.6440\n",
      "Epoch 491/700, Training Loss: 336.5604, Validation Loss: 337.6427\n",
      "Epoch 492/700, Training Loss: 336.5599, Validation Loss: 337.6435\n",
      "Epoch 493/700, Training Loss: 336.5594, Validation Loss: 337.6427\n",
      "Epoch 494/700, Training Loss: 336.5588, Validation Loss: 337.6417\n",
      "Epoch 495/700, Training Loss: 336.5584, Validation Loss: 337.6421\n",
      "Epoch 496/700, Training Loss: 336.5578, Validation Loss: 337.6414\n",
      "Epoch 497/700, Training Loss: 336.5573, Validation Loss: 337.6405\n",
      "Epoch 498/700, Training Loss: 336.5569, Validation Loss: 337.6410\n",
      "Epoch 499/700, Training Loss: 336.5563, Validation Loss: 337.6404\n",
      "Epoch 500/700, Training Loss: 336.5558, Validation Loss: 337.6395\n",
      "Epoch 501/700, Training Loss: 336.5554, Validation Loss: 337.6399\n",
      "Epoch 502/700, Training Loss: 336.5548, Validation Loss: 337.6393\n",
      "Epoch 503/700, Training Loss: 336.5544, Validation Loss: 337.6383\n",
      "Epoch 504/700, Training Loss: 336.5540, Validation Loss: 337.6385\n",
      "Epoch 505/700, Training Loss: 336.5534, Validation Loss: 337.6380\n",
      "Epoch 506/700, Training Loss: 336.5530, Validation Loss: 337.6369\n",
      "Epoch 507/700, Training Loss: 336.5526, Validation Loss: 337.6371\n",
      "Epoch 508/700, Training Loss: 336.5521, Validation Loss: 337.6368\n",
      "Epoch 509/700, Training Loss: 336.5516, Validation Loss: 337.6358\n",
      "Epoch 510/700, Training Loss: 336.5512, Validation Loss: 337.6359\n",
      "Epoch 511/700, Training Loss: 336.5508, Validation Loss: 337.6357\n",
      "Epoch 512/700, Training Loss: 336.5503, Validation Loss: 337.6348\n",
      "Epoch 513/700, Training Loss: 336.5499, Validation Loss: 337.6348\n",
      "Epoch 514/700, Training Loss: 336.5494, Validation Loss: 337.6345\n",
      "Epoch 515/700, Training Loss: 336.5490, Validation Loss: 337.6338\n",
      "Epoch 516/700, Training Loss: 336.5486, Validation Loss: 337.6336\n",
      "Epoch 517/700, Training Loss: 336.5482, Validation Loss: 337.6333\n",
      "Epoch 518/700, Training Loss: 336.5478, Validation Loss: 337.6327\n",
      "Epoch 519/700, Training Loss: 336.5474, Validation Loss: 337.6326\n",
      "Epoch 520/700, Training Loss: 336.5469, Validation Loss: 337.6321\n",
      "Epoch 521/700, Training Loss: 336.5465, Validation Loss: 337.6317\n",
      "Epoch 522/700, Training Loss: 336.5461, Validation Loss: 337.6317\n",
      "Epoch 523/700, Training Loss: 336.5457, Validation Loss: 337.6313\n",
      "Epoch 524/700, Training Loss: 336.5453, Validation Loss: 337.6309\n",
      "Epoch 525/700, Training Loss: 336.5450, Validation Loss: 337.6309\n",
      "Epoch 526/700, Training Loss: 336.5446, Validation Loss: 337.6304\n",
      "Epoch 527/700, Training Loss: 336.5442, Validation Loss: 337.6300\n",
      "Epoch 528/700, Training Loss: 336.5438, Validation Loss: 337.6299\n",
      "Epoch 529/700, Training Loss: 336.5435, Validation Loss: 337.6294\n",
      "Epoch 530/700, Training Loss: 336.5430, Validation Loss: 337.6292\n",
      "Epoch 531/700, Training Loss: 336.5427, Validation Loss: 337.6291\n",
      "Epoch 532/700, Training Loss: 336.5423, Validation Loss: 337.6287\n",
      "Epoch 533/700, Training Loss: 336.5420, Validation Loss: 337.6284\n",
      "Epoch 534/700, Training Loss: 336.5416, Validation Loss: 337.6282\n",
      "Epoch 535/700, Training Loss: 336.5412, Validation Loss: 337.6278\n",
      "Epoch 536/700, Training Loss: 336.5409, Validation Loss: 337.6276\n",
      "Epoch 537/700, Training Loss: 336.5405, Validation Loss: 337.6274\n",
      "Epoch 538/700, Training Loss: 336.5402, Validation Loss: 337.6271\n",
      "Epoch 539/700, Training Loss: 336.5398, Validation Loss: 337.6270\n",
      "Epoch 540/700, Training Loss: 336.5394, Validation Loss: 337.6267\n",
      "Epoch 541/700, Training Loss: 336.5391, Validation Loss: 337.6264\n",
      "Epoch 542/700, Training Loss: 336.5388, Validation Loss: 337.6263\n",
      "Epoch 543/700, Training Loss: 336.5384, Validation Loss: 337.6259\n",
      "Epoch 544/700, Training Loss: 336.5381, Validation Loss: 337.6256\n",
      "Epoch 545/700, Training Loss: 336.5377, Validation Loss: 337.6255\n",
      "Epoch 546/700, Training Loss: 336.5374, Validation Loss: 337.6252\n",
      "Epoch 547/700, Training Loss: 336.5371, Validation Loss: 337.6249\n",
      "Epoch 548/700, Training Loss: 336.5367, Validation Loss: 337.6248\n",
      "Epoch 549/700, Training Loss: 336.5364, Validation Loss: 337.6245\n",
      "Epoch 550/700, Training Loss: 336.5361, Validation Loss: 337.6242\n",
      "Epoch 551/700, Training Loss: 336.5358, Validation Loss: 337.6241\n",
      "Epoch 552/700, Training Loss: 336.5354, Validation Loss: 337.6238\n",
      "Epoch 553/700, Training Loss: 336.5351, Validation Loss: 337.6236\n",
      "Epoch 554/700, Training Loss: 336.5348, Validation Loss: 337.6234\n",
      "Epoch 555/700, Training Loss: 336.5345, Validation Loss: 337.6231\n",
      "Epoch 556/700, Training Loss: 336.5341, Validation Loss: 337.6229\n",
      "Epoch 557/700, Training Loss: 336.5339, Validation Loss: 337.6227\n",
      "Epoch 558/700, Training Loss: 336.5336, Validation Loss: 337.6224\n",
      "Epoch 559/700, Training Loss: 336.5332, Validation Loss: 337.6222\n",
      "Epoch 560/700, Training Loss: 336.5329, Validation Loss: 337.6219\n",
      "Epoch 561/700, Training Loss: 336.5327, Validation Loss: 337.6217\n",
      "Epoch 562/700, Training Loss: 336.5323, Validation Loss: 337.6216\n",
      "Epoch 563/700, Training Loss: 336.5320, Validation Loss: 337.6213\n",
      "Epoch 564/700, Training Loss: 336.5317, Validation Loss: 337.6211\n",
      "Epoch 565/700, Training Loss: 336.5314, Validation Loss: 337.6209\n",
      "Epoch 566/700, Training Loss: 336.5312, Validation Loss: 337.6206\n",
      "Epoch 567/700, Training Loss: 336.5309, Validation Loss: 337.6204\n",
      "Epoch 568/700, Training Loss: 336.5306, Validation Loss: 337.6202\n",
      "Epoch 569/700, Training Loss: 336.5303, Validation Loss: 337.6200\n",
      "Epoch 570/700, Training Loss: 336.5300, Validation Loss: 337.6198\n",
      "Epoch 571/700, Training Loss: 336.5297, Validation Loss: 337.6195\n",
      "Epoch 572/700, Training Loss: 336.5294, Validation Loss: 337.6194\n",
      "Epoch 573/700, Training Loss: 336.5291, Validation Loss: 337.6192\n",
      "Epoch 574/700, Training Loss: 336.5289, Validation Loss: 337.6189\n",
      "Epoch 575/700, Training Loss: 336.5286, Validation Loss: 337.6188\n",
      "Epoch 576/700, Training Loss: 336.5283, Validation Loss: 337.6186\n",
      "Epoch 577/700, Training Loss: 336.5280, Validation Loss: 337.6183\n",
      "Epoch 578/700, Training Loss: 336.5277, Validation Loss: 337.6182\n",
      "Epoch 579/700, Training Loss: 336.5274, Validation Loss: 337.6180\n",
      "Epoch 580/700, Training Loss: 336.5272, Validation Loss: 337.6178\n",
      "Epoch 581/700, Training Loss: 336.5269, Validation Loss: 337.6176\n",
      "Epoch 582/700, Training Loss: 336.5267, Validation Loss: 337.6174\n",
      "Epoch 583/700, Training Loss: 336.5264, Validation Loss: 337.6173\n",
      "Epoch 584/700, Training Loss: 336.5261, Validation Loss: 337.6170\n",
      "Epoch 585/700, Training Loss: 336.5259, Validation Loss: 337.6169\n",
      "Epoch 586/700, Training Loss: 336.5256, Validation Loss: 337.6167\n",
      "Epoch 587/700, Training Loss: 336.5254, Validation Loss: 337.6164\n",
      "Epoch 588/700, Training Loss: 336.5251, Validation Loss: 337.6163\n",
      "Epoch 589/700, Training Loss: 336.5248, Validation Loss: 337.6161\n",
      "Epoch 590/700, Training Loss: 336.5246, Validation Loss: 337.6159\n",
      "Epoch 591/700, Training Loss: 336.5243, Validation Loss: 337.6158\n",
      "Epoch 592/700, Training Loss: 336.5240, Validation Loss: 337.6155\n",
      "Epoch 593/700, Training Loss: 336.5238, Validation Loss: 337.6154\n",
      "Epoch 594/700, Training Loss: 336.5236, Validation Loss: 337.6152\n",
      "Epoch 595/700, Training Loss: 336.5233, Validation Loss: 337.6150\n",
      "Epoch 596/700, Training Loss: 336.5230, Validation Loss: 337.6149\n",
      "Epoch 597/700, Training Loss: 336.5228, Validation Loss: 337.6147\n",
      "Epoch 598/700, Training Loss: 336.5225, Validation Loss: 337.6146\n",
      "Epoch 599/700, Training Loss: 336.5223, Validation Loss: 337.6144\n",
      "Epoch 600/700, Training Loss: 336.5221, Validation Loss: 337.6142\n",
      "Epoch 601/700, Training Loss: 336.5218, Validation Loss: 337.6141\n",
      "Epoch 602/700, Training Loss: 336.5216, Validation Loss: 337.6139\n",
      "Epoch 603/700, Training Loss: 336.5213, Validation Loss: 337.6137\n",
      "Epoch 604/700, Training Loss: 336.5211, Validation Loss: 337.6137\n",
      "Epoch 605/700, Training Loss: 336.5208, Validation Loss: 337.6134\n",
      "Epoch 606/700, Training Loss: 336.5206, Validation Loss: 337.6133\n",
      "Epoch 607/700, Training Loss: 336.5204, Validation Loss: 337.6132\n",
      "Epoch 608/700, Training Loss: 336.5201, Validation Loss: 337.6129\n",
      "Epoch 609/700, Training Loss: 336.5199, Validation Loss: 337.6129\n",
      "Epoch 610/700, Training Loss: 336.5197, Validation Loss: 337.6127\n",
      "Epoch 611/700, Training Loss: 336.5194, Validation Loss: 337.6125\n",
      "Epoch 612/700, Training Loss: 336.5192, Validation Loss: 337.6124\n",
      "Epoch 613/700, Training Loss: 336.5189, Validation Loss: 337.6122\n",
      "Epoch 614/700, Training Loss: 336.5187, Validation Loss: 337.6121\n",
      "Epoch 615/700, Training Loss: 336.5185, Validation Loss: 337.6119\n",
      "Epoch 616/700, Training Loss: 336.5182, Validation Loss: 337.6117\n",
      "Epoch 617/700, Training Loss: 336.5180, Validation Loss: 337.6116\n",
      "Epoch 618/700, Training Loss: 336.5178, Validation Loss: 337.6114\n",
      "Epoch 619/700, Training Loss: 336.5175, Validation Loss: 337.6113\n",
      "Epoch 620/700, Training Loss: 336.5173, Validation Loss: 337.6112\n",
      "Epoch 621/700, Training Loss: 336.5171, Validation Loss: 337.6110\n",
      "Epoch 622/700, Training Loss: 336.5169, Validation Loss: 337.6109\n",
      "Epoch 623/700, Training Loss: 336.5167, Validation Loss: 337.6107\n",
      "Epoch 624/700, Training Loss: 336.5164, Validation Loss: 337.6106\n",
      "Epoch 625/700, Training Loss: 336.5162, Validation Loss: 337.6104\n",
      "Epoch 626/700, Training Loss: 336.5160, Validation Loss: 337.6104\n",
      "Epoch 627/700, Training Loss: 336.5157, Validation Loss: 337.6102\n",
      "Epoch 628/700, Training Loss: 336.5155, Validation Loss: 337.6101\n",
      "Epoch 629/700, Training Loss: 336.5153, Validation Loss: 337.6099\n",
      "Epoch 630/700, Training Loss: 336.5151, Validation Loss: 337.6098\n",
      "Epoch 631/700, Training Loss: 336.5149, Validation Loss: 337.6096\n",
      "Epoch 632/700, Training Loss: 336.5147, Validation Loss: 337.6095\n",
      "Epoch 633/700, Training Loss: 336.5144, Validation Loss: 337.6095\n",
      "Epoch 634/700, Training Loss: 336.5142, Validation Loss: 337.6093\n",
      "Epoch 635/700, Training Loss: 336.5140, Validation Loss: 337.6093\n",
      "Epoch 636/700, Training Loss: 336.5138, Validation Loss: 337.6091\n",
      "Epoch 637/700, Training Loss: 336.5136, Validation Loss: 337.6089\n",
      "Epoch 638/700, Training Loss: 336.5134, Validation Loss: 337.6088\n",
      "Epoch 639/700, Training Loss: 336.5132, Validation Loss: 337.6086\n",
      "Epoch 640/700, Training Loss: 336.5129, Validation Loss: 337.6085\n",
      "Epoch 641/700, Training Loss: 336.5128, Validation Loss: 337.6084\n",
      "Epoch 642/700, Training Loss: 336.5125, Validation Loss: 337.6081\n",
      "Epoch 643/700, Training Loss: 336.5123, Validation Loss: 337.6080\n",
      "Epoch 644/700, Training Loss: 336.5121, Validation Loss: 337.6078\n",
      "Epoch 645/700, Training Loss: 336.5119, Validation Loss: 337.6078\n",
      "Epoch 646/700, Training Loss: 336.5117, Validation Loss: 337.6077\n",
      "Epoch 647/700, Training Loss: 336.5115, Validation Loss: 337.6077\n",
      "Epoch 648/700, Training Loss: 336.5113, Validation Loss: 337.6076\n",
      "Epoch 649/700, Training Loss: 336.5111, Validation Loss: 337.6075\n",
      "Epoch 650/700, Training Loss: 336.5109, Validation Loss: 337.6073\n",
      "Epoch 651/700, Training Loss: 336.5107, Validation Loss: 337.6071\n",
      "Epoch 652/700, Training Loss: 336.5105, Validation Loss: 337.6070\n",
      "Epoch 653/700, Training Loss: 336.5102, Validation Loss: 337.6069\n",
      "Epoch 654/700, Training Loss: 336.5101, Validation Loss: 337.6068\n",
      "Epoch 655/700, Training Loss: 336.5099, Validation Loss: 337.6068\n",
      "Epoch 656/700, Training Loss: 336.5096, Validation Loss: 337.6067\n",
      "Epoch 657/700, Training Loss: 336.5095, Validation Loss: 337.6065\n",
      "Epoch 658/700, Training Loss: 336.5092, Validation Loss: 337.6065\n",
      "Epoch 659/700, Training Loss: 336.5090, Validation Loss: 337.6064\n",
      "Epoch 660/700, Training Loss: 336.5089, Validation Loss: 337.6063\n",
      "Epoch 661/700, Training Loss: 336.5087, Validation Loss: 337.6063\n",
      "Epoch 662/700, Training Loss: 336.5084, Validation Loss: 337.6062\n",
      "Epoch 663/700, Training Loss: 336.5083, Validation Loss: 337.6060\n",
      "Epoch 664/700, Training Loss: 336.5081, Validation Loss: 337.6058\n",
      "Epoch 665/700, Training Loss: 336.5079, Validation Loss: 337.6058\n",
      "Epoch 666/700, Training Loss: 336.5077, Validation Loss: 337.6057\n",
      "Epoch 667/700, Training Loss: 336.5075, Validation Loss: 337.6057\n",
      "Epoch 668/700, Training Loss: 336.5073, Validation Loss: 337.6057\n",
      "Epoch 669/700, Training Loss: 336.5071, Validation Loss: 337.6057\n",
      "Epoch 670/700, Training Loss: 336.5069, Validation Loss: 337.6056\n",
      "Epoch 671/700, Training Loss: 336.5067, Validation Loss: 337.6055\n",
      "Epoch 672/700, Training Loss: 336.5065, Validation Loss: 337.6052\n",
      "Epoch 673/700, Training Loss: 336.5063, Validation Loss: 337.6051\n",
      "Epoch 674/700, Training Loss: 336.5062, Validation Loss: 337.6052\n",
      "Epoch 675/700, Training Loss: 336.5060, Validation Loss: 337.6050\n",
      "Epoch 676/700, Training Loss: 336.5058, Validation Loss: 337.6051\n",
      "Epoch 677/700, Training Loss: 336.5056, Validation Loss: 337.6049\n",
      "Epoch 678/700, Training Loss: 336.5054, Validation Loss: 337.6050\n",
      "Epoch 679/700, Training Loss: 336.5052, Validation Loss: 337.6049\n",
      "Epoch 680/700, Training Loss: 336.5050, Validation Loss: 337.6049\n",
      "Epoch 681/700, Training Loss: 336.5049, Validation Loss: 337.6047\n",
      "Epoch 682/700, Training Loss: 336.5047, Validation Loss: 337.6046\n",
      "Epoch 683/700, Training Loss: 336.5045, Validation Loss: 337.6045\n",
      "Epoch 684/700, Training Loss: 336.5043, Validation Loss: 337.6045\n",
      "Epoch 685/700, Training Loss: 336.5041, Validation Loss: 337.6043\n",
      "Epoch 686/700, Training Loss: 336.5039, Validation Loss: 337.6045\n",
      "Epoch 687/700, Training Loss: 336.5038, Validation Loss: 337.6042\n",
      "Epoch 688/700, Training Loss: 336.5036, Validation Loss: 337.6046\n",
      "Epoch 689/700, Training Loss: 336.5034, Validation Loss: 337.6039\n",
      "Epoch 690/700, Training Loss: 336.5033, Validation Loss: 337.6044\n",
      "Epoch 691/700, Training Loss: 336.5031, Validation Loss: 337.6037\n",
      "Epoch 692/700, Training Loss: 336.5029, Validation Loss: 337.6045\n",
      "Epoch 693/700, Training Loss: 336.5027, Validation Loss: 337.6035\n",
      "Epoch 694/700, Training Loss: 336.5026, Validation Loss: 337.6047\n",
      "Epoch 695/700, Training Loss: 336.5025, Validation Loss: 337.6032\n",
      "Epoch 696/700, Training Loss: 336.5023, Validation Loss: 337.6046\n",
      "Epoch 697/700, Training Loss: 336.5022, Validation Loss: 337.6030\n",
      "Epoch 698/700, Training Loss: 336.5020, Validation Loss: 337.6048\n",
      "Epoch 699/700, Training Loss: 336.5019, Validation Loss: 337.6028\n",
      "Epoch 700/700, Training Loss: 336.5018, Validation Loss: 337.6053\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.25%\n",
      "Mean Percent Accuracy (Yield): 89.03%\n",
      "Median Percent Accuracy (Days to Harvest): 79.45%\n",
      "Median Percent Accuracy (Yield): 92.42%\n",
      "Accuracy Within 10% (Days to Harvest): 23.59%\n",
      "Accuracy Within 10% (Yield): 61.77%\n",
      "MAE (Days to Harvest): 22.45\n",
      "MAE (Yield): 0.41\n",
      "RMSE (Days to Harvest): 25.94\n",
      "RMSE (Yield): 0.51\n",
      "\n",
      "Validation Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.17%\n",
      "Mean Percent Accuracy (Yield): 88.74%\n",
      "Median Percent Accuracy (Days to Harvest): 79.42%\n",
      "Median Percent Accuracy (Yield): 92.39%\n",
      "Accuracy Within 10% (Days to Harvest): 23.46%\n",
      "Accuracy Within 10% (Yield): 61.63%\n",
      "MAE (Days to Harvest): 22.49\n",
      "MAE (Yield): 0.41\n",
      "RMSE (Days to Harvest): 25.98\n",
      "RMSE (Yield): 0.51\n",
      "\n",
      "Training completed in 40.08 seconds\n",
      "Final Training Loss: 336.5018\n",
      "Final Validation Loss: 337.6053\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y.values, test_size=0.2, random_state=88)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Define the neural network\n",
    "class CropYieldNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CropYieldNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 128)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 2)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.swish = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.swish(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.leaky_relu(self.fc4(x))\n",
    "        x = self.leaky_relu(self.fc5(x))\n",
    "        return self.output(x)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move data tensors to the same device as the model\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = CropYieldNN(input_dim).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.008, weight_decay=2e-6)\n",
    "\n",
    "\n",
    "# Train the neural network\n",
    "n_epochs = 700\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_train_tensor)\n",
    "    train_loss = criterion(predictions, y_train_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_val_tensor)\n",
    "        val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "    # Store losses\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Final training and validation loss\n",
    "final_train_loss = train_losses[-1]\n",
    "final_val_loss = val_losses[-1]\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, n_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate metrics\n",
    "def evaluate_model_with_metrics(y_actual, y_pred, tolerance=0.1):\n",
    "    # Percent accuracy\n",
    "    percent_accuracies = 100 * (1 - np.abs(y_actual - y_pred) / y_actual)\n",
    "    mean_percent_accuracy = np.mean(percent_accuracies, axis=0)\n",
    "    median_percent_accuracy = np.median(percent_accuracies, axis=0)  # Median for robustness\n",
    "\n",
    "    # Accuracy within tolerance\n",
    "    within_tolerance = np.abs(y_actual - y_pred) <= (tolerance * y_actual)\n",
    "    percent_within_tolerance = np.mean(within_tolerance, axis=0) * 100\n",
    "\n",
    "    # MAE\n",
    "    mae = np.mean(np.abs(y_actual - y_pred), axis=0)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(np.mean((y_actual - y_pred) ** 2, axis=0))\n",
    "\n",
    "    return {\n",
    "        \"mean_percent_accuracy\": mean_percent_accuracy,\n",
    "        \"median_percent_accuracy\": median_percent_accuracy,\n",
    "        \"percent_within_tolerance\": percent_within_tolerance,\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "    }\n",
    "\n",
    "# Evaluate metrics\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_preds = model(X_train_tensor).cpu().numpy()\n",
    "    val_preds = model(X_val_tensor).cpu().numpy()\n",
    "    y_train_np = y_train_tensor.cpu().numpy()\n",
    "    y_val_np = y_val_tensor.cpu().numpy()\n",
    "\n",
    "    # Training metrics\n",
    "    train_metrics = evaluate_model_with_metrics(y_train_np, train_preds, tolerance=0.1)\n",
    "    \n",
    "    # Validation metrics\n",
    "    val_metrics = evaluate_model_with_metrics(y_val_np, val_preds, tolerance=0.1)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {train_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {train_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {train_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {train_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {train_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {train_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {train_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {train_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {train_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {train_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {val_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {val_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {val_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {val_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {val_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {val_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {val_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {val_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {val_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {val_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {final_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6000, Training Loss: 54.2205, Validation Loss: 54.1467\n",
      "Epoch 2/6000, Training Loss: 54.1805, Validation Loss: 54.1068\n",
      "Epoch 3/6000, Training Loss: 54.1405, Validation Loss: 54.0669\n",
      "Epoch 4/6000, Training Loss: 54.1005, Validation Loss: 54.0269\n",
      "Epoch 5/6000, Training Loss: 54.0605, Validation Loss: 53.9870\n",
      "Epoch 6/6000, Training Loss: 54.0205, Validation Loss: 53.9471\n",
      "Epoch 7/6000, Training Loss: 53.9805, Validation Loss: 53.9071\n",
      "Epoch 8/6000, Training Loss: 53.9405, Validation Loss: 53.8672\n",
      "Epoch 9/6000, Training Loss: 53.9005, Validation Loss: 53.8273\n",
      "Epoch 10/6000, Training Loss: 53.8605, Validation Loss: 53.7873\n",
      "Epoch 11/6000, Training Loss: 53.8205, Validation Loss: 53.7474\n",
      "Epoch 12/6000, Training Loss: 53.7805, Validation Loss: 53.7075\n",
      "Epoch 13/6000, Training Loss: 53.7405, Validation Loss: 53.6676\n",
      "Epoch 14/6000, Training Loss: 53.7005, Validation Loss: 53.6276\n",
      "Epoch 15/6000, Training Loss: 53.6605, Validation Loss: 53.5877\n",
      "Epoch 16/6000, Training Loss: 53.6205, Validation Loss: 53.5478\n",
      "Epoch 17/6000, Training Loss: 53.5805, Validation Loss: 53.5079\n",
      "Epoch 18/6000, Training Loss: 53.5405, Validation Loss: 53.4679\n",
      "Epoch 19/6000, Training Loss: 53.5005, Validation Loss: 53.4280\n",
      "Epoch 20/6000, Training Loss: 53.4605, Validation Loss: 53.3881\n",
      "Epoch 21/6000, Training Loss: 53.4205, Validation Loss: 53.3482\n",
      "Epoch 22/6000, Training Loss: 53.3805, Validation Loss: 53.3082\n",
      "Epoch 23/6000, Training Loss: 53.3405, Validation Loss: 53.2683\n",
      "Epoch 24/6000, Training Loss: 53.3005, Validation Loss: 53.2284\n",
      "Epoch 25/6000, Training Loss: 53.2606, Validation Loss: 53.1885\n",
      "Epoch 26/6000, Training Loss: 53.2206, Validation Loss: 53.1486\n",
      "Epoch 27/6000, Training Loss: 53.1806, Validation Loss: 53.1086\n",
      "Epoch 28/6000, Training Loss: 53.1406, Validation Loss: 53.0687\n",
      "Epoch 29/6000, Training Loss: 53.1006, Validation Loss: 53.0288\n",
      "Epoch 30/6000, Training Loss: 53.0606, Validation Loss: 52.9889\n",
      "Epoch 31/6000, Training Loss: 53.0206, Validation Loss: 52.9490\n",
      "Epoch 32/6000, Training Loss: 52.9806, Validation Loss: 52.9091\n",
      "Epoch 33/6000, Training Loss: 52.9407, Validation Loss: 52.8691\n",
      "Epoch 34/6000, Training Loss: 52.9007, Validation Loss: 52.8292\n",
      "Epoch 35/6000, Training Loss: 52.8607, Validation Loss: 52.7893\n",
      "Epoch 36/6000, Training Loss: 52.8207, Validation Loss: 52.7494\n",
      "Epoch 37/6000, Training Loss: 52.7808, Validation Loss: 52.7095\n",
      "Epoch 38/6000, Training Loss: 52.7408, Validation Loss: 52.6696\n",
      "Epoch 39/6000, Training Loss: 52.7008, Validation Loss: 52.6297\n",
      "Epoch 40/6000, Training Loss: 52.6608, Validation Loss: 52.5898\n",
      "Epoch 41/6000, Training Loss: 52.6209, Validation Loss: 52.5499\n",
      "Epoch 42/6000, Training Loss: 52.5809, Validation Loss: 52.5100\n",
      "Epoch 43/6000, Training Loss: 52.5409, Validation Loss: 52.4701\n",
      "Epoch 44/6000, Training Loss: 52.5010, Validation Loss: 52.4303\n",
      "Epoch 45/6000, Training Loss: 52.4610, Validation Loss: 52.3904\n",
      "Epoch 46/6000, Training Loss: 52.4211, Validation Loss: 52.3505\n",
      "Epoch 47/6000, Training Loss: 52.3811, Validation Loss: 52.3106\n",
      "Epoch 48/6000, Training Loss: 52.3412, Validation Loss: 52.2708\n",
      "Epoch 49/6000, Training Loss: 52.3013, Validation Loss: 52.2309\n",
      "Epoch 50/6000, Training Loss: 52.2613, Validation Loss: 52.1911\n",
      "Epoch 51/6000, Training Loss: 52.2214, Validation Loss: 52.1512\n",
      "Epoch 52/6000, Training Loss: 52.1815, Validation Loss: 52.1114\n",
      "Epoch 53/6000, Training Loss: 52.1416, Validation Loss: 52.0716\n",
      "Epoch 54/6000, Training Loss: 52.1017, Validation Loss: 52.0318\n",
      "Epoch 55/6000, Training Loss: 52.0619, Validation Loss: 51.9920\n",
      "Epoch 56/6000, Training Loss: 52.0220, Validation Loss: 51.9522\n",
      "Epoch 57/6000, Training Loss: 51.9821, Validation Loss: 51.9124\n",
      "Epoch 58/6000, Training Loss: 51.9423, Validation Loss: 51.8727\n",
      "Epoch 59/6000, Training Loss: 51.9025, Validation Loss: 51.8330\n",
      "Epoch 60/6000, Training Loss: 51.8627, Validation Loss: 51.7933\n",
      "Epoch 61/6000, Training Loss: 51.8229, Validation Loss: 51.7536\n",
      "Epoch 62/6000, Training Loss: 51.7831, Validation Loss: 51.7139\n",
      "Epoch 63/6000, Training Loss: 51.7434, Validation Loss: 51.6743\n",
      "Epoch 64/6000, Training Loss: 51.7037, Validation Loss: 51.6347\n",
      "Epoch 65/6000, Training Loss: 51.6640, Validation Loss: 51.5951\n",
      "Epoch 66/6000, Training Loss: 51.6243, Validation Loss: 51.5555\n",
      "Epoch 67/6000, Training Loss: 51.5847, Validation Loss: 51.5160\n",
      "Epoch 68/6000, Training Loss: 51.5452, Validation Loss: 51.4766\n",
      "Epoch 69/6000, Training Loss: 51.5056, Validation Loss: 51.4372\n",
      "Epoch 70/6000, Training Loss: 51.4661, Validation Loss: 51.3978\n",
      "Epoch 71/6000, Training Loss: 51.4267, Validation Loss: 51.3585\n",
      "Epoch 72/6000, Training Loss: 51.3873, Validation Loss: 51.3192\n",
      "Epoch 73/6000, Training Loss: 51.3479, Validation Loss: 51.2800\n",
      "Epoch 74/6000, Training Loss: 51.3086, Validation Loss: 51.2409\n",
      "Epoch 75/6000, Training Loss: 51.2694, Validation Loss: 51.2018\n",
      "Epoch 76/6000, Training Loss: 51.2302, Validation Loss: 51.1628\n",
      "Epoch 77/6000, Training Loss: 51.1912, Validation Loss: 51.1238\n",
      "Epoch 78/6000, Training Loss: 51.1521, Validation Loss: 51.0850\n",
      "Epoch 79/6000, Training Loss: 51.1132, Validation Loss: 51.0462\n",
      "Epoch 80/6000, Training Loss: 51.0744, Validation Loss: 51.0076\n",
      "Epoch 81/6000, Training Loss: 51.0356, Validation Loss: 50.9690\n",
      "Epoch 82/6000, Training Loss: 50.9970, Validation Loss: 50.9306\n",
      "Epoch 83/6000, Training Loss: 50.9584, Validation Loss: 50.8923\n",
      "Epoch 84/6000, Training Loss: 50.9201, Validation Loss: 50.8542\n",
      "Epoch 85/6000, Training Loss: 50.8818, Validation Loss: 50.8162\n",
      "Epoch 86/6000, Training Loss: 50.8438, Validation Loss: 50.7784\n",
      "Epoch 87/6000, Training Loss: 50.8059, Validation Loss: 50.7409\n",
      "Epoch 88/6000, Training Loss: 50.7683, Validation Loss: 50.7036\n",
      "Epoch 89/6000, Training Loss: 50.7310, Validation Loss: 50.6666\n",
      "Epoch 90/6000, Training Loss: 50.6939, Validation Loss: 50.6300\n",
      "Epoch 91/6000, Training Loss: 50.6572, Validation Loss: 50.5938\n",
      "Epoch 92/6000, Training Loss: 50.6209, Validation Loss: 50.5580\n",
      "Epoch 93/6000, Training Loss: 50.5850, Validation Loss: 50.5227\n",
      "Epoch 94/6000, Training Loss: 50.5496, Validation Loss: 50.4879\n",
      "Epoch 95/6000, Training Loss: 50.5148, Validation Loss: 50.4538\n",
      "Epoch 96/6000, Training Loss: 50.4806, Validation Loss: 50.4204\n",
      "Epoch 97/6000, Training Loss: 50.4471, Validation Loss: 50.3878\n",
      "Epoch 98/6000, Training Loss: 50.4142, Validation Loss: 50.3558\n",
      "Epoch 99/6000, Training Loss: 50.3822, Validation Loss: 50.3247\n",
      "Epoch 100/6000, Training Loss: 50.3509, Validation Loss: 50.2945\n",
      "Epoch 101/6000, Training Loss: 50.3205, Validation Loss: 50.2651\n",
      "Epoch 102/6000, Training Loss: 50.2909, Validation Loss: 50.2365\n",
      "Epoch 103/6000, Training Loss: 50.2622, Validation Loss: 50.2088\n",
      "Epoch 104/6000, Training Loss: 50.2343, Validation Loss: 50.1818\n",
      "Epoch 105/6000, Training Loss: 50.2073, Validation Loss: 50.1556\n",
      "Epoch 106/6000, Training Loss: 50.1809, Validation Loss: 50.1301\n",
      "Epoch 107/6000, Training Loss: 50.1553, Validation Loss: 50.1052\n",
      "Epoch 108/6000, Training Loss: 50.1303, Validation Loss: 50.0809\n",
      "Epoch 109/6000, Training Loss: 50.1059, Validation Loss: 50.0571\n",
      "Epoch 110/6000, Training Loss: 50.0820, Validation Loss: 50.0338\n",
      "Epoch 111/6000, Training Loss: 50.0586, Validation Loss: 50.0109\n",
      "Epoch 112/6000, Training Loss: 50.0356, Validation Loss: 49.9885\n",
      "Epoch 113/6000, Training Loss: 50.0131, Validation Loss: 49.9664\n",
      "Epoch 114/6000, Training Loss: 49.9909, Validation Loss: 49.9446\n",
      "Epoch 115/6000, Training Loss: 49.9690, Validation Loss: 49.9232\n",
      "Epoch 116/6000, Training Loss: 49.9475, Validation Loss: 49.9020\n",
      "Epoch 117/6000, Training Loss: 49.9262, Validation Loss: 49.8811\n",
      "Epoch 118/6000, Training Loss: 49.9052, Validation Loss: 49.8604\n",
      "Epoch 119/6000, Training Loss: 49.8844, Validation Loss: 49.8398\n",
      "Epoch 120/6000, Training Loss: 49.8638, Validation Loss: 49.8194\n",
      "Epoch 121/6000, Training Loss: 49.8433, Validation Loss: 49.7992\n",
      "Epoch 122/6000, Training Loss: 49.8230, Validation Loss: 49.7790\n",
      "Epoch 123/6000, Training Loss: 49.8027, Validation Loss: 49.7588\n",
      "Epoch 124/6000, Training Loss: 49.7825, Validation Loss: 49.7388\n",
      "Epoch 125/6000, Training Loss: 49.7624, Validation Loss: 49.7187\n",
      "Epoch 126/6000, Training Loss: 49.7423, Validation Loss: 49.6986\n",
      "Epoch 127/6000, Training Loss: 49.7221, Validation Loss: 49.6785\n",
      "Epoch 128/6000, Training Loss: 49.7020, Validation Loss: 49.6584\n",
      "Epoch 129/6000, Training Loss: 49.6818, Validation Loss: 49.6383\n",
      "Epoch 130/6000, Training Loss: 49.6616, Validation Loss: 49.6181\n",
      "Epoch 131/6000, Training Loss: 49.6414, Validation Loss: 49.5979\n",
      "Epoch 132/6000, Training Loss: 49.6212, Validation Loss: 49.5777\n",
      "Epoch 133/6000, Training Loss: 49.6009, Validation Loss: 49.5575\n",
      "Epoch 134/6000, Training Loss: 49.5807, Validation Loss: 49.5373\n",
      "Epoch 135/6000, Training Loss: 49.5604, Validation Loss: 49.5170\n",
      "Epoch 136/6000, Training Loss: 49.5401, Validation Loss: 49.4967\n",
      "Epoch 137/6000, Training Loss: 49.5198, Validation Loss: 49.4765\n",
      "Epoch 138/6000, Training Loss: 49.4995, Validation Loss: 49.4562\n",
      "Epoch 139/6000, Training Loss: 49.4791, Validation Loss: 49.4359\n",
      "Epoch 140/6000, Training Loss: 49.4588, Validation Loss: 49.4156\n",
      "Epoch 141/6000, Training Loss: 49.4385, Validation Loss: 49.3954\n",
      "Epoch 142/6000, Training Loss: 49.4182, Validation Loss: 49.3751\n",
      "Epoch 143/6000, Training Loss: 49.3979, Validation Loss: 49.3548\n",
      "Epoch 144/6000, Training Loss: 49.3776, Validation Loss: 49.3346\n",
      "Epoch 145/6000, Training Loss: 49.3573, Validation Loss: 49.3143\n",
      "Epoch 146/6000, Training Loss: 49.3370, Validation Loss: 49.2941\n",
      "Epoch 147/6000, Training Loss: 49.3167, Validation Loss: 49.2739\n",
      "Epoch 148/6000, Training Loss: 49.2965, Validation Loss: 49.2537\n",
      "Epoch 149/6000, Training Loss: 49.2762, Validation Loss: 49.2334\n",
      "Epoch 150/6000, Training Loss: 49.2560, Validation Loss: 49.2132\n",
      "Epoch 151/6000, Training Loss: 49.2357, Validation Loss: 49.1931\n",
      "Epoch 152/6000, Training Loss: 49.2155, Validation Loss: 49.1729\n",
      "Epoch 153/6000, Training Loss: 49.1953, Validation Loss: 49.1527\n",
      "Epoch 154/6000, Training Loss: 49.1751, Validation Loss: 49.1326\n",
      "Epoch 155/6000, Training Loss: 49.1549, Validation Loss: 49.1125\n",
      "Epoch 156/6000, Training Loss: 49.1348, Validation Loss: 49.0924\n",
      "Epoch 157/6000, Training Loss: 49.1146, Validation Loss: 49.0722\n",
      "Epoch 158/6000, Training Loss: 49.0945, Validation Loss: 49.0521\n",
      "Epoch 159/6000, Training Loss: 49.0743, Validation Loss: 49.0321\n",
      "Epoch 160/6000, Training Loss: 49.0542, Validation Loss: 49.0120\n",
      "Epoch 161/6000, Training Loss: 49.0341, Validation Loss: 48.9919\n",
      "Epoch 162/6000, Training Loss: 49.0140, Validation Loss: 48.9718\n",
      "Epoch 163/6000, Training Loss: 48.9939, Validation Loss: 48.9518\n",
      "Epoch 164/6000, Training Loss: 48.9738, Validation Loss: 48.9317\n",
      "Epoch 165/6000, Training Loss: 48.9537, Validation Loss: 48.9117\n",
      "Epoch 166/6000, Training Loss: 48.9336, Validation Loss: 48.8916\n",
      "Epoch 167/6000, Training Loss: 48.9135, Validation Loss: 48.8716\n",
      "Epoch 168/6000, Training Loss: 48.8934, Validation Loss: 48.8516\n",
      "Epoch 169/6000, Training Loss: 48.8734, Validation Loss: 48.8315\n",
      "Epoch 170/6000, Training Loss: 48.8533, Validation Loss: 48.8115\n",
      "Epoch 171/6000, Training Loss: 48.8332, Validation Loss: 48.7915\n",
      "Epoch 172/6000, Training Loss: 48.8132, Validation Loss: 48.7715\n",
      "Epoch 173/6000, Training Loss: 48.7931, Validation Loss: 48.7514\n",
      "Epoch 174/6000, Training Loss: 48.7730, Validation Loss: 48.7314\n",
      "Epoch 175/6000, Training Loss: 48.7530, Validation Loss: 48.7114\n",
      "Epoch 176/6000, Training Loss: 48.7329, Validation Loss: 48.6914\n",
      "Epoch 177/6000, Training Loss: 48.7129, Validation Loss: 48.6714\n",
      "Epoch 178/6000, Training Loss: 48.6928, Validation Loss: 48.6514\n",
      "Epoch 179/6000, Training Loss: 48.6727, Validation Loss: 48.6314\n",
      "Epoch 180/6000, Training Loss: 48.6527, Validation Loss: 48.6114\n",
      "Epoch 181/6000, Training Loss: 48.6326, Validation Loss: 48.5914\n",
      "Epoch 182/6000, Training Loss: 48.6126, Validation Loss: 48.5714\n",
      "Epoch 183/6000, Training Loss: 48.5926, Validation Loss: 48.5513\n",
      "Epoch 184/6000, Training Loss: 48.5725, Validation Loss: 48.5314\n",
      "Epoch 185/6000, Training Loss: 48.5525, Validation Loss: 48.5114\n",
      "Epoch 186/6000, Training Loss: 48.5324, Validation Loss: 48.4914\n",
      "Epoch 187/6000, Training Loss: 48.5124, Validation Loss: 48.4714\n",
      "Epoch 188/6000, Training Loss: 48.4924, Validation Loss: 48.4514\n",
      "Epoch 189/6000, Training Loss: 48.4723, Validation Loss: 48.4314\n",
      "Epoch 190/6000, Training Loss: 48.4523, Validation Loss: 48.4114\n",
      "Epoch 191/6000, Training Loss: 48.4323, Validation Loss: 48.3914\n",
      "Epoch 192/6000, Training Loss: 48.4122, Validation Loss: 48.3714\n",
      "Epoch 193/6000, Training Loss: 48.3922, Validation Loss: 48.3514\n",
      "Epoch 194/6000, Training Loss: 48.3722, Validation Loss: 48.3315\n",
      "Epoch 195/6000, Training Loss: 48.3521, Validation Loss: 48.3115\n",
      "Epoch 196/6000, Training Loss: 48.3321, Validation Loss: 48.2915\n",
      "Epoch 197/6000, Training Loss: 48.3121, Validation Loss: 48.2715\n",
      "Epoch 198/6000, Training Loss: 48.2921, Validation Loss: 48.2515\n",
      "Epoch 199/6000, Training Loss: 48.2720, Validation Loss: 48.2316\n",
      "Epoch 200/6000, Training Loss: 48.2520, Validation Loss: 48.2116\n",
      "Epoch 201/6000, Training Loss: 48.2320, Validation Loss: 48.1916\n",
      "Epoch 202/6000, Training Loss: 48.2120, Validation Loss: 48.1716\n",
      "Epoch 203/6000, Training Loss: 48.1920, Validation Loss: 48.1516\n",
      "Epoch 204/6000, Training Loss: 48.1719, Validation Loss: 48.1317\n",
      "Epoch 205/6000, Training Loss: 48.1519, Validation Loss: 48.1117\n",
      "Epoch 206/6000, Training Loss: 48.1319, Validation Loss: 48.0917\n",
      "Epoch 207/6000, Training Loss: 48.1119, Validation Loss: 48.0718\n",
      "Epoch 208/6000, Training Loss: 48.0919, Validation Loss: 48.0518\n",
      "Epoch 209/6000, Training Loss: 48.0719, Validation Loss: 48.0318\n",
      "Epoch 210/6000, Training Loss: 48.0518, Validation Loss: 48.0118\n",
      "Epoch 211/6000, Training Loss: 48.0318, Validation Loss: 47.9919\n",
      "Epoch 212/6000, Training Loss: 48.0118, Validation Loss: 47.9719\n",
      "Epoch 213/6000, Training Loss: 47.9918, Validation Loss: 47.9519\n",
      "Epoch 214/6000, Training Loss: 47.9718, Validation Loss: 47.9319\n",
      "Epoch 215/6000, Training Loss: 47.9518, Validation Loss: 47.9120\n",
      "Epoch 216/6000, Training Loss: 47.9318, Validation Loss: 47.8920\n",
      "Epoch 217/6000, Training Loss: 47.9117, Validation Loss: 47.8720\n",
      "Epoch 218/6000, Training Loss: 47.8917, Validation Loss: 47.8521\n",
      "Epoch 219/6000, Training Loss: 47.8717, Validation Loss: 47.8321\n",
      "Epoch 220/6000, Training Loss: 47.8517, Validation Loss: 47.8121\n",
      "Epoch 221/6000, Training Loss: 47.8317, Validation Loss: 47.7922\n",
      "Epoch 222/6000, Training Loss: 47.8117, Validation Loss: 47.7722\n",
      "Epoch 223/6000, Training Loss: 47.7917, Validation Loss: 47.7522\n",
      "Epoch 224/6000, Training Loss: 47.7716, Validation Loss: 47.7323\n",
      "Epoch 225/6000, Training Loss: 47.7516, Validation Loss: 47.7123\n",
      "Epoch 226/6000, Training Loss: 47.7316, Validation Loss: 47.6923\n",
      "Epoch 227/6000, Training Loss: 47.7116, Validation Loss: 47.6723\n",
      "Epoch 228/6000, Training Loss: 47.6916, Validation Loss: 47.6524\n",
      "Epoch 229/6000, Training Loss: 47.6716, Validation Loss: 47.6324\n",
      "Epoch 230/6000, Training Loss: 47.6516, Validation Loss: 47.6124\n",
      "Epoch 231/6000, Training Loss: 47.6316, Validation Loss: 47.5925\n",
      "Epoch 232/6000, Training Loss: 47.6116, Validation Loss: 47.5725\n",
      "Epoch 233/6000, Training Loss: 47.5916, Validation Loss: 47.5525\n",
      "Epoch 234/6000, Training Loss: 47.5715, Validation Loss: 47.5326\n",
      "Epoch 235/6000, Training Loss: 47.5515, Validation Loss: 47.5126\n",
      "Epoch 236/6000, Training Loss: 47.5315, Validation Loss: 47.4926\n",
      "Epoch 237/6000, Training Loss: 47.5115, Validation Loss: 47.4727\n",
      "Epoch 238/6000, Training Loss: 47.4915, Validation Loss: 47.4527\n",
      "Epoch 239/6000, Training Loss: 47.4715, Validation Loss: 47.4327\n",
      "Epoch 240/6000, Training Loss: 47.4515, Validation Loss: 47.4128\n",
      "Epoch 241/6000, Training Loss: 47.4315, Validation Loss: 47.3928\n",
      "Epoch 242/6000, Training Loss: 47.4115, Validation Loss: 47.3728\n",
      "Epoch 243/6000, Training Loss: 47.3915, Validation Loss: 47.3529\n",
      "Epoch 244/6000, Training Loss: 47.3714, Validation Loss: 47.3329\n",
      "Epoch 245/6000, Training Loss: 47.3514, Validation Loss: 47.3130\n",
      "Epoch 246/6000, Training Loss: 47.3314, Validation Loss: 47.2930\n",
      "Epoch 247/6000, Training Loss: 47.3114, Validation Loss: 47.2730\n",
      "Epoch 248/6000, Training Loss: 47.2914, Validation Loss: 47.2531\n",
      "Epoch 249/6000, Training Loss: 47.2714, Validation Loss: 47.2331\n",
      "Epoch 250/6000, Training Loss: 47.2514, Validation Loss: 47.2131\n",
      "Epoch 251/6000, Training Loss: 47.2314, Validation Loss: 47.1931\n",
      "Epoch 252/6000, Training Loss: 47.2114, Validation Loss: 47.1732\n",
      "Epoch 253/6000, Training Loss: 47.1914, Validation Loss: 47.1532\n",
      "Epoch 254/6000, Training Loss: 47.1714, Validation Loss: 47.1333\n",
      "Epoch 255/6000, Training Loss: 47.1513, Validation Loss: 47.1133\n",
      "Epoch 256/6000, Training Loss: 47.1313, Validation Loss: 47.0933\n",
      "Epoch 257/6000, Training Loss: 47.1113, Validation Loss: 47.0734\n",
      "Epoch 258/6000, Training Loss: 47.0913, Validation Loss: 47.0534\n",
      "Epoch 259/6000, Training Loss: 47.0713, Validation Loss: 47.0334\n",
      "Epoch 260/6000, Training Loss: 47.0513, Validation Loss: 47.0135\n",
      "Epoch 261/6000, Training Loss: 47.0313, Validation Loss: 46.9935\n",
      "Epoch 262/6000, Training Loss: 47.0113, Validation Loss: 46.9735\n",
      "Epoch 263/6000, Training Loss: 46.9913, Validation Loss: 46.9536\n",
      "Epoch 264/6000, Training Loss: 46.9713, Validation Loss: 46.9336\n",
      "Epoch 265/6000, Training Loss: 46.9513, Validation Loss: 46.9136\n",
      "Epoch 266/6000, Training Loss: 46.9313, Validation Loss: 46.8937\n",
      "Epoch 267/6000, Training Loss: 46.9112, Validation Loss: 46.8737\n",
      "Epoch 268/6000, Training Loss: 46.8912, Validation Loss: 46.8537\n",
      "Epoch 269/6000, Training Loss: 46.8712, Validation Loss: 46.8338\n",
      "Epoch 270/6000, Training Loss: 46.8512, Validation Loss: 46.8138\n",
      "Epoch 271/6000, Training Loss: 46.8312, Validation Loss: 46.7938\n",
      "Epoch 272/6000, Training Loss: 46.8112, Validation Loss: 46.7739\n",
      "Epoch 273/6000, Training Loss: 46.7912, Validation Loss: 46.7539\n",
      "Epoch 274/6000, Training Loss: 46.7712, Validation Loss: 46.7339\n",
      "Epoch 275/6000, Training Loss: 46.7512, Validation Loss: 46.7140\n",
      "Epoch 276/6000, Training Loss: 46.7312, Validation Loss: 46.6940\n",
      "Epoch 277/6000, Training Loss: 46.7112, Validation Loss: 46.6740\n",
      "Epoch 278/6000, Training Loss: 46.6911, Validation Loss: 46.6541\n",
      "Epoch 279/6000, Training Loss: 46.6711, Validation Loss: 46.6341\n",
      "Epoch 280/6000, Training Loss: 46.6511, Validation Loss: 46.6142\n",
      "Epoch 281/6000, Training Loss: 46.6311, Validation Loss: 46.5942\n",
      "Epoch 282/6000, Training Loss: 46.6111, Validation Loss: 46.5742\n",
      "Epoch 283/6000, Training Loss: 46.5911, Validation Loss: 46.5543\n",
      "Epoch 284/6000, Training Loss: 46.5711, Validation Loss: 46.5343\n",
      "Epoch 285/6000, Training Loss: 46.5511, Validation Loss: 46.5143\n",
      "Epoch 286/6000, Training Loss: 46.5311, Validation Loss: 46.4944\n",
      "Epoch 287/6000, Training Loss: 46.5111, Validation Loss: 46.4744\n",
      "Epoch 288/6000, Training Loss: 46.4911, Validation Loss: 46.4544\n",
      "Epoch 289/6000, Training Loss: 46.4711, Validation Loss: 46.4345\n",
      "Epoch 290/6000, Training Loss: 46.4510, Validation Loss: 46.4145\n",
      "Epoch 291/6000, Training Loss: 46.4310, Validation Loss: 46.3945\n",
      "Epoch 292/6000, Training Loss: 46.4110, Validation Loss: 46.3746\n",
      "Epoch 293/6000, Training Loss: 46.3910, Validation Loss: 46.3546\n",
      "Epoch 294/6000, Training Loss: 46.3710, Validation Loss: 46.3346\n",
      "Epoch 295/6000, Training Loss: 46.3510, Validation Loss: 46.3147\n",
      "Epoch 296/6000, Training Loss: 46.3310, Validation Loss: 46.2947\n",
      "Epoch 297/6000, Training Loss: 46.3110, Validation Loss: 46.2747\n",
      "Epoch 298/6000, Training Loss: 46.2910, Validation Loss: 46.2548\n",
      "Epoch 299/6000, Training Loss: 46.2710, Validation Loss: 46.2348\n",
      "Epoch 300/6000, Training Loss: 46.2510, Validation Loss: 46.2148\n",
      "Epoch 301/6000, Training Loss: 46.2309, Validation Loss: 46.1949\n",
      "Epoch 302/6000, Training Loss: 46.2109, Validation Loss: 46.1749\n",
      "Epoch 303/6000, Training Loss: 46.1909, Validation Loss: 46.1549\n",
      "Epoch 304/6000, Training Loss: 46.1709, Validation Loss: 46.1350\n",
      "Epoch 305/6000, Training Loss: 46.1509, Validation Loss: 46.1150\n",
      "Epoch 306/6000, Training Loss: 46.1309, Validation Loss: 46.0951\n",
      "Epoch 307/6000, Training Loss: 46.1109, Validation Loss: 46.0751\n",
      "Epoch 308/6000, Training Loss: 46.0909, Validation Loss: 46.0551\n",
      "Epoch 309/6000, Training Loss: 46.0709, Validation Loss: 46.0352\n",
      "Epoch 310/6000, Training Loss: 46.0509, Validation Loss: 46.0152\n",
      "Epoch 311/6000, Training Loss: 46.0309, Validation Loss: 45.9952\n",
      "Epoch 312/6000, Training Loss: 46.0108, Validation Loss: 45.9753\n",
      "Epoch 313/6000, Training Loss: 45.9908, Validation Loss: 45.9553\n",
      "Epoch 314/6000, Training Loss: 45.9708, Validation Loss: 45.9353\n",
      "Epoch 315/6000, Training Loss: 45.9508, Validation Loss: 45.9154\n",
      "Epoch 316/6000, Training Loss: 45.9308, Validation Loss: 45.8954\n",
      "Epoch 317/6000, Training Loss: 45.9108, Validation Loss: 45.8754\n",
      "Epoch 318/6000, Training Loss: 45.8908, Validation Loss: 45.8555\n",
      "Epoch 319/6000, Training Loss: 45.8708, Validation Loss: 45.8355\n",
      "Epoch 320/6000, Training Loss: 45.8508, Validation Loss: 45.8155\n",
      "Epoch 321/6000, Training Loss: 45.8308, Validation Loss: 45.7956\n",
      "Epoch 322/6000, Training Loss: 45.8108, Validation Loss: 45.7756\n",
      "Epoch 323/6000, Training Loss: 45.7908, Validation Loss: 45.7556\n",
      "Epoch 324/6000, Training Loss: 45.7707, Validation Loss: 45.7357\n",
      "Epoch 325/6000, Training Loss: 45.7507, Validation Loss: 45.7157\n",
      "Epoch 326/6000, Training Loss: 45.7307, Validation Loss: 45.6957\n",
      "Epoch 327/6000, Training Loss: 45.7107, Validation Loss: 45.6758\n",
      "Epoch 328/6000, Training Loss: 45.6907, Validation Loss: 45.6558\n",
      "Epoch 329/6000, Training Loss: 45.6707, Validation Loss: 45.6358\n",
      "Epoch 330/6000, Training Loss: 45.6507, Validation Loss: 45.6159\n",
      "Epoch 331/6000, Training Loss: 45.6307, Validation Loss: 45.5959\n",
      "Epoch 332/6000, Training Loss: 45.6107, Validation Loss: 45.5760\n",
      "Epoch 333/6000, Training Loss: 45.5907, Validation Loss: 45.5560\n",
      "Epoch 334/6000, Training Loss: 45.5707, Validation Loss: 45.5360\n",
      "Epoch 335/6000, Training Loss: 45.5507, Validation Loss: 45.5161\n",
      "Epoch 336/6000, Training Loss: 45.5306, Validation Loss: 45.4961\n",
      "Epoch 337/6000, Training Loss: 45.5106, Validation Loss: 45.4761\n",
      "Epoch 338/6000, Training Loss: 45.4906, Validation Loss: 45.4562\n",
      "Epoch 339/6000, Training Loss: 45.4706, Validation Loss: 45.4362\n",
      "Epoch 340/6000, Training Loss: 45.4506, Validation Loss: 45.4162\n",
      "Epoch 341/6000, Training Loss: 45.4306, Validation Loss: 45.3963\n",
      "Epoch 342/6000, Training Loss: 45.4106, Validation Loss: 45.3763\n",
      "Epoch 343/6000, Training Loss: 45.3906, Validation Loss: 45.3563\n",
      "Epoch 344/6000, Training Loss: 45.3706, Validation Loss: 45.3364\n",
      "Epoch 345/6000, Training Loss: 45.3506, Validation Loss: 45.3164\n",
      "Epoch 346/6000, Training Loss: 45.3306, Validation Loss: 45.2964\n",
      "Epoch 347/6000, Training Loss: 45.3105, Validation Loss: 45.2765\n",
      "Epoch 348/6000, Training Loss: 45.2905, Validation Loss: 45.2565\n",
      "Epoch 349/6000, Training Loss: 45.2705, Validation Loss: 45.2365\n",
      "Epoch 350/6000, Training Loss: 45.2505, Validation Loss: 45.2166\n",
      "Epoch 351/6000, Training Loss: 45.2305, Validation Loss: 45.1966\n",
      "Epoch 352/6000, Training Loss: 45.2105, Validation Loss: 45.1766\n",
      "Epoch 353/6000, Training Loss: 45.1905, Validation Loss: 45.1567\n",
      "Epoch 354/6000, Training Loss: 45.1705, Validation Loss: 45.1367\n",
      "Epoch 355/6000, Training Loss: 45.1505, Validation Loss: 45.1167\n",
      "Epoch 356/6000, Training Loss: 45.1305, Validation Loss: 45.0968\n",
      "Epoch 357/6000, Training Loss: 45.1105, Validation Loss: 45.0768\n",
      "Epoch 358/6000, Training Loss: 45.0905, Validation Loss: 45.0569\n",
      "Epoch 359/6000, Training Loss: 45.0704, Validation Loss: 45.0369\n",
      "Epoch 360/6000, Training Loss: 45.0504, Validation Loss: 45.0169\n",
      "Epoch 361/6000, Training Loss: 45.0304, Validation Loss: 44.9970\n",
      "Epoch 362/6000, Training Loss: 45.0104, Validation Loss: 44.9770\n",
      "Epoch 363/6000, Training Loss: 44.9904, Validation Loss: 44.9570\n",
      "Epoch 364/6000, Training Loss: 44.9704, Validation Loss: 44.9371\n",
      "Epoch 365/6000, Training Loss: 44.9504, Validation Loss: 44.9171\n",
      "Epoch 366/6000, Training Loss: 44.9304, Validation Loss: 44.8971\n",
      "Epoch 367/6000, Training Loss: 44.9104, Validation Loss: 44.8772\n",
      "Epoch 368/6000, Training Loss: 44.8904, Validation Loss: 44.8572\n",
      "Epoch 369/6000, Training Loss: 44.8704, Validation Loss: 44.8372\n",
      "Epoch 370/6000, Training Loss: 44.8504, Validation Loss: 44.8173\n",
      "Epoch 371/6000, Training Loss: 44.8303, Validation Loss: 44.7973\n",
      "Epoch 372/6000, Training Loss: 44.8103, Validation Loss: 44.7773\n",
      "Epoch 373/6000, Training Loss: 44.7903, Validation Loss: 44.7574\n",
      "Epoch 374/6000, Training Loss: 44.7703, Validation Loss: 44.7374\n",
      "Epoch 375/6000, Training Loss: 44.7503, Validation Loss: 44.7174\n",
      "Epoch 376/6000, Training Loss: 44.7303, Validation Loss: 44.6975\n",
      "Epoch 377/6000, Training Loss: 44.7103, Validation Loss: 44.6775\n",
      "Epoch 378/6000, Training Loss: 44.6903, Validation Loss: 44.6575\n",
      "Epoch 379/6000, Training Loss: 44.6703, Validation Loss: 44.6376\n",
      "Epoch 380/6000, Training Loss: 44.6503, Validation Loss: 44.6176\n",
      "Epoch 381/6000, Training Loss: 44.6303, Validation Loss: 44.5976\n",
      "Epoch 382/6000, Training Loss: 44.6102, Validation Loss: 44.5777\n",
      "Epoch 383/6000, Training Loss: 44.5902, Validation Loss: 44.5577\n",
      "Epoch 384/6000, Training Loss: 44.5702, Validation Loss: 44.5378\n",
      "Epoch 385/6000, Training Loss: 44.5502, Validation Loss: 44.5178\n",
      "Epoch 386/6000, Training Loss: 44.5302, Validation Loss: 44.4978\n",
      "Epoch 387/6000, Training Loss: 44.5102, Validation Loss: 44.4779\n",
      "Epoch 388/6000, Training Loss: 44.4902, Validation Loss: 44.4579\n",
      "Epoch 389/6000, Training Loss: 44.4702, Validation Loss: 44.4379\n",
      "Epoch 390/6000, Training Loss: 44.4502, Validation Loss: 44.4180\n",
      "Epoch 391/6000, Training Loss: 44.4302, Validation Loss: 44.3980\n",
      "Epoch 392/6000, Training Loss: 44.4102, Validation Loss: 44.3780\n",
      "Epoch 393/6000, Training Loss: 44.3901, Validation Loss: 44.3581\n",
      "Epoch 394/6000, Training Loss: 44.3701, Validation Loss: 44.3381\n",
      "Epoch 395/6000, Training Loss: 44.3501, Validation Loss: 44.3181\n",
      "Epoch 396/6000, Training Loss: 44.3301, Validation Loss: 44.2982\n",
      "Epoch 397/6000, Training Loss: 44.3101, Validation Loss: 44.2782\n",
      "Epoch 398/6000, Training Loss: 44.2901, Validation Loss: 44.2582\n",
      "Epoch 399/6000, Training Loss: 44.2701, Validation Loss: 44.2383\n",
      "Epoch 400/6000, Training Loss: 44.2501, Validation Loss: 44.2183\n",
      "Epoch 401/6000, Training Loss: 44.2301, Validation Loss: 44.1983\n",
      "Epoch 402/6000, Training Loss: 44.2101, Validation Loss: 44.1784\n",
      "Epoch 403/6000, Training Loss: 44.1901, Validation Loss: 44.1584\n",
      "Epoch 404/6000, Training Loss: 44.1701, Validation Loss: 44.1384\n",
      "Epoch 405/6000, Training Loss: 44.1500, Validation Loss: 44.1185\n",
      "Epoch 406/6000, Training Loss: 44.1300, Validation Loss: 44.0985\n",
      "Epoch 407/6000, Training Loss: 44.1100, Validation Loss: 44.0785\n",
      "Epoch 408/6000, Training Loss: 44.0900, Validation Loss: 44.0586\n",
      "Epoch 409/6000, Training Loss: 44.0700, Validation Loss: 44.0386\n",
      "Epoch 410/6000, Training Loss: 44.0500, Validation Loss: 44.0187\n",
      "Epoch 411/6000, Training Loss: 44.0300, Validation Loss: 43.9987\n",
      "Epoch 412/6000, Training Loss: 44.0100, Validation Loss: 43.9787\n",
      "Epoch 413/6000, Training Loss: 43.9900, Validation Loss: 43.9588\n",
      "Epoch 414/6000, Training Loss: 43.9700, Validation Loss: 43.9388\n",
      "Epoch 415/6000, Training Loss: 43.9500, Validation Loss: 43.9188\n",
      "Epoch 416/6000, Training Loss: 43.9299, Validation Loss: 43.8989\n",
      "Epoch 417/6000, Training Loss: 43.9099, Validation Loss: 43.8789\n",
      "Epoch 418/6000, Training Loss: 43.8899, Validation Loss: 43.8589\n",
      "Epoch 419/6000, Training Loss: 43.8699, Validation Loss: 43.8390\n",
      "Epoch 420/6000, Training Loss: 43.8499, Validation Loss: 43.8190\n",
      "Epoch 421/6000, Training Loss: 43.8299, Validation Loss: 43.7990\n",
      "Epoch 422/6000, Training Loss: 43.8099, Validation Loss: 43.7791\n",
      "Epoch 423/6000, Training Loss: 43.7899, Validation Loss: 43.7591\n",
      "Epoch 424/6000, Training Loss: 43.7699, Validation Loss: 43.7391\n",
      "Epoch 425/6000, Training Loss: 43.7499, Validation Loss: 43.7192\n",
      "Epoch 426/6000, Training Loss: 43.7299, Validation Loss: 43.6992\n",
      "Epoch 427/6000, Training Loss: 43.7099, Validation Loss: 43.6792\n",
      "Epoch 428/6000, Training Loss: 43.6898, Validation Loss: 43.6593\n",
      "Epoch 429/6000, Training Loss: 43.6698, Validation Loss: 43.6393\n",
      "Epoch 430/6000, Training Loss: 43.6498, Validation Loss: 43.6193\n",
      "Epoch 431/6000, Training Loss: 43.6298, Validation Loss: 43.5994\n",
      "Epoch 432/6000, Training Loss: 43.6098, Validation Loss: 43.5794\n",
      "Epoch 433/6000, Training Loss: 43.5898, Validation Loss: 43.5594\n",
      "Epoch 434/6000, Training Loss: 43.5698, Validation Loss: 43.5395\n",
      "Epoch 435/6000, Training Loss: 43.5498, Validation Loss: 43.5195\n",
      "Epoch 436/6000, Training Loss: 43.5298, Validation Loss: 43.4995\n",
      "Epoch 437/6000, Training Loss: 43.5098, Validation Loss: 43.4796\n",
      "Epoch 438/6000, Training Loss: 43.4898, Validation Loss: 43.4596\n",
      "Epoch 439/6000, Training Loss: 43.4698, Validation Loss: 43.4397\n",
      "Epoch 440/6000, Training Loss: 43.4497, Validation Loss: 43.4197\n",
      "Epoch 441/6000, Training Loss: 43.4297, Validation Loss: 43.3997\n",
      "Epoch 442/6000, Training Loss: 43.4097, Validation Loss: 43.3798\n",
      "Epoch 443/6000, Training Loss: 43.3897, Validation Loss: 43.3598\n",
      "Epoch 444/6000, Training Loss: 43.3697, Validation Loss: 43.3398\n",
      "Epoch 445/6000, Training Loss: 43.3497, Validation Loss: 43.3199\n",
      "Epoch 446/6000, Training Loss: 43.3297, Validation Loss: 43.2999\n",
      "Epoch 447/6000, Training Loss: 43.3097, Validation Loss: 43.2799\n",
      "Epoch 448/6000, Training Loss: 43.2897, Validation Loss: 43.2600\n",
      "Epoch 449/6000, Training Loss: 43.2697, Validation Loss: 43.2400\n",
      "Epoch 450/6000, Training Loss: 43.2497, Validation Loss: 43.2200\n",
      "Epoch 451/6000, Training Loss: 43.2296, Validation Loss: 43.2001\n",
      "Epoch 452/6000, Training Loss: 43.2096, Validation Loss: 43.1801\n",
      "Epoch 453/6000, Training Loss: 43.1896, Validation Loss: 43.1601\n",
      "Epoch 454/6000, Training Loss: 43.1696, Validation Loss: 43.1402\n",
      "Epoch 455/6000, Training Loss: 43.1496, Validation Loss: 43.1202\n",
      "Epoch 456/6000, Training Loss: 43.1296, Validation Loss: 43.1002\n",
      "Epoch 457/6000, Training Loss: 43.1096, Validation Loss: 43.0803\n",
      "Epoch 458/6000, Training Loss: 43.0896, Validation Loss: 43.0603\n",
      "Epoch 459/6000, Training Loss: 43.0696, Validation Loss: 43.0403\n",
      "Epoch 460/6000, Training Loss: 43.0496, Validation Loss: 43.0204\n",
      "Epoch 461/6000, Training Loss: 43.0296, Validation Loss: 43.0004\n",
      "Epoch 462/6000, Training Loss: 43.0095, Validation Loss: 42.9804\n",
      "Epoch 463/6000, Training Loss: 42.9895, Validation Loss: 42.9605\n",
      "Epoch 464/6000, Training Loss: 42.9695, Validation Loss: 42.9405\n",
      "Epoch 465/6000, Training Loss: 42.9495, Validation Loss: 42.9206\n",
      "Epoch 466/6000, Training Loss: 42.9295, Validation Loss: 42.9006\n",
      "Epoch 467/6000, Training Loss: 42.9095, Validation Loss: 42.8806\n",
      "Epoch 468/6000, Training Loss: 42.8895, Validation Loss: 42.8607\n",
      "Epoch 469/6000, Training Loss: 42.8695, Validation Loss: 42.8407\n",
      "Epoch 470/6000, Training Loss: 42.8495, Validation Loss: 42.8207\n",
      "Epoch 471/6000, Training Loss: 42.8295, Validation Loss: 42.8008\n",
      "Epoch 472/6000, Training Loss: 42.8095, Validation Loss: 42.7808\n",
      "Epoch 473/6000, Training Loss: 42.7895, Validation Loss: 42.7608\n",
      "Epoch 474/6000, Training Loss: 42.7694, Validation Loss: 42.7409\n",
      "Epoch 475/6000, Training Loss: 42.7494, Validation Loss: 42.7209\n",
      "Epoch 476/6000, Training Loss: 42.7294, Validation Loss: 42.7009\n",
      "Epoch 477/6000, Training Loss: 42.7094, Validation Loss: 42.6810\n",
      "Epoch 478/6000, Training Loss: 42.6894, Validation Loss: 42.6610\n",
      "Epoch 479/6000, Training Loss: 42.6694, Validation Loss: 42.6410\n",
      "Epoch 480/6000, Training Loss: 42.6494, Validation Loss: 42.6211\n",
      "Epoch 481/6000, Training Loss: 42.6294, Validation Loss: 42.6011\n",
      "Epoch 482/6000, Training Loss: 42.6094, Validation Loss: 42.5811\n",
      "Epoch 483/6000, Training Loss: 42.5894, Validation Loss: 42.5612\n",
      "Epoch 484/6000, Training Loss: 42.5694, Validation Loss: 42.5412\n",
      "Epoch 485/6000, Training Loss: 42.5493, Validation Loss: 42.5212\n",
      "Epoch 486/6000, Training Loss: 42.5293, Validation Loss: 42.5013\n",
      "Epoch 487/6000, Training Loss: 42.5093, Validation Loss: 42.4813\n",
      "Epoch 488/6000, Training Loss: 42.4893, Validation Loss: 42.4613\n",
      "Epoch 489/6000, Training Loss: 42.4693, Validation Loss: 42.4414\n",
      "Epoch 490/6000, Training Loss: 42.4493, Validation Loss: 42.4214\n",
      "Epoch 491/6000, Training Loss: 42.4293, Validation Loss: 42.4014\n",
      "Epoch 492/6000, Training Loss: 42.4093, Validation Loss: 42.3815\n",
      "Epoch 493/6000, Training Loss: 42.3893, Validation Loss: 42.3615\n",
      "Epoch 494/6000, Training Loss: 42.3693, Validation Loss: 42.3416\n",
      "Epoch 495/6000, Training Loss: 42.3493, Validation Loss: 42.3216\n",
      "Epoch 496/6000, Training Loss: 42.3293, Validation Loss: 42.3016\n",
      "Epoch 497/6000, Training Loss: 42.3092, Validation Loss: 42.2817\n",
      "Epoch 498/6000, Training Loss: 42.2892, Validation Loss: 42.2617\n",
      "Epoch 499/6000, Training Loss: 42.2692, Validation Loss: 42.2417\n",
      "Epoch 500/6000, Training Loss: 42.2492, Validation Loss: 42.2218\n",
      "Epoch 501/6000, Training Loss: 42.2292, Validation Loss: 42.2018\n",
      "Epoch 502/6000, Training Loss: 42.2092, Validation Loss: 42.1818\n",
      "Epoch 503/6000, Training Loss: 42.1892, Validation Loss: 42.1619\n",
      "Epoch 504/6000, Training Loss: 42.1692, Validation Loss: 42.1419\n",
      "Epoch 505/6000, Training Loss: 42.1492, Validation Loss: 42.1219\n",
      "Epoch 506/6000, Training Loss: 42.1292, Validation Loss: 42.1020\n",
      "Epoch 507/6000, Training Loss: 42.1092, Validation Loss: 42.0820\n",
      "Epoch 508/6000, Training Loss: 42.0891, Validation Loss: 42.0620\n",
      "Epoch 509/6000, Training Loss: 42.0691, Validation Loss: 42.0421\n",
      "Epoch 510/6000, Training Loss: 42.0491, Validation Loss: 42.0221\n",
      "Epoch 511/6000, Training Loss: 42.0291, Validation Loss: 42.0021\n",
      "Epoch 512/6000, Training Loss: 42.0091, Validation Loss: 41.9822\n",
      "Epoch 513/6000, Training Loss: 41.9891, Validation Loss: 41.9622\n",
      "Epoch 514/6000, Training Loss: 41.9691, Validation Loss: 41.9422\n",
      "Epoch 515/6000, Training Loss: 41.9491, Validation Loss: 41.9223\n",
      "Epoch 516/6000, Training Loss: 41.9291, Validation Loss: 41.9023\n",
      "Epoch 517/6000, Training Loss: 41.9091, Validation Loss: 41.8824\n",
      "Epoch 518/6000, Training Loss: 41.8891, Validation Loss: 41.8624\n",
      "Epoch 519/6000, Training Loss: 41.8691, Validation Loss: 41.8424\n",
      "Epoch 520/6000, Training Loss: 41.8490, Validation Loss: 41.8224\n",
      "Epoch 521/6000, Training Loss: 41.8290, Validation Loss: 41.8025\n",
      "Epoch 522/6000, Training Loss: 41.8090, Validation Loss: 41.7825\n",
      "Epoch 523/6000, Training Loss: 41.7890, Validation Loss: 41.7626\n",
      "Epoch 524/6000, Training Loss: 41.7690, Validation Loss: 41.7426\n",
      "Epoch 525/6000, Training Loss: 41.7490, Validation Loss: 41.7226\n",
      "Epoch 526/6000, Training Loss: 41.7290, Validation Loss: 41.7027\n",
      "Epoch 527/6000, Training Loss: 41.7090, Validation Loss: 41.6827\n",
      "Epoch 528/6000, Training Loss: 41.6890, Validation Loss: 41.6627\n",
      "Epoch 529/6000, Training Loss: 41.6690, Validation Loss: 41.6428\n",
      "Epoch 530/6000, Training Loss: 41.6490, Validation Loss: 41.6228\n",
      "Epoch 531/6000, Training Loss: 41.6289, Validation Loss: 41.6028\n",
      "Epoch 532/6000, Training Loss: 41.6089, Validation Loss: 41.5829\n",
      "Epoch 533/6000, Training Loss: 41.5889, Validation Loss: 41.5629\n",
      "Epoch 534/6000, Training Loss: 41.5689, Validation Loss: 41.5429\n",
      "Epoch 535/6000, Training Loss: 41.5489, Validation Loss: 41.5230\n",
      "Epoch 536/6000, Training Loss: 41.5289, Validation Loss: 41.5030\n",
      "Epoch 537/6000, Training Loss: 41.5089, Validation Loss: 41.4830\n",
      "Epoch 538/6000, Training Loss: 41.4889, Validation Loss: 41.4631\n",
      "Epoch 539/6000, Training Loss: 41.4689, Validation Loss: 41.4431\n",
      "Epoch 540/6000, Training Loss: 41.4489, Validation Loss: 41.4231\n",
      "Epoch 541/6000, Training Loss: 41.4289, Validation Loss: 41.4032\n",
      "Epoch 542/6000, Training Loss: 41.4089, Validation Loss: 41.3832\n",
      "Epoch 543/6000, Training Loss: 41.3888, Validation Loss: 41.3632\n",
      "Epoch 544/6000, Training Loss: 41.3688, Validation Loss: 41.3433\n",
      "Epoch 545/6000, Training Loss: 41.3488, Validation Loss: 41.3233\n",
      "Epoch 546/6000, Training Loss: 41.3288, Validation Loss: 41.3034\n",
      "Epoch 547/6000, Training Loss: 41.3088, Validation Loss: 41.2834\n",
      "Epoch 548/6000, Training Loss: 41.2888, Validation Loss: 41.2634\n",
      "Epoch 549/6000, Training Loss: 41.2688, Validation Loss: 41.2435\n",
      "Epoch 550/6000, Training Loss: 41.2488, Validation Loss: 41.2235\n",
      "Epoch 551/6000, Training Loss: 41.2288, Validation Loss: 41.2035\n",
      "Epoch 552/6000, Training Loss: 41.2088, Validation Loss: 41.1836\n",
      "Epoch 553/6000, Training Loss: 41.1888, Validation Loss: 41.1636\n",
      "Epoch 554/6000, Training Loss: 41.1687, Validation Loss: 41.1436\n",
      "Epoch 555/6000, Training Loss: 41.1487, Validation Loss: 41.1237\n",
      "Epoch 556/6000, Training Loss: 41.1287, Validation Loss: 41.1037\n",
      "Epoch 557/6000, Training Loss: 41.1087, Validation Loss: 41.0837\n",
      "Epoch 558/6000, Training Loss: 41.0887, Validation Loss: 41.0638\n",
      "Epoch 559/6000, Training Loss: 41.0687, Validation Loss: 41.0438\n",
      "Epoch 560/6000, Training Loss: 41.0487, Validation Loss: 41.0238\n",
      "Epoch 561/6000, Training Loss: 41.0287, Validation Loss: 41.0039\n",
      "Epoch 562/6000, Training Loss: 41.0087, Validation Loss: 40.9839\n",
      "Epoch 563/6000, Training Loss: 40.9887, Validation Loss: 40.9639\n",
      "Epoch 564/6000, Training Loss: 40.9687, Validation Loss: 40.9440\n",
      "Epoch 565/6000, Training Loss: 40.9486, Validation Loss: 40.9240\n",
      "Epoch 566/6000, Training Loss: 40.9286, Validation Loss: 40.9040\n",
      "Epoch 567/6000, Training Loss: 40.9086, Validation Loss: 40.8841\n",
      "Epoch 568/6000, Training Loss: 40.8886, Validation Loss: 40.8641\n",
      "Epoch 569/6000, Training Loss: 40.8686, Validation Loss: 40.8441\n",
      "Epoch 570/6000, Training Loss: 40.8486, Validation Loss: 40.8242\n",
      "Epoch 571/6000, Training Loss: 40.8286, Validation Loss: 40.8042\n",
      "Epoch 572/6000, Training Loss: 40.8086, Validation Loss: 40.7842\n",
      "Epoch 573/6000, Training Loss: 40.7886, Validation Loss: 40.7643\n",
      "Epoch 574/6000, Training Loss: 40.7686, Validation Loss: 40.7443\n",
      "Epoch 575/6000, Training Loss: 40.7486, Validation Loss: 40.7244\n",
      "Epoch 576/6000, Training Loss: 40.7286, Validation Loss: 40.7044\n",
      "Epoch 577/6000, Training Loss: 40.7085, Validation Loss: 40.6844\n",
      "Epoch 578/6000, Training Loss: 40.6885, Validation Loss: 40.6645\n",
      "Epoch 579/6000, Training Loss: 40.6685, Validation Loss: 40.6445\n",
      "Epoch 580/6000, Training Loss: 40.6485, Validation Loss: 40.6245\n",
      "Epoch 581/6000, Training Loss: 40.6285, Validation Loss: 40.6046\n",
      "Epoch 582/6000, Training Loss: 40.6085, Validation Loss: 40.5846\n",
      "Epoch 583/6000, Training Loss: 40.5885, Validation Loss: 40.5646\n",
      "Epoch 584/6000, Training Loss: 40.5685, Validation Loss: 40.5447\n",
      "Epoch 585/6000, Training Loss: 40.5485, Validation Loss: 40.5247\n",
      "Epoch 586/6000, Training Loss: 40.5285, Validation Loss: 40.5047\n",
      "Epoch 587/6000, Training Loss: 40.5085, Validation Loss: 40.4848\n",
      "Epoch 588/6000, Training Loss: 40.4885, Validation Loss: 40.4648\n",
      "Epoch 589/6000, Training Loss: 40.4684, Validation Loss: 40.4448\n",
      "Epoch 590/6000, Training Loss: 40.4484, Validation Loss: 40.4249\n",
      "Epoch 591/6000, Training Loss: 40.4284, Validation Loss: 40.4049\n",
      "Epoch 592/6000, Training Loss: 40.4084, Validation Loss: 40.3849\n",
      "Epoch 593/6000, Training Loss: 40.3884, Validation Loss: 40.3650\n",
      "Epoch 594/6000, Training Loss: 40.3684, Validation Loss: 40.3450\n",
      "Epoch 595/6000, Training Loss: 40.3484, Validation Loss: 40.3250\n",
      "Epoch 596/6000, Training Loss: 40.3284, Validation Loss: 40.3051\n",
      "Epoch 597/6000, Training Loss: 40.3084, Validation Loss: 40.2851\n",
      "Epoch 598/6000, Training Loss: 40.2884, Validation Loss: 40.2652\n",
      "Epoch 599/6000, Training Loss: 40.2684, Validation Loss: 40.2452\n",
      "Epoch 600/6000, Training Loss: 40.2483, Validation Loss: 40.2252\n",
      "Epoch 601/6000, Training Loss: 40.2283, Validation Loss: 40.2053\n",
      "Epoch 602/6000, Training Loss: 40.2083, Validation Loss: 40.1853\n",
      "Epoch 603/6000, Training Loss: 40.1883, Validation Loss: 40.1653\n",
      "Epoch 604/6000, Training Loss: 40.1683, Validation Loss: 40.1454\n",
      "Epoch 605/6000, Training Loss: 40.1483, Validation Loss: 40.1254\n",
      "Epoch 606/6000, Training Loss: 40.1283, Validation Loss: 40.1054\n",
      "Epoch 607/6000, Training Loss: 40.1083, Validation Loss: 40.0855\n",
      "Epoch 608/6000, Training Loss: 40.0883, Validation Loss: 40.0655\n",
      "Epoch 609/6000, Training Loss: 40.0683, Validation Loss: 40.0455\n",
      "Epoch 610/6000, Training Loss: 40.0483, Validation Loss: 40.0256\n",
      "Epoch 611/6000, Training Loss: 40.0283, Validation Loss: 40.0056\n",
      "Epoch 612/6000, Training Loss: 40.0082, Validation Loss: 39.9856\n",
      "Epoch 613/6000, Training Loss: 39.9882, Validation Loss: 39.9657\n",
      "Epoch 614/6000, Training Loss: 39.9682, Validation Loss: 39.9457\n",
      "Epoch 615/6000, Training Loss: 39.9482, Validation Loss: 39.9257\n",
      "Epoch 616/6000, Training Loss: 39.9282, Validation Loss: 39.9058\n",
      "Epoch 617/6000, Training Loss: 39.9082, Validation Loss: 39.8858\n",
      "Epoch 618/6000, Training Loss: 39.8882, Validation Loss: 39.8658\n",
      "Epoch 619/6000, Training Loss: 39.8682, Validation Loss: 39.8459\n",
      "Epoch 620/6000, Training Loss: 39.8482, Validation Loss: 39.8259\n",
      "Epoch 621/6000, Training Loss: 39.8282, Validation Loss: 39.8059\n",
      "Epoch 622/6000, Training Loss: 39.8082, Validation Loss: 39.7860\n",
      "Epoch 623/6000, Training Loss: 39.7881, Validation Loss: 39.7660\n",
      "Epoch 624/6000, Training Loss: 39.7681, Validation Loss: 39.7461\n",
      "Epoch 625/6000, Training Loss: 39.7481, Validation Loss: 39.7261\n",
      "Epoch 626/6000, Training Loss: 39.7281, Validation Loss: 39.7061\n",
      "Epoch 627/6000, Training Loss: 39.7081, Validation Loss: 39.6862\n",
      "Epoch 628/6000, Training Loss: 39.6881, Validation Loss: 39.6662\n",
      "Epoch 629/6000, Training Loss: 39.6681, Validation Loss: 39.6462\n",
      "Epoch 630/6000, Training Loss: 39.6481, Validation Loss: 39.6263\n",
      "Epoch 631/6000, Training Loss: 39.6281, Validation Loss: 39.6063\n",
      "Epoch 632/6000, Training Loss: 39.6081, Validation Loss: 39.5863\n",
      "Epoch 633/6000, Training Loss: 39.5881, Validation Loss: 39.5664\n",
      "Epoch 634/6000, Training Loss: 39.5681, Validation Loss: 39.5464\n",
      "Epoch 635/6000, Training Loss: 39.5480, Validation Loss: 39.5264\n",
      "Epoch 636/6000, Training Loss: 39.5280, Validation Loss: 39.5065\n",
      "Epoch 637/6000, Training Loss: 39.5080, Validation Loss: 39.4865\n",
      "Epoch 638/6000, Training Loss: 39.4880, Validation Loss: 39.4665\n",
      "Epoch 639/6000, Training Loss: 39.4680, Validation Loss: 39.4466\n",
      "Epoch 640/6000, Training Loss: 39.4480, Validation Loss: 39.4266\n",
      "Epoch 641/6000, Training Loss: 39.4280, Validation Loss: 39.4066\n",
      "Epoch 642/6000, Training Loss: 39.4080, Validation Loss: 39.3867\n",
      "Epoch 643/6000, Training Loss: 39.3880, Validation Loss: 39.3667\n",
      "Epoch 644/6000, Training Loss: 39.3680, Validation Loss: 39.3467\n",
      "Epoch 645/6000, Training Loss: 39.3480, Validation Loss: 39.3268\n",
      "Epoch 646/6000, Training Loss: 39.3280, Validation Loss: 39.3068\n",
      "Epoch 647/6000, Training Loss: 39.3079, Validation Loss: 39.2868\n",
      "Epoch 648/6000, Training Loss: 39.2879, Validation Loss: 39.2669\n",
      "Epoch 649/6000, Training Loss: 39.2679, Validation Loss: 39.2469\n",
      "Epoch 650/6000, Training Loss: 39.2479, Validation Loss: 39.2269\n",
      "Epoch 651/6000, Training Loss: 39.2279, Validation Loss: 39.2070\n",
      "Epoch 652/6000, Training Loss: 39.2079, Validation Loss: 39.1870\n",
      "Epoch 653/6000, Training Loss: 39.1879, Validation Loss: 39.1670\n",
      "Epoch 654/6000, Training Loss: 39.1679, Validation Loss: 39.1471\n",
      "Epoch 655/6000, Training Loss: 39.1479, Validation Loss: 39.1271\n",
      "Epoch 656/6000, Training Loss: 39.1279, Validation Loss: 39.1072\n",
      "Epoch 657/6000, Training Loss: 39.1079, Validation Loss: 39.0872\n",
      "Epoch 658/6000, Training Loss: 39.0879, Validation Loss: 39.0672\n",
      "Epoch 659/6000, Training Loss: 39.0679, Validation Loss: 39.0472\n",
      "Epoch 660/6000, Training Loss: 39.0479, Validation Loss: 39.0273\n",
      "Epoch 661/6000, Training Loss: 39.0279, Validation Loss: 39.0073\n",
      "Epoch 662/6000, Training Loss: 39.0078, Validation Loss: 38.9873\n",
      "Epoch 663/6000, Training Loss: 38.9878, Validation Loss: 38.9674\n",
      "Epoch 664/6000, Training Loss: 38.9678, Validation Loss: 38.9474\n",
      "Epoch 665/6000, Training Loss: 38.9478, Validation Loss: 38.9274\n",
      "Epoch 666/6000, Training Loss: 38.9278, Validation Loss: 38.9075\n",
      "Epoch 667/6000, Training Loss: 38.9078, Validation Loss: 38.8875\n",
      "Epoch 668/6000, Training Loss: 38.8878, Validation Loss: 38.8675\n",
      "Epoch 669/6000, Training Loss: 38.8678, Validation Loss: 38.8476\n",
      "Epoch 670/6000, Training Loss: 38.8478, Validation Loss: 38.8276\n",
      "Epoch 671/6000, Training Loss: 38.8278, Validation Loss: 38.8076\n",
      "Epoch 672/6000, Training Loss: 38.8078, Validation Loss: 38.7877\n",
      "Epoch 673/6000, Training Loss: 38.7878, Validation Loss: 38.7677\n",
      "Epoch 674/6000, Training Loss: 38.7678, Validation Loss: 38.7477\n",
      "Epoch 675/6000, Training Loss: 38.7477, Validation Loss: 38.7278\n",
      "Epoch 676/6000, Training Loss: 38.7277, Validation Loss: 38.7078\n",
      "Epoch 677/6000, Training Loss: 38.7077, Validation Loss: 38.6878\n",
      "Epoch 678/6000, Training Loss: 38.6877, Validation Loss: 38.6679\n",
      "Epoch 679/6000, Training Loss: 38.6677, Validation Loss: 38.6479\n",
      "Epoch 680/6000, Training Loss: 38.6477, Validation Loss: 38.6279\n",
      "Epoch 681/6000, Training Loss: 38.6277, Validation Loss: 38.6080\n",
      "Epoch 682/6000, Training Loss: 38.6077, Validation Loss: 38.5880\n",
      "Epoch 683/6000, Training Loss: 38.5877, Validation Loss: 38.5680\n",
      "Epoch 684/6000, Training Loss: 38.5677, Validation Loss: 38.5481\n",
      "Epoch 685/6000, Training Loss: 38.5477, Validation Loss: 38.5281\n",
      "Epoch 686/6000, Training Loss: 38.5277, Validation Loss: 38.5081\n",
      "Epoch 687/6000, Training Loss: 38.5077, Validation Loss: 38.4882\n",
      "Epoch 688/6000, Training Loss: 38.4877, Validation Loss: 38.4682\n",
      "Epoch 689/6000, Training Loss: 38.4677, Validation Loss: 38.4482\n",
      "Epoch 690/6000, Training Loss: 38.4477, Validation Loss: 38.4282\n",
      "Epoch 691/6000, Training Loss: 38.4276, Validation Loss: 38.4083\n",
      "Epoch 692/6000, Training Loss: 38.4076, Validation Loss: 38.3883\n",
      "Epoch 693/6000, Training Loss: 38.3876, Validation Loss: 38.3683\n",
      "Epoch 694/6000, Training Loss: 38.3676, Validation Loss: 38.3484\n",
      "Epoch 695/6000, Training Loss: 38.3476, Validation Loss: 38.3284\n",
      "Epoch 696/6000, Training Loss: 38.3276, Validation Loss: 38.3084\n",
      "Epoch 697/6000, Training Loss: 38.3076, Validation Loss: 38.2885\n",
      "Epoch 698/6000, Training Loss: 38.2876, Validation Loss: 38.2685\n",
      "Epoch 699/6000, Training Loss: 38.2676, Validation Loss: 38.2485\n",
      "Epoch 700/6000, Training Loss: 38.2476, Validation Loss: 38.2286\n",
      "Epoch 701/6000, Training Loss: 38.2276, Validation Loss: 38.2086\n",
      "Epoch 702/6000, Training Loss: 38.2076, Validation Loss: 38.1886\n",
      "Epoch 703/6000, Training Loss: 38.1876, Validation Loss: 38.1687\n",
      "Epoch 704/6000, Training Loss: 38.1676, Validation Loss: 38.1487\n",
      "Epoch 705/6000, Training Loss: 38.1476, Validation Loss: 38.1287\n",
      "Epoch 706/6000, Training Loss: 38.1276, Validation Loss: 38.1087\n",
      "Epoch 707/6000, Training Loss: 38.1076, Validation Loss: 38.0888\n",
      "Epoch 708/6000, Training Loss: 38.0875, Validation Loss: 38.0688\n",
      "Epoch 709/6000, Training Loss: 38.0675, Validation Loss: 38.0488\n",
      "Epoch 710/6000, Training Loss: 38.0475, Validation Loss: 38.0289\n",
      "Epoch 711/6000, Training Loss: 38.0275, Validation Loss: 38.0089\n",
      "Epoch 712/6000, Training Loss: 38.0075, Validation Loss: 37.9889\n",
      "Epoch 713/6000, Training Loss: 37.9875, Validation Loss: 37.9690\n",
      "Epoch 714/6000, Training Loss: 37.9675, Validation Loss: 37.9490\n",
      "Epoch 715/6000, Training Loss: 37.9475, Validation Loss: 37.9290\n",
      "Epoch 716/6000, Training Loss: 37.9275, Validation Loss: 37.9090\n",
      "Epoch 717/6000, Training Loss: 37.9075, Validation Loss: 37.8891\n",
      "Epoch 718/6000, Training Loss: 37.8875, Validation Loss: 37.8691\n",
      "Epoch 719/6000, Training Loss: 37.8675, Validation Loss: 37.8491\n",
      "Epoch 720/6000, Training Loss: 37.8475, Validation Loss: 37.8292\n",
      "Epoch 721/6000, Training Loss: 37.8275, Validation Loss: 37.8092\n",
      "Epoch 722/6000, Training Loss: 37.8075, Validation Loss: 37.7892\n",
      "Epoch 723/6000, Training Loss: 37.7875, Validation Loss: 37.7693\n",
      "Epoch 724/6000, Training Loss: 37.7675, Validation Loss: 37.7493\n",
      "Epoch 725/6000, Training Loss: 37.7475, Validation Loss: 37.7293\n",
      "Epoch 726/6000, Training Loss: 37.7275, Validation Loss: 37.7093\n",
      "Epoch 727/6000, Training Loss: 37.7075, Validation Loss: 37.6894\n",
      "Epoch 728/6000, Training Loss: 37.6875, Validation Loss: 37.6694\n",
      "Epoch 729/6000, Training Loss: 37.6674, Validation Loss: 37.6494\n",
      "Epoch 730/6000, Training Loss: 37.6474, Validation Loss: 37.6295\n",
      "Epoch 731/6000, Training Loss: 37.6274, Validation Loss: 37.6095\n",
      "Epoch 732/6000, Training Loss: 37.6074, Validation Loss: 37.5895\n",
      "Epoch 733/6000, Training Loss: 37.5874, Validation Loss: 37.5695\n",
      "Epoch 734/6000, Training Loss: 37.5674, Validation Loss: 37.5496\n",
      "Epoch 735/6000, Training Loss: 37.5474, Validation Loss: 37.5296\n",
      "Epoch 736/6000, Training Loss: 37.5274, Validation Loss: 37.5096\n",
      "Epoch 737/6000, Training Loss: 37.5074, Validation Loss: 37.4897\n",
      "Epoch 738/6000, Training Loss: 37.4874, Validation Loss: 37.4697\n",
      "Epoch 739/6000, Training Loss: 37.4674, Validation Loss: 37.4497\n",
      "Epoch 740/6000, Training Loss: 37.4474, Validation Loss: 37.4297\n",
      "Epoch 741/6000, Training Loss: 37.4274, Validation Loss: 37.4098\n",
      "Epoch 742/6000, Training Loss: 37.4074, Validation Loss: 37.3898\n",
      "Epoch 743/6000, Training Loss: 37.3874, Validation Loss: 37.3698\n",
      "Epoch 744/6000, Training Loss: 37.3674, Validation Loss: 37.3499\n",
      "Epoch 745/6000, Training Loss: 37.3474, Validation Loss: 37.3299\n",
      "Epoch 746/6000, Training Loss: 37.3274, Validation Loss: 37.3099\n",
      "Epoch 747/6000, Training Loss: 37.3074, Validation Loss: 37.2899\n",
      "Epoch 748/6000, Training Loss: 37.2874, Validation Loss: 37.2700\n",
      "Epoch 749/6000, Training Loss: 37.2674, Validation Loss: 37.2500\n",
      "Epoch 750/6000, Training Loss: 37.2474, Validation Loss: 37.2300\n",
      "Epoch 751/6000, Training Loss: 37.2274, Validation Loss: 37.2101\n",
      "Epoch 752/6000, Training Loss: 37.2074, Validation Loss: 37.1901\n",
      "Epoch 753/6000, Training Loss: 37.1874, Validation Loss: 37.1701\n",
      "Epoch 754/6000, Training Loss: 37.1674, Validation Loss: 37.1501\n",
      "Epoch 755/6000, Training Loss: 37.1474, Validation Loss: 37.1302\n",
      "Epoch 756/6000, Training Loss: 37.1274, Validation Loss: 37.1102\n",
      "Epoch 757/6000, Training Loss: 37.1074, Validation Loss: 37.0902\n",
      "Epoch 758/6000, Training Loss: 37.0874, Validation Loss: 37.0703\n",
      "Epoch 759/6000, Training Loss: 37.0674, Validation Loss: 37.0503\n",
      "Epoch 760/6000, Training Loss: 37.0474, Validation Loss: 37.0303\n",
      "Epoch 761/6000, Training Loss: 37.0274, Validation Loss: 37.0103\n",
      "Epoch 762/6000, Training Loss: 37.0074, Validation Loss: 36.9904\n",
      "Epoch 763/6000, Training Loss: 36.9874, Validation Loss: 36.9704\n",
      "Epoch 764/6000, Training Loss: 36.9674, Validation Loss: 36.9504\n",
      "Epoch 765/6000, Training Loss: 36.9474, Validation Loss: 36.9304\n",
      "Epoch 766/6000, Training Loss: 36.9273, Validation Loss: 36.9105\n",
      "Epoch 767/6000, Training Loss: 36.9073, Validation Loss: 36.8905\n",
      "Epoch 768/6000, Training Loss: 36.8873, Validation Loss: 36.8705\n",
      "Epoch 769/6000, Training Loss: 36.8673, Validation Loss: 36.8506\n",
      "Epoch 770/6000, Training Loss: 36.8473, Validation Loss: 36.8306\n",
      "Epoch 771/6000, Training Loss: 36.8273, Validation Loss: 36.8106\n",
      "Epoch 772/6000, Training Loss: 36.8073, Validation Loss: 36.7906\n",
      "Epoch 773/6000, Training Loss: 36.7873, Validation Loss: 36.7707\n",
      "Epoch 774/6000, Training Loss: 36.7673, Validation Loss: 36.7507\n",
      "Epoch 775/6000, Training Loss: 36.7473, Validation Loss: 36.7307\n",
      "Epoch 776/6000, Training Loss: 36.7273, Validation Loss: 36.7107\n",
      "Epoch 777/6000, Training Loss: 36.7073, Validation Loss: 36.6908\n",
      "Epoch 778/6000, Training Loss: 36.6873, Validation Loss: 36.6708\n",
      "Epoch 779/6000, Training Loss: 36.6673, Validation Loss: 36.6508\n",
      "Epoch 780/6000, Training Loss: 36.6473, Validation Loss: 36.6308\n",
      "Epoch 781/6000, Training Loss: 36.6273, Validation Loss: 36.6109\n",
      "Epoch 782/6000, Training Loss: 36.6073, Validation Loss: 36.5909\n",
      "Epoch 783/6000, Training Loss: 36.5873, Validation Loss: 36.5709\n",
      "Epoch 784/6000, Training Loss: 36.5673, Validation Loss: 36.5509\n",
      "Epoch 785/6000, Training Loss: 36.5473, Validation Loss: 36.5310\n",
      "Epoch 786/6000, Training Loss: 36.5274, Validation Loss: 36.5110\n",
      "Epoch 787/6000, Training Loss: 36.5074, Validation Loss: 36.4910\n",
      "Epoch 788/6000, Training Loss: 36.4874, Validation Loss: 36.4710\n",
      "Epoch 789/6000, Training Loss: 36.4674, Validation Loss: 36.4511\n",
      "Epoch 790/6000, Training Loss: 36.4474, Validation Loss: 36.4311\n",
      "Epoch 791/6000, Training Loss: 36.4274, Validation Loss: 36.4111\n",
      "Epoch 792/6000, Training Loss: 36.4074, Validation Loss: 36.3911\n",
      "Epoch 793/6000, Training Loss: 36.3874, Validation Loss: 36.3711\n",
      "Epoch 794/6000, Training Loss: 36.3674, Validation Loss: 36.3512\n",
      "Epoch 795/6000, Training Loss: 36.3474, Validation Loss: 36.3312\n",
      "Epoch 796/6000, Training Loss: 36.3274, Validation Loss: 36.3112\n",
      "Epoch 797/6000, Training Loss: 36.3074, Validation Loss: 36.2912\n",
      "Epoch 798/6000, Training Loss: 36.2874, Validation Loss: 36.2713\n",
      "Epoch 799/6000, Training Loss: 36.2674, Validation Loss: 36.2513\n",
      "Epoch 800/6000, Training Loss: 36.2474, Validation Loss: 36.2313\n",
      "Epoch 801/6000, Training Loss: 36.2274, Validation Loss: 36.2113\n",
      "Epoch 802/6000, Training Loss: 36.2074, Validation Loss: 36.1913\n",
      "Epoch 803/6000, Training Loss: 36.1874, Validation Loss: 36.1714\n",
      "Epoch 804/6000, Training Loss: 36.1674, Validation Loss: 36.1514\n",
      "Epoch 805/6000, Training Loss: 36.1474, Validation Loss: 36.1314\n",
      "Epoch 806/6000, Training Loss: 36.1274, Validation Loss: 36.1114\n",
      "Epoch 807/6000, Training Loss: 36.1074, Validation Loss: 36.0914\n",
      "Epoch 808/6000, Training Loss: 36.0874, Validation Loss: 36.0715\n",
      "Epoch 809/6000, Training Loss: 36.0675, Validation Loss: 36.0515\n",
      "Epoch 810/6000, Training Loss: 36.0475, Validation Loss: 36.0315\n",
      "Epoch 811/6000, Training Loss: 36.0275, Validation Loss: 36.0115\n",
      "Epoch 812/6000, Training Loss: 36.0075, Validation Loss: 35.9916\n",
      "Epoch 813/6000, Training Loss: 35.9875, Validation Loss: 35.9716\n",
      "Epoch 814/6000, Training Loss: 35.9675, Validation Loss: 35.9516\n",
      "Epoch 815/6000, Training Loss: 35.9475, Validation Loss: 35.9316\n",
      "Epoch 816/6000, Training Loss: 35.9275, Validation Loss: 35.9116\n",
      "Epoch 817/6000, Training Loss: 35.9075, Validation Loss: 35.8916\n",
      "Epoch 818/6000, Training Loss: 35.8876, Validation Loss: 35.8717\n",
      "Epoch 819/6000, Training Loss: 35.8676, Validation Loss: 35.8517\n",
      "Epoch 820/6000, Training Loss: 35.8476, Validation Loss: 35.8317\n",
      "Epoch 821/6000, Training Loss: 35.8276, Validation Loss: 35.8117\n",
      "Epoch 822/6000, Training Loss: 35.8076, Validation Loss: 35.7917\n",
      "Epoch 823/6000, Training Loss: 35.7876, Validation Loss: 35.7717\n",
      "Epoch 824/6000, Training Loss: 35.7676, Validation Loss: 35.7517\n",
      "Epoch 825/6000, Training Loss: 35.7477, Validation Loss: 35.7317\n",
      "Epoch 826/6000, Training Loss: 35.7277, Validation Loss: 35.7117\n",
      "Epoch 827/6000, Training Loss: 35.7077, Validation Loss: 35.6918\n",
      "Epoch 828/6000, Training Loss: 35.6877, Validation Loss: 35.6718\n",
      "Epoch 829/6000, Training Loss: 35.6677, Validation Loss: 35.6518\n",
      "Epoch 830/6000, Training Loss: 35.6478, Validation Loss: 35.6318\n",
      "Epoch 831/6000, Training Loss: 35.6278, Validation Loss: 35.6118\n",
      "Epoch 832/6000, Training Loss: 35.6078, Validation Loss: 35.5918\n",
      "Epoch 833/6000, Training Loss: 35.5878, Validation Loss: 35.5718\n",
      "Epoch 834/6000, Training Loss: 35.5678, Validation Loss: 35.5518\n",
      "Epoch 835/6000, Training Loss: 35.5479, Validation Loss: 35.5318\n",
      "Epoch 836/6000, Training Loss: 35.5279, Validation Loss: 35.5118\n",
      "Epoch 837/6000, Training Loss: 35.5079, Validation Loss: 35.4918\n",
      "Epoch 838/6000, Training Loss: 35.4879, Validation Loss: 35.4718\n",
      "Epoch 839/6000, Training Loss: 35.4680, Validation Loss: 35.4518\n",
      "Epoch 840/6000, Training Loss: 35.4480, Validation Loss: 35.4318\n",
      "Epoch 841/6000, Training Loss: 35.4280, Validation Loss: 35.4117\n",
      "Epoch 842/6000, Training Loss: 35.4080, Validation Loss: 35.3917\n",
      "Epoch 843/6000, Training Loss: 35.3880, Validation Loss: 35.3717\n",
      "Epoch 844/6000, Training Loss: 35.3681, Validation Loss: 35.3517\n",
      "Epoch 845/6000, Training Loss: 35.3481, Validation Loss: 35.3317\n",
      "Epoch 846/6000, Training Loss: 35.3281, Validation Loss: 35.3117\n",
      "Epoch 847/6000, Training Loss: 35.3081, Validation Loss: 35.2917\n",
      "Epoch 848/6000, Training Loss: 35.2881, Validation Loss: 35.2717\n",
      "Epoch 849/6000, Training Loss: 35.2682, Validation Loss: 35.2517\n",
      "Epoch 850/6000, Training Loss: 35.2482, Validation Loss: 35.2317\n",
      "Epoch 851/6000, Training Loss: 35.2282, Validation Loss: 35.2117\n",
      "Epoch 852/6000, Training Loss: 35.2082, Validation Loss: 35.1917\n",
      "Epoch 853/6000, Training Loss: 35.1883, Validation Loss: 35.1716\n",
      "Epoch 854/6000, Training Loss: 35.1683, Validation Loss: 35.1516\n",
      "Epoch 855/6000, Training Loss: 35.1483, Validation Loss: 35.1316\n",
      "Epoch 856/6000, Training Loss: 35.1283, Validation Loss: 35.1116\n",
      "Epoch 857/6000, Training Loss: 35.1083, Validation Loss: 35.0916\n",
      "Epoch 858/6000, Training Loss: 35.0884, Validation Loss: 35.0716\n",
      "Epoch 859/6000, Training Loss: 35.0684, Validation Loss: 35.0516\n",
      "Epoch 860/6000, Training Loss: 35.0484, Validation Loss: 35.0316\n",
      "Epoch 861/6000, Training Loss: 35.0284, Validation Loss: 35.0115\n",
      "Epoch 862/6000, Training Loss: 35.0084, Validation Loss: 34.9915\n",
      "Epoch 863/6000, Training Loss: 34.9885, Validation Loss: 34.9715\n",
      "Epoch 864/6000, Training Loss: 34.9685, Validation Loss: 34.9515\n",
      "Epoch 865/6000, Training Loss: 34.9485, Validation Loss: 34.9315\n",
      "Epoch 866/6000, Training Loss: 34.9285, Validation Loss: 34.9115\n",
      "Epoch 867/6000, Training Loss: 34.9086, Validation Loss: 34.8915\n",
      "Epoch 868/6000, Training Loss: 34.8886, Validation Loss: 34.8715\n",
      "Epoch 869/6000, Training Loss: 34.8686, Validation Loss: 34.8514\n",
      "Epoch 870/6000, Training Loss: 34.8486, Validation Loss: 34.8314\n",
      "Epoch 871/6000, Training Loss: 34.8286, Validation Loss: 34.8114\n",
      "Epoch 872/6000, Training Loss: 34.8087, Validation Loss: 34.7914\n",
      "Epoch 873/6000, Training Loss: 34.7887, Validation Loss: 34.7714\n",
      "Epoch 874/6000, Training Loss: 34.7687, Validation Loss: 34.7514\n",
      "Epoch 875/6000, Training Loss: 34.7487, Validation Loss: 34.7314\n",
      "Epoch 876/6000, Training Loss: 34.7287, Validation Loss: 34.7113\n",
      "Epoch 877/6000, Training Loss: 34.7088, Validation Loss: 34.6913\n",
      "Epoch 878/6000, Training Loss: 34.6888, Validation Loss: 34.6713\n",
      "Epoch 879/6000, Training Loss: 34.6688, Validation Loss: 34.6513\n",
      "Epoch 880/6000, Training Loss: 34.6488, Validation Loss: 34.6313\n",
      "Epoch 881/6000, Training Loss: 34.6288, Validation Loss: 34.6113\n",
      "Epoch 882/6000, Training Loss: 34.6089, Validation Loss: 34.5913\n",
      "Epoch 883/6000, Training Loss: 34.5889, Validation Loss: 34.5712\n",
      "Epoch 884/6000, Training Loss: 34.5689, Validation Loss: 34.5512\n",
      "Epoch 885/6000, Training Loss: 34.5489, Validation Loss: 34.5312\n",
      "Epoch 886/6000, Training Loss: 34.5290, Validation Loss: 34.5112\n",
      "Epoch 887/6000, Training Loss: 34.5090, Validation Loss: 34.4912\n",
      "Epoch 888/6000, Training Loss: 34.4890, Validation Loss: 34.4712\n",
      "Epoch 889/6000, Training Loss: 34.4690, Validation Loss: 34.4512\n",
      "Epoch 890/6000, Training Loss: 34.4490, Validation Loss: 34.4312\n",
      "Epoch 891/6000, Training Loss: 34.4291, Validation Loss: 34.4111\n",
      "Epoch 892/6000, Training Loss: 34.4091, Validation Loss: 34.3911\n",
      "Epoch 893/6000, Training Loss: 34.3891, Validation Loss: 34.3711\n",
      "Epoch 894/6000, Training Loss: 34.3691, Validation Loss: 34.3511\n",
      "Epoch 895/6000, Training Loss: 34.3491, Validation Loss: 34.3311\n",
      "Epoch 896/6000, Training Loss: 34.3292, Validation Loss: 34.3111\n",
      "Epoch 897/6000, Training Loss: 34.3092, Validation Loss: 34.2911\n",
      "Epoch 898/6000, Training Loss: 34.2892, Validation Loss: 34.2710\n",
      "Epoch 899/6000, Training Loss: 34.2692, Validation Loss: 34.2510\n",
      "Epoch 900/6000, Training Loss: 34.2493, Validation Loss: 34.2310\n",
      "Epoch 901/6000, Training Loss: 34.2293, Validation Loss: 34.2110\n",
      "Epoch 902/6000, Training Loss: 34.2093, Validation Loss: 34.1910\n",
      "Epoch 903/6000, Training Loss: 34.1893, Validation Loss: 34.1710\n",
      "Epoch 904/6000, Training Loss: 34.1693, Validation Loss: 34.1510\n",
      "Epoch 905/6000, Training Loss: 34.1494, Validation Loss: 34.1310\n",
      "Epoch 906/6000, Training Loss: 34.1294, Validation Loss: 34.1109\n",
      "Epoch 907/6000, Training Loss: 34.1094, Validation Loss: 34.0909\n",
      "Epoch 908/6000, Training Loss: 34.0894, Validation Loss: 34.0709\n",
      "Epoch 909/6000, Training Loss: 34.0694, Validation Loss: 34.0509\n",
      "Epoch 910/6000, Training Loss: 34.0495, Validation Loss: 34.0309\n",
      "Epoch 911/6000, Training Loss: 34.0295, Validation Loss: 34.0109\n",
      "Epoch 912/6000, Training Loss: 34.0095, Validation Loss: 33.9909\n",
      "Epoch 913/6000, Training Loss: 33.9895, Validation Loss: 33.9708\n",
      "Epoch 914/6000, Training Loss: 33.9696, Validation Loss: 33.9508\n",
      "Epoch 915/6000, Training Loss: 33.9496, Validation Loss: 33.9308\n",
      "Epoch 916/6000, Training Loss: 33.9296, Validation Loss: 33.9108\n",
      "Epoch 917/6000, Training Loss: 33.9096, Validation Loss: 33.8908\n",
      "Epoch 918/6000, Training Loss: 33.8896, Validation Loss: 33.8708\n",
      "Epoch 919/6000, Training Loss: 33.8697, Validation Loss: 33.8508\n",
      "Epoch 920/6000, Training Loss: 33.8497, Validation Loss: 33.8308\n",
      "Epoch 921/6000, Training Loss: 33.8297, Validation Loss: 33.8107\n",
      "Epoch 922/6000, Training Loss: 33.8097, Validation Loss: 33.7907\n",
      "Epoch 923/6000, Training Loss: 33.7897, Validation Loss: 33.7707\n",
      "Epoch 924/6000, Training Loss: 33.7698, Validation Loss: 33.7507\n",
      "Epoch 925/6000, Training Loss: 33.7498, Validation Loss: 33.7307\n",
      "Epoch 926/6000, Training Loss: 33.7298, Validation Loss: 33.7107\n",
      "Epoch 927/6000, Training Loss: 33.7098, Validation Loss: 33.6907\n",
      "Epoch 928/6000, Training Loss: 33.6898, Validation Loss: 33.6706\n",
      "Epoch 929/6000, Training Loss: 33.6699, Validation Loss: 33.6506\n",
      "Epoch 930/6000, Training Loss: 33.6499, Validation Loss: 33.6306\n",
      "Epoch 931/6000, Training Loss: 33.6299, Validation Loss: 33.6106\n",
      "Epoch 932/6000, Training Loss: 33.6099, Validation Loss: 33.5906\n",
      "Epoch 933/6000, Training Loss: 33.5899, Validation Loss: 33.5706\n",
      "Epoch 934/6000, Training Loss: 33.5700, Validation Loss: 33.5506\n",
      "Epoch 935/6000, Training Loss: 33.5500, Validation Loss: 33.5306\n",
      "Epoch 936/6000, Training Loss: 33.5300, Validation Loss: 33.5105\n",
      "Epoch 937/6000, Training Loss: 33.5100, Validation Loss: 33.4905\n",
      "Epoch 938/6000, Training Loss: 33.4901, Validation Loss: 33.4705\n",
      "Epoch 939/6000, Training Loss: 33.4701, Validation Loss: 33.4505\n",
      "Epoch 940/6000, Training Loss: 33.4501, Validation Loss: 33.4305\n",
      "Epoch 941/6000, Training Loss: 33.4301, Validation Loss: 33.4105\n",
      "Epoch 942/6000, Training Loss: 33.4101, Validation Loss: 33.3905\n",
      "Epoch 943/6000, Training Loss: 33.3902, Validation Loss: 33.3704\n",
      "Epoch 944/6000, Training Loss: 33.3702, Validation Loss: 33.3504\n",
      "Epoch 945/6000, Training Loss: 33.3502, Validation Loss: 33.3304\n",
      "Epoch 946/6000, Training Loss: 33.3302, Validation Loss: 33.3104\n",
      "Epoch 947/6000, Training Loss: 33.3102, Validation Loss: 33.2904\n",
      "Epoch 948/6000, Training Loss: 33.2903, Validation Loss: 33.2704\n",
      "Epoch 949/6000, Training Loss: 33.2703, Validation Loss: 33.2503\n",
      "Epoch 950/6000, Training Loss: 33.2503, Validation Loss: 33.2303\n",
      "Epoch 951/6000, Training Loss: 33.2303, Validation Loss: 33.2103\n",
      "Epoch 952/6000, Training Loss: 33.2103, Validation Loss: 33.1903\n",
      "Epoch 953/6000, Training Loss: 33.1904, Validation Loss: 33.1703\n",
      "Epoch 954/6000, Training Loss: 33.1704, Validation Loss: 33.1503\n",
      "Epoch 955/6000, Training Loss: 33.1504, Validation Loss: 33.1303\n",
      "Epoch 956/6000, Training Loss: 33.1304, Validation Loss: 33.1102\n",
      "Epoch 957/6000, Training Loss: 33.1104, Validation Loss: 33.0902\n",
      "Epoch 958/6000, Training Loss: 33.0905, Validation Loss: 33.0702\n",
      "Epoch 959/6000, Training Loss: 33.0705, Validation Loss: 33.0502\n",
      "Epoch 960/6000, Training Loss: 33.0505, Validation Loss: 33.0302\n",
      "Epoch 961/6000, Training Loss: 33.0305, Validation Loss: 33.0102\n",
      "Epoch 962/6000, Training Loss: 33.0105, Validation Loss: 32.9902\n",
      "Epoch 963/6000, Training Loss: 32.9906, Validation Loss: 32.9701\n",
      "Epoch 964/6000, Training Loss: 32.9706, Validation Loss: 32.9501\n",
      "Epoch 965/6000, Training Loss: 32.9506, Validation Loss: 32.9301\n",
      "Epoch 966/6000, Training Loss: 32.9306, Validation Loss: 32.9101\n",
      "Epoch 967/6000, Training Loss: 32.9106, Validation Loss: 32.8901\n",
      "Epoch 968/6000, Training Loss: 32.8907, Validation Loss: 32.8701\n",
      "Epoch 969/6000, Training Loss: 32.8707, Validation Loss: 32.8500\n",
      "Epoch 970/6000, Training Loss: 32.8507, Validation Loss: 32.8300\n",
      "Epoch 971/6000, Training Loss: 32.8307, Validation Loss: 32.8100\n",
      "Epoch 972/6000, Training Loss: 32.8107, Validation Loss: 32.7900\n",
      "Epoch 973/6000, Training Loss: 32.7908, Validation Loss: 32.7700\n",
      "Epoch 974/6000, Training Loss: 32.7708, Validation Loss: 32.7500\n",
      "Epoch 975/6000, Training Loss: 32.7508, Validation Loss: 32.7300\n",
      "Epoch 976/6000, Training Loss: 32.7308, Validation Loss: 32.7099\n",
      "Epoch 977/6000, Training Loss: 32.7109, Validation Loss: 32.6899\n",
      "Epoch 978/6000, Training Loss: 32.6909, Validation Loss: 32.6699\n",
      "Epoch 979/6000, Training Loss: 32.6709, Validation Loss: 32.6499\n",
      "Epoch 980/6000, Training Loss: 32.6509, Validation Loss: 32.6299\n",
      "Epoch 981/6000, Training Loss: 32.6309, Validation Loss: 32.6099\n",
      "Epoch 982/6000, Training Loss: 32.6110, Validation Loss: 32.5898\n",
      "Epoch 983/6000, Training Loss: 32.5910, Validation Loss: 32.5698\n",
      "Epoch 984/6000, Training Loss: 32.5710, Validation Loss: 32.5498\n",
      "Epoch 985/6000, Training Loss: 32.5510, Validation Loss: 32.5298\n",
      "Epoch 986/6000, Training Loss: 32.5310, Validation Loss: 32.5098\n",
      "Epoch 987/6000, Training Loss: 32.5111, Validation Loss: 32.4898\n",
      "Epoch 988/6000, Training Loss: 32.4911, Validation Loss: 32.4697\n",
      "Epoch 989/6000, Training Loss: 32.4711, Validation Loss: 32.4497\n",
      "Epoch 990/6000, Training Loss: 32.4511, Validation Loss: 32.4297\n",
      "Epoch 991/6000, Training Loss: 32.4311, Validation Loss: 32.4097\n",
      "Epoch 992/6000, Training Loss: 32.4112, Validation Loss: 32.3897\n",
      "Epoch 993/6000, Training Loss: 32.3912, Validation Loss: 32.3697\n",
      "Epoch 994/6000, Training Loss: 32.3712, Validation Loss: 32.3497\n",
      "Epoch 995/6000, Training Loss: 32.3512, Validation Loss: 32.3296\n",
      "Epoch 996/6000, Training Loss: 32.3312, Validation Loss: 32.3096\n",
      "Epoch 997/6000, Training Loss: 32.3113, Validation Loss: 32.2896\n",
      "Epoch 998/6000, Training Loss: 32.2913, Validation Loss: 32.2696\n",
      "Epoch 999/6000, Training Loss: 32.2713, Validation Loss: 32.2496\n",
      "Epoch 1000/6000, Training Loss: 32.2513, Validation Loss: 32.2296\n",
      "Epoch 1001/6000, Training Loss: 32.2313, Validation Loss: 32.2095\n",
      "Epoch 1002/6000, Training Loss: 32.2114, Validation Loss: 32.1895\n",
      "Epoch 1003/6000, Training Loss: 32.1914, Validation Loss: 32.1695\n",
      "Epoch 1004/6000, Training Loss: 32.1714, Validation Loss: 32.1495\n",
      "Epoch 1005/6000, Training Loss: 32.1514, Validation Loss: 32.1295\n",
      "Epoch 1006/6000, Training Loss: 32.1314, Validation Loss: 32.1095\n",
      "Epoch 1007/6000, Training Loss: 32.1115, Validation Loss: 32.0895\n",
      "Epoch 1008/6000, Training Loss: 32.0915, Validation Loss: 32.0694\n",
      "Epoch 1009/6000, Training Loss: 32.0715, Validation Loss: 32.0494\n",
      "Epoch 1010/6000, Training Loss: 32.0515, Validation Loss: 32.0294\n",
      "Epoch 1011/6000, Training Loss: 32.0315, Validation Loss: 32.0094\n",
      "Epoch 1012/6000, Training Loss: 32.0116, Validation Loss: 31.9894\n",
      "Epoch 1013/6000, Training Loss: 31.9916, Validation Loss: 31.9694\n",
      "Epoch 1014/6000, Training Loss: 31.9716, Validation Loss: 31.9493\n",
      "Epoch 1015/6000, Training Loss: 31.9516, Validation Loss: 31.9293\n",
      "Epoch 1016/6000, Training Loss: 31.9316, Validation Loss: 31.9093\n",
      "Epoch 1017/6000, Training Loss: 31.9117, Validation Loss: 31.8893\n",
      "Epoch 1018/6000, Training Loss: 31.8917, Validation Loss: 31.8693\n",
      "Epoch 1019/6000, Training Loss: 31.8717, Validation Loss: 31.8493\n",
      "Epoch 1020/6000, Training Loss: 31.8517, Validation Loss: 31.8292\n",
      "Epoch 1021/6000, Training Loss: 31.8317, Validation Loss: 31.8092\n",
      "Epoch 1022/6000, Training Loss: 31.8118, Validation Loss: 31.7892\n",
      "Epoch 1023/6000, Training Loss: 31.7918, Validation Loss: 31.7692\n",
      "Epoch 1024/6000, Training Loss: 31.7718, Validation Loss: 31.7492\n",
      "Epoch 1025/6000, Training Loss: 31.7518, Validation Loss: 31.7292\n",
      "Epoch 1026/6000, Training Loss: 31.7318, Validation Loss: 31.7091\n",
      "Epoch 1027/6000, Training Loss: 31.7119, Validation Loss: 31.6891\n",
      "Epoch 1028/6000, Training Loss: 31.6919, Validation Loss: 31.6691\n",
      "Epoch 1029/6000, Training Loss: 31.6719, Validation Loss: 31.6491\n",
      "Epoch 1030/6000, Training Loss: 31.6519, Validation Loss: 31.6291\n",
      "Epoch 1031/6000, Training Loss: 31.6319, Validation Loss: 31.6091\n",
      "Epoch 1032/6000, Training Loss: 31.6120, Validation Loss: 31.5890\n",
      "Epoch 1033/6000, Training Loss: 31.5920, Validation Loss: 31.5690\n",
      "Epoch 1034/6000, Training Loss: 31.5720, Validation Loss: 31.5490\n",
      "Epoch 1035/6000, Training Loss: 31.5520, Validation Loss: 31.5290\n",
      "Epoch 1036/6000, Training Loss: 31.5320, Validation Loss: 31.5090\n",
      "Epoch 1037/6000, Training Loss: 31.5121, Validation Loss: 31.4890\n",
      "Epoch 1038/6000, Training Loss: 31.4921, Validation Loss: 31.4689\n",
      "Epoch 1039/6000, Training Loss: 31.4721, Validation Loss: 31.4489\n",
      "Epoch 1040/6000, Training Loss: 31.4521, Validation Loss: 31.4289\n",
      "Epoch 1041/6000, Training Loss: 31.4321, Validation Loss: 31.4089\n",
      "Epoch 1042/6000, Training Loss: 31.4122, Validation Loss: 31.3889\n",
      "Epoch 1043/6000, Training Loss: 31.3922, Validation Loss: 31.3689\n",
      "Epoch 1044/6000, Training Loss: 31.3722, Validation Loss: 31.3488\n",
      "Epoch 1045/6000, Training Loss: 31.3522, Validation Loss: 31.3288\n",
      "Epoch 1046/6000, Training Loss: 31.3322, Validation Loss: 31.3088\n",
      "Epoch 1047/6000, Training Loss: 31.3123, Validation Loss: 31.2888\n",
      "Epoch 1048/6000, Training Loss: 31.2923, Validation Loss: 31.2688\n",
      "Epoch 1049/6000, Training Loss: 31.2723, Validation Loss: 31.2488\n",
      "Epoch 1050/6000, Training Loss: 31.2523, Validation Loss: 31.2287\n",
      "Epoch 1051/6000, Training Loss: 31.2323, Validation Loss: 31.2087\n",
      "Epoch 1052/6000, Training Loss: 31.2124, Validation Loss: 31.1887\n",
      "Epoch 1053/6000, Training Loss: 31.1924, Validation Loss: 31.1687\n",
      "Epoch 1054/6000, Training Loss: 31.1724, Validation Loss: 31.1487\n",
      "Epoch 1055/6000, Training Loss: 31.1524, Validation Loss: 31.1287\n",
      "Epoch 1056/6000, Training Loss: 31.1324, Validation Loss: 31.1086\n",
      "Epoch 1057/6000, Training Loss: 31.1125, Validation Loss: 31.0886\n",
      "Epoch 1058/6000, Training Loss: 31.0925, Validation Loss: 31.0686\n",
      "Epoch 1059/6000, Training Loss: 31.0725, Validation Loss: 31.0486\n",
      "Epoch 1060/6000, Training Loss: 31.0525, Validation Loss: 31.0286\n",
      "Epoch 1061/6000, Training Loss: 31.0325, Validation Loss: 31.0086\n",
      "Epoch 1062/6000, Training Loss: 31.0126, Validation Loss: 30.9885\n",
      "Epoch 1063/6000, Training Loss: 30.9926, Validation Loss: 30.9685\n",
      "Epoch 1064/6000, Training Loss: 30.9726, Validation Loss: 30.9485\n",
      "Epoch 1065/6000, Training Loss: 30.9526, Validation Loss: 30.9285\n",
      "Epoch 1066/6000, Training Loss: 30.9326, Validation Loss: 30.9085\n",
      "Epoch 1067/6000, Training Loss: 30.9127, Validation Loss: 30.8885\n",
      "Epoch 1068/6000, Training Loss: 30.8927, Validation Loss: 30.8684\n",
      "Epoch 1069/6000, Training Loss: 30.8727, Validation Loss: 30.8484\n",
      "Epoch 1070/6000, Training Loss: 30.8527, Validation Loss: 30.8284\n",
      "Epoch 1071/6000, Training Loss: 30.8327, Validation Loss: 30.8084\n",
      "Epoch 1072/6000, Training Loss: 30.8128, Validation Loss: 30.7884\n",
      "Epoch 1073/6000, Training Loss: 30.7928, Validation Loss: 30.7684\n",
      "Epoch 1074/6000, Training Loss: 30.7728, Validation Loss: 30.7483\n",
      "Epoch 1075/6000, Training Loss: 30.7528, Validation Loss: 30.7283\n",
      "Epoch 1076/6000, Training Loss: 30.7328, Validation Loss: 30.7083\n",
      "Epoch 1077/6000, Training Loss: 30.7129, Validation Loss: 30.6883\n",
      "Epoch 1078/6000, Training Loss: 30.6929, Validation Loss: 30.6683\n",
      "Epoch 1079/6000, Training Loss: 30.6729, Validation Loss: 30.6482\n",
      "Epoch 1080/6000, Training Loss: 30.6529, Validation Loss: 30.6282\n",
      "Epoch 1081/6000, Training Loss: 30.6329, Validation Loss: 30.6082\n",
      "Epoch 1082/6000, Training Loss: 30.6130, Validation Loss: 30.5882\n",
      "Epoch 1083/6000, Training Loss: 30.5930, Validation Loss: 30.5682\n",
      "Epoch 1084/6000, Training Loss: 30.5730, Validation Loss: 30.5482\n",
      "Epoch 1085/6000, Training Loss: 30.5530, Validation Loss: 30.5281\n",
      "Epoch 1086/6000, Training Loss: 30.5330, Validation Loss: 30.5081\n",
      "Epoch 1087/6000, Training Loss: 30.5131, Validation Loss: 30.4881\n",
      "Epoch 1088/6000, Training Loss: 30.4931, Validation Loss: 30.4681\n",
      "Epoch 1089/6000, Training Loss: 30.4731, Validation Loss: 30.4481\n",
      "Epoch 1090/6000, Training Loss: 30.4531, Validation Loss: 30.4281\n",
      "Epoch 1091/6000, Training Loss: 30.4331, Validation Loss: 30.4080\n",
      "Epoch 1092/6000, Training Loss: 30.4132, Validation Loss: 30.3880\n",
      "Epoch 1093/6000, Training Loss: 30.3932, Validation Loss: 30.3680\n",
      "Epoch 1094/6000, Training Loss: 30.3732, Validation Loss: 30.3480\n",
      "Epoch 1095/6000, Training Loss: 30.3532, Validation Loss: 30.3280\n",
      "Epoch 1096/6000, Training Loss: 30.3332, Validation Loss: 30.3079\n",
      "Epoch 1097/6000, Training Loss: 30.3133, Validation Loss: 30.2879\n",
      "Epoch 1098/6000, Training Loss: 30.2933, Validation Loss: 30.2679\n",
      "Epoch 1099/6000, Training Loss: 30.2733, Validation Loss: 30.2479\n",
      "Epoch 1100/6000, Training Loss: 30.2533, Validation Loss: 30.2279\n",
      "Epoch 1101/6000, Training Loss: 30.2333, Validation Loss: 30.2079\n",
      "Epoch 1102/6000, Training Loss: 30.2134, Validation Loss: 30.1878\n",
      "Epoch 1103/6000, Training Loss: 30.1934, Validation Loss: 30.1678\n",
      "Epoch 1104/6000, Training Loss: 30.1734, Validation Loss: 30.1478\n",
      "Epoch 1105/6000, Training Loss: 30.1534, Validation Loss: 30.1278\n",
      "Epoch 1106/6000, Training Loss: 30.1334, Validation Loss: 30.1078\n",
      "Epoch 1107/6000, Training Loss: 30.1135, Validation Loss: 30.0877\n",
      "Epoch 1108/6000, Training Loss: 30.0935, Validation Loss: 30.0677\n",
      "Epoch 1109/6000, Training Loss: 30.0735, Validation Loss: 30.0477\n",
      "Epoch 1110/6000, Training Loss: 30.0535, Validation Loss: 30.0277\n",
      "Epoch 1111/6000, Training Loss: 30.0335, Validation Loss: 30.0077\n",
      "Epoch 1112/6000, Training Loss: 30.0136, Validation Loss: 29.9876\n",
      "Epoch 1113/6000, Training Loss: 29.9936, Validation Loss: 29.9676\n",
      "Epoch 1114/6000, Training Loss: 29.9736, Validation Loss: 29.9476\n",
      "Epoch 1115/6000, Training Loss: 29.9536, Validation Loss: 29.9276\n",
      "Epoch 1116/6000, Training Loss: 29.9336, Validation Loss: 29.9076\n",
      "Epoch 1117/6000, Training Loss: 29.9137, Validation Loss: 29.8876\n",
      "Epoch 1118/6000, Training Loss: 29.8937, Validation Loss: 29.8675\n",
      "Epoch 1119/6000, Training Loss: 29.8737, Validation Loss: 29.8475\n",
      "Epoch 1120/6000, Training Loss: 29.8537, Validation Loss: 29.8275\n",
      "Epoch 1121/6000, Training Loss: 29.8337, Validation Loss: 29.8075\n",
      "Epoch 1122/6000, Training Loss: 29.8138, Validation Loss: 29.7875\n",
      "Epoch 1123/6000, Training Loss: 29.7938, Validation Loss: 29.7674\n",
      "Epoch 1124/6000, Training Loss: 29.7738, Validation Loss: 29.7474\n",
      "Epoch 1125/6000, Training Loss: 29.7538, Validation Loss: 29.7274\n",
      "Epoch 1126/6000, Training Loss: 29.7338, Validation Loss: 29.7074\n",
      "Epoch 1127/6000, Training Loss: 29.7139, Validation Loss: 29.6874\n",
      "Epoch 1128/6000, Training Loss: 29.6939, Validation Loss: 29.6674\n",
      "Epoch 1129/6000, Training Loss: 29.6739, Validation Loss: 29.6473\n",
      "Epoch 1130/6000, Training Loss: 29.6539, Validation Loss: 29.6273\n",
      "Epoch 1131/6000, Training Loss: 29.6339, Validation Loss: 29.6073\n",
      "Epoch 1132/6000, Training Loss: 29.6140, Validation Loss: 29.5873\n",
      "Epoch 1133/6000, Training Loss: 29.5940, Validation Loss: 29.5673\n",
      "Epoch 1134/6000, Training Loss: 29.5740, Validation Loss: 29.5472\n",
      "Epoch 1135/6000, Training Loss: 29.5540, Validation Loss: 29.5272\n",
      "Epoch 1136/6000, Training Loss: 29.5340, Validation Loss: 29.5072\n",
      "Epoch 1137/6000, Training Loss: 29.5141, Validation Loss: 29.4872\n",
      "Epoch 1138/6000, Training Loss: 29.4941, Validation Loss: 29.4672\n",
      "Epoch 1139/6000, Training Loss: 29.4741, Validation Loss: 29.4472\n",
      "Epoch 1140/6000, Training Loss: 29.4541, Validation Loss: 29.4271\n",
      "Epoch 1141/6000, Training Loss: 29.4342, Validation Loss: 29.4071\n",
      "Epoch 1142/6000, Training Loss: 29.4142, Validation Loss: 29.3871\n",
      "Epoch 1143/6000, Training Loss: 29.3942, Validation Loss: 29.3671\n",
      "Epoch 1144/6000, Training Loss: 29.3742, Validation Loss: 29.3471\n",
      "Epoch 1145/6000, Training Loss: 29.3542, Validation Loss: 29.3270\n",
      "Epoch 1146/6000, Training Loss: 29.3343, Validation Loss: 29.3070\n",
      "Epoch 1147/6000, Training Loss: 29.3143, Validation Loss: 29.2870\n",
      "Epoch 1148/6000, Training Loss: 29.2943, Validation Loss: 29.2670\n",
      "Epoch 1149/6000, Training Loss: 29.2743, Validation Loss: 29.2470\n",
      "Epoch 1150/6000, Training Loss: 29.2543, Validation Loss: 29.2269\n",
      "Epoch 1151/6000, Training Loss: 29.2344, Validation Loss: 29.2069\n",
      "Epoch 1152/6000, Training Loss: 29.2144, Validation Loss: 29.1869\n",
      "Epoch 1153/6000, Training Loss: 29.1944, Validation Loss: 29.1669\n",
      "Epoch 1154/6000, Training Loss: 29.1744, Validation Loss: 29.1469\n",
      "Epoch 1155/6000, Training Loss: 29.1544, Validation Loss: 29.1268\n",
      "Epoch 1156/6000, Training Loss: 29.1345, Validation Loss: 29.1068\n",
      "Epoch 1157/6000, Training Loss: 29.1145, Validation Loss: 29.0868\n",
      "Epoch 1158/6000, Training Loss: 29.0945, Validation Loss: 29.0668\n",
      "Epoch 1159/6000, Training Loss: 29.0745, Validation Loss: 29.0468\n",
      "Epoch 1160/6000, Training Loss: 29.0545, Validation Loss: 29.0267\n",
      "Epoch 1161/6000, Training Loss: 29.0346, Validation Loss: 29.0067\n",
      "Epoch 1162/6000, Training Loss: 29.0146, Validation Loss: 28.9867\n",
      "Epoch 1163/6000, Training Loss: 28.9946, Validation Loss: 28.9667\n",
      "Epoch 1164/6000, Training Loss: 28.9746, Validation Loss: 28.9467\n",
      "Epoch 1165/6000, Training Loss: 28.9546, Validation Loss: 28.9266\n",
      "Epoch 1166/6000, Training Loss: 28.9347, Validation Loss: 28.9066\n",
      "Epoch 1167/6000, Training Loss: 28.9147, Validation Loss: 28.8866\n",
      "Epoch 1168/6000, Training Loss: 28.8947, Validation Loss: 28.8666\n",
      "Epoch 1169/6000, Training Loss: 28.8747, Validation Loss: 28.8466\n",
      "Epoch 1170/6000, Training Loss: 28.8547, Validation Loss: 28.8266\n",
      "Epoch 1171/6000, Training Loss: 28.8348, Validation Loss: 28.8065\n",
      "Epoch 1172/6000, Training Loss: 28.8148, Validation Loss: 28.7865\n",
      "Epoch 1173/6000, Training Loss: 28.7948, Validation Loss: 28.7665\n",
      "Epoch 1174/6000, Training Loss: 28.7748, Validation Loss: 28.7465\n",
      "Epoch 1175/6000, Training Loss: 28.7548, Validation Loss: 28.7265\n",
      "Epoch 1176/6000, Training Loss: 28.7349, Validation Loss: 28.7064\n",
      "Epoch 1177/6000, Training Loss: 28.7149, Validation Loss: 28.6864\n",
      "Epoch 1178/6000, Training Loss: 28.6949, Validation Loss: 28.6664\n",
      "Epoch 1179/6000, Training Loss: 28.6749, Validation Loss: 28.6464\n",
      "Epoch 1180/6000, Training Loss: 28.6549, Validation Loss: 28.6264\n",
      "Epoch 1181/6000, Training Loss: 28.6350, Validation Loss: 28.6063\n",
      "Epoch 1182/6000, Training Loss: 28.6150, Validation Loss: 28.5863\n",
      "Epoch 1183/6000, Training Loss: 28.5950, Validation Loss: 28.5663\n",
      "Epoch 1184/6000, Training Loss: 28.5750, Validation Loss: 28.5463\n",
      "Epoch 1185/6000, Training Loss: 28.5550, Validation Loss: 28.5263\n",
      "Epoch 1186/6000, Training Loss: 28.5351, Validation Loss: 28.5063\n",
      "Epoch 1187/6000, Training Loss: 28.5151, Validation Loss: 28.4862\n",
      "Epoch 1188/6000, Training Loss: 28.4951, Validation Loss: 28.4662\n",
      "Epoch 1189/6000, Training Loss: 28.4751, Validation Loss: 28.4462\n",
      "Epoch 1190/6000, Training Loss: 28.4551, Validation Loss: 28.4262\n",
      "Epoch 1191/6000, Training Loss: 28.4352, Validation Loss: 28.4062\n",
      "Epoch 1192/6000, Training Loss: 28.4152, Validation Loss: 28.3861\n",
      "Epoch 1193/6000, Training Loss: 28.3952, Validation Loss: 28.3661\n",
      "Epoch 1194/6000, Training Loss: 28.3752, Validation Loss: 28.3461\n",
      "Epoch 1195/6000, Training Loss: 28.3552, Validation Loss: 28.3261\n",
      "Epoch 1196/6000, Training Loss: 28.3353, Validation Loss: 28.3061\n",
      "Epoch 1197/6000, Training Loss: 28.3153, Validation Loss: 28.2860\n",
      "Epoch 1198/6000, Training Loss: 28.2953, Validation Loss: 28.2660\n",
      "Epoch 1199/6000, Training Loss: 28.2753, Validation Loss: 28.2460\n",
      "Epoch 1200/6000, Training Loss: 28.2553, Validation Loss: 28.2260\n",
      "Epoch 1201/6000, Training Loss: 28.2354, Validation Loss: 28.2060\n",
      "Epoch 1202/6000, Training Loss: 28.2154, Validation Loss: 28.1859\n",
      "Epoch 1203/6000, Training Loss: 28.1954, Validation Loss: 28.1659\n",
      "Epoch 1204/6000, Training Loss: 28.1754, Validation Loss: 28.1459\n",
      "Epoch 1205/6000, Training Loss: 28.1554, Validation Loss: 28.1259\n",
      "Epoch 1206/6000, Training Loss: 28.1355, Validation Loss: 28.1059\n",
      "Epoch 1207/6000, Training Loss: 28.1155, Validation Loss: 28.0858\n",
      "Epoch 1208/6000, Training Loss: 28.0955, Validation Loss: 28.0658\n",
      "Epoch 1209/6000, Training Loss: 28.0755, Validation Loss: 28.0458\n",
      "Epoch 1210/6000, Training Loss: 28.0555, Validation Loss: 28.0258\n",
      "Epoch 1211/6000, Training Loss: 28.0356, Validation Loss: 28.0057\n",
      "Epoch 1212/6000, Training Loss: 28.0156, Validation Loss: 27.9857\n",
      "Epoch 1213/6000, Training Loss: 27.9956, Validation Loss: 27.9657\n",
      "Epoch 1214/6000, Training Loss: 27.9756, Validation Loss: 27.9457\n",
      "Epoch 1215/6000, Training Loss: 27.9557, Validation Loss: 27.9257\n",
      "Epoch 1216/6000, Training Loss: 27.9357, Validation Loss: 27.9056\n",
      "Epoch 1217/6000, Training Loss: 27.9157, Validation Loss: 27.8856\n",
      "Epoch 1218/6000, Training Loss: 27.8957, Validation Loss: 27.8656\n",
      "Epoch 1219/6000, Training Loss: 27.8757, Validation Loss: 27.8456\n",
      "Epoch 1220/6000, Training Loss: 27.8558, Validation Loss: 27.8256\n",
      "Epoch 1221/6000, Training Loss: 27.8358, Validation Loss: 27.8055\n",
      "Epoch 1222/6000, Training Loss: 27.8158, Validation Loss: 27.7855\n",
      "Epoch 1223/6000, Training Loss: 27.7958, Validation Loss: 27.7655\n",
      "Epoch 1224/6000, Training Loss: 27.7758, Validation Loss: 27.7455\n",
      "Epoch 1225/6000, Training Loss: 27.7559, Validation Loss: 27.7254\n",
      "Epoch 1226/6000, Training Loss: 27.7359, Validation Loss: 27.7054\n",
      "Epoch 1227/6000, Training Loss: 27.7159, Validation Loss: 27.6854\n",
      "Epoch 1228/6000, Training Loss: 27.6959, Validation Loss: 27.6654\n",
      "Epoch 1229/6000, Training Loss: 27.6759, Validation Loss: 27.6454\n",
      "Epoch 1230/6000, Training Loss: 27.6560, Validation Loss: 27.6253\n",
      "Epoch 1231/6000, Training Loss: 27.6360, Validation Loss: 27.6053\n",
      "Epoch 1232/6000, Training Loss: 27.6160, Validation Loss: 27.5853\n",
      "Epoch 1233/6000, Training Loss: 27.5960, Validation Loss: 27.5653\n",
      "Epoch 1234/6000, Training Loss: 27.5760, Validation Loss: 27.5453\n",
      "Epoch 1235/6000, Training Loss: 27.5561, Validation Loss: 27.5252\n",
      "Epoch 1236/6000, Training Loss: 27.5361, Validation Loss: 27.5052\n",
      "Epoch 1237/6000, Training Loss: 27.5161, Validation Loss: 27.4852\n",
      "Epoch 1238/6000, Training Loss: 27.4961, Validation Loss: 27.4652\n",
      "Epoch 1239/6000, Training Loss: 27.4762, Validation Loss: 27.4451\n",
      "Epoch 1240/6000, Training Loss: 27.4562, Validation Loss: 27.4251\n",
      "Epoch 1241/6000, Training Loss: 27.4362, Validation Loss: 27.4051\n",
      "Epoch 1242/6000, Training Loss: 27.4162, Validation Loss: 27.3851\n",
      "Epoch 1243/6000, Training Loss: 27.3962, Validation Loss: 27.3651\n",
      "Epoch 1244/6000, Training Loss: 27.3763, Validation Loss: 27.3450\n",
      "Epoch 1245/6000, Training Loss: 27.3563, Validation Loss: 27.3250\n",
      "Epoch 1246/6000, Training Loss: 27.3363, Validation Loss: 27.3050\n",
      "Epoch 1247/6000, Training Loss: 27.3163, Validation Loss: 27.2850\n",
      "Epoch 1248/6000, Training Loss: 27.2963, Validation Loss: 27.2649\n",
      "Epoch 1249/6000, Training Loss: 27.2764, Validation Loss: 27.2449\n",
      "Epoch 1250/6000, Training Loss: 27.2564, Validation Loss: 27.2249\n",
      "Epoch 1251/6000, Training Loss: 27.2364, Validation Loss: 27.2049\n",
      "Epoch 1252/6000, Training Loss: 27.2164, Validation Loss: 27.1848\n",
      "Epoch 1253/6000, Training Loss: 27.1964, Validation Loss: 27.1648\n",
      "Epoch 1254/6000, Training Loss: 27.1765, Validation Loss: 27.1448\n",
      "Epoch 1255/6000, Training Loss: 27.1565, Validation Loss: 27.1248\n",
      "Epoch 1256/6000, Training Loss: 27.1365, Validation Loss: 27.1047\n",
      "Epoch 1257/6000, Training Loss: 27.1165, Validation Loss: 27.0847\n",
      "Epoch 1258/6000, Training Loss: 27.0966, Validation Loss: 27.0647\n",
      "Epoch 1259/6000, Training Loss: 27.0766, Validation Loss: 27.0447\n",
      "Epoch 1260/6000, Training Loss: 27.0566, Validation Loss: 27.0247\n",
      "Epoch 1261/6000, Training Loss: 27.0366, Validation Loss: 27.0046\n",
      "Epoch 1262/6000, Training Loss: 27.0166, Validation Loss: 26.9846\n",
      "Epoch 1263/6000, Training Loss: 26.9967, Validation Loss: 26.9646\n",
      "Epoch 1264/6000, Training Loss: 26.9767, Validation Loss: 26.9446\n",
      "Epoch 1265/6000, Training Loss: 26.9567, Validation Loss: 26.9245\n",
      "Epoch 1266/6000, Training Loss: 26.9367, Validation Loss: 26.9045\n",
      "Epoch 1267/6000, Training Loss: 26.9167, Validation Loss: 26.8845\n",
      "Epoch 1268/6000, Training Loss: 26.8968, Validation Loss: 26.8645\n",
      "Epoch 1269/6000, Training Loss: 26.8768, Validation Loss: 26.8444\n",
      "Epoch 1270/6000, Training Loss: 26.8568, Validation Loss: 26.8244\n",
      "Epoch 1271/6000, Training Loss: 26.8368, Validation Loss: 26.8044\n",
      "Epoch 1272/6000, Training Loss: 26.8169, Validation Loss: 26.7844\n",
      "Epoch 1273/6000, Training Loss: 26.7969, Validation Loss: 26.7643\n",
      "Epoch 1274/6000, Training Loss: 26.7769, Validation Loss: 26.7443\n",
      "Epoch 1275/6000, Training Loss: 26.7569, Validation Loss: 26.7243\n",
      "Epoch 1276/6000, Training Loss: 26.7369, Validation Loss: 26.7042\n",
      "Epoch 1277/6000, Training Loss: 26.7170, Validation Loss: 26.6842\n",
      "Epoch 1278/6000, Training Loss: 26.6970, Validation Loss: 26.6642\n",
      "Epoch 1279/6000, Training Loss: 26.6770, Validation Loss: 26.6442\n",
      "Epoch 1280/6000, Training Loss: 26.6570, Validation Loss: 26.6241\n",
      "Epoch 1281/6000, Training Loss: 26.6371, Validation Loss: 26.6041\n",
      "Epoch 1282/6000, Training Loss: 26.6171, Validation Loss: 26.5841\n",
      "Epoch 1283/6000, Training Loss: 26.5971, Validation Loss: 26.5640\n",
      "Epoch 1284/6000, Training Loss: 26.5771, Validation Loss: 26.5440\n",
      "Epoch 1285/6000, Training Loss: 26.5571, Validation Loss: 26.5240\n",
      "Epoch 1286/6000, Training Loss: 26.5372, Validation Loss: 26.5040\n",
      "Epoch 1287/6000, Training Loss: 26.5172, Validation Loss: 26.4839\n",
      "Epoch 1288/6000, Training Loss: 26.4972, Validation Loss: 26.4639\n",
      "Epoch 1289/6000, Training Loss: 26.4772, Validation Loss: 26.4439\n",
      "Epoch 1290/6000, Training Loss: 26.4573, Validation Loss: 26.4238\n",
      "Epoch 1291/6000, Training Loss: 26.4373, Validation Loss: 26.4038\n",
      "Epoch 1292/6000, Training Loss: 26.4173, Validation Loss: 26.3838\n",
      "Epoch 1293/6000, Training Loss: 26.3973, Validation Loss: 26.3637\n",
      "Epoch 1294/6000, Training Loss: 26.3774, Validation Loss: 26.3437\n",
      "Epoch 1295/6000, Training Loss: 26.3574, Validation Loss: 26.3237\n",
      "Epoch 1296/6000, Training Loss: 26.3374, Validation Loss: 26.3037\n",
      "Epoch 1297/6000, Training Loss: 26.3174, Validation Loss: 26.2836\n",
      "Epoch 1298/6000, Training Loss: 26.2975, Validation Loss: 26.2636\n",
      "Epoch 1299/6000, Training Loss: 26.2775, Validation Loss: 26.2436\n",
      "Epoch 1300/6000, Training Loss: 26.2575, Validation Loss: 26.2235\n",
      "Epoch 1301/6000, Training Loss: 26.2375, Validation Loss: 26.2035\n",
      "Epoch 1302/6000, Training Loss: 26.2176, Validation Loss: 26.1835\n",
      "Epoch 1303/6000, Training Loss: 26.1976, Validation Loss: 26.1635\n",
      "Epoch 1304/6000, Training Loss: 26.1776, Validation Loss: 26.1434\n",
      "Epoch 1305/6000, Training Loss: 26.1576, Validation Loss: 26.1234\n",
      "Epoch 1306/6000, Training Loss: 26.1377, Validation Loss: 26.1034\n",
      "Epoch 1307/6000, Training Loss: 26.1177, Validation Loss: 26.0833\n",
      "Epoch 1308/6000, Training Loss: 26.0977, Validation Loss: 26.0633\n",
      "Epoch 1309/6000, Training Loss: 26.0777, Validation Loss: 26.0433\n",
      "Epoch 1310/6000, Training Loss: 26.0578, Validation Loss: 26.0232\n",
      "Epoch 1311/6000, Training Loss: 26.0378, Validation Loss: 26.0032\n",
      "Epoch 1312/6000, Training Loss: 26.0178, Validation Loss: 25.9832\n",
      "Epoch 1313/6000, Training Loss: 25.9979, Validation Loss: 25.9632\n",
      "Epoch 1314/6000, Training Loss: 25.9779, Validation Loss: 25.9431\n",
      "Epoch 1315/6000, Training Loss: 25.9579, Validation Loss: 25.9231\n",
      "Epoch 1316/6000, Training Loss: 25.9380, Validation Loss: 25.9031\n",
      "Epoch 1317/6000, Training Loss: 25.9180, Validation Loss: 25.8831\n",
      "Epoch 1318/6000, Training Loss: 25.8981, Validation Loss: 25.8631\n",
      "Epoch 1319/6000, Training Loss: 25.8781, Validation Loss: 25.8430\n",
      "Epoch 1320/6000, Training Loss: 25.8582, Validation Loss: 25.8230\n",
      "Epoch 1321/6000, Training Loss: 25.8382, Validation Loss: 25.8030\n",
      "Epoch 1322/6000, Training Loss: 25.8183, Validation Loss: 25.7830\n",
      "Epoch 1323/6000, Training Loss: 25.7984, Validation Loss: 25.7630\n",
      "Epoch 1324/6000, Training Loss: 25.7785, Validation Loss: 25.7430\n",
      "Epoch 1325/6000, Training Loss: 25.7585, Validation Loss: 25.7230\n",
      "Epoch 1326/6000, Training Loss: 25.7386, Validation Loss: 25.7030\n",
      "Epoch 1327/6000, Training Loss: 25.7187, Validation Loss: 25.6830\n",
      "Epoch 1328/6000, Training Loss: 25.6988, Validation Loss: 25.6630\n",
      "Epoch 1329/6000, Training Loss: 25.6789, Validation Loss: 25.6430\n",
      "Epoch 1330/6000, Training Loss: 25.6589, Validation Loss: 25.6230\n",
      "Epoch 1331/6000, Training Loss: 25.6390, Validation Loss: 25.6030\n",
      "Epoch 1332/6000, Training Loss: 25.6191, Validation Loss: 25.5830\n",
      "Epoch 1333/6000, Training Loss: 25.5992, Validation Loss: 25.5630\n",
      "Epoch 1334/6000, Training Loss: 25.5793, Validation Loss: 25.5430\n",
      "Epoch 1335/6000, Training Loss: 25.5593, Validation Loss: 25.5230\n",
      "Epoch 1336/6000, Training Loss: 25.5394, Validation Loss: 25.5030\n",
      "Epoch 1337/6000, Training Loss: 25.5195, Validation Loss: 25.4830\n",
      "Epoch 1338/6000, Training Loss: 25.4996, Validation Loss: 25.4630\n",
      "Epoch 1339/6000, Training Loss: 25.4797, Validation Loss: 25.4430\n",
      "Epoch 1340/6000, Training Loss: 25.4598, Validation Loss: 25.4230\n",
      "Epoch 1341/6000, Training Loss: 25.4398, Validation Loss: 25.4031\n",
      "Epoch 1342/6000, Training Loss: 25.4199, Validation Loss: 25.3831\n",
      "Epoch 1343/6000, Training Loss: 25.4000, Validation Loss: 25.3631\n",
      "Epoch 1344/6000, Training Loss: 25.3801, Validation Loss: 25.3431\n",
      "Epoch 1345/6000, Training Loss: 25.3602, Validation Loss: 25.3231\n",
      "Epoch 1346/6000, Training Loss: 25.3403, Validation Loss: 25.3031\n",
      "Epoch 1347/6000, Training Loss: 25.3204, Validation Loss: 25.2831\n",
      "Epoch 1348/6000, Training Loss: 25.3005, Validation Loss: 25.2631\n",
      "Epoch 1349/6000, Training Loss: 25.2806, Validation Loss: 25.2431\n",
      "Epoch 1350/6000, Training Loss: 25.2607, Validation Loss: 25.2232\n",
      "Epoch 1351/6000, Training Loss: 25.2408, Validation Loss: 25.2032\n",
      "Epoch 1352/6000, Training Loss: 25.2208, Validation Loss: 25.1832\n",
      "Epoch 1353/6000, Training Loss: 25.2009, Validation Loss: 25.1632\n",
      "Epoch 1354/6000, Training Loss: 25.1810, Validation Loss: 25.1432\n",
      "Epoch 1355/6000, Training Loss: 25.1611, Validation Loss: 25.1232\n",
      "Epoch 1356/6000, Training Loss: 25.1412, Validation Loss: 25.1033\n",
      "Epoch 1357/6000, Training Loss: 25.1213, Validation Loss: 25.0833\n",
      "Epoch 1358/6000, Training Loss: 25.1014, Validation Loss: 25.0633\n",
      "Epoch 1359/6000, Training Loss: 25.0815, Validation Loss: 25.0433\n",
      "Epoch 1360/6000, Training Loss: 25.0616, Validation Loss: 25.0233\n",
      "Epoch 1361/6000, Training Loss: 25.0417, Validation Loss: 25.0034\n",
      "Epoch 1362/6000, Training Loss: 25.0218, Validation Loss: 24.9834\n",
      "Epoch 1363/6000, Training Loss: 25.0019, Validation Loss: 24.9634\n",
      "Epoch 1364/6000, Training Loss: 24.9820, Validation Loss: 24.9434\n",
      "Epoch 1365/6000, Training Loss: 24.9621, Validation Loss: 24.9235\n",
      "Epoch 1366/6000, Training Loss: 24.9422, Validation Loss: 24.9035\n",
      "Epoch 1367/6000, Training Loss: 24.9223, Validation Loss: 24.8835\n",
      "Epoch 1368/6000, Training Loss: 24.9025, Validation Loss: 24.8635\n",
      "Epoch 1369/6000, Training Loss: 24.8826, Validation Loss: 24.8436\n",
      "Epoch 1370/6000, Training Loss: 24.8627, Validation Loss: 24.8236\n",
      "Epoch 1371/6000, Training Loss: 24.8428, Validation Loss: 24.8036\n",
      "Epoch 1372/6000, Training Loss: 24.8229, Validation Loss: 24.7837\n",
      "Epoch 1373/6000, Training Loss: 24.8030, Validation Loss: 24.7637\n",
      "Epoch 1374/6000, Training Loss: 24.7831, Validation Loss: 24.7437\n",
      "Epoch 1375/6000, Training Loss: 24.7632, Validation Loss: 24.7238\n",
      "Epoch 1376/6000, Training Loss: 24.7433, Validation Loss: 24.7038\n",
      "Epoch 1377/6000, Training Loss: 24.7235, Validation Loss: 24.6839\n",
      "Epoch 1378/6000, Training Loss: 24.7036, Validation Loss: 24.6639\n",
      "Epoch 1379/6000, Training Loss: 24.6837, Validation Loss: 24.6439\n",
      "Epoch 1380/6000, Training Loss: 24.6638, Validation Loss: 24.6240\n",
      "Epoch 1381/6000, Training Loss: 24.6439, Validation Loss: 24.6040\n",
      "Epoch 1382/6000, Training Loss: 24.6240, Validation Loss: 24.5841\n",
      "Epoch 1383/6000, Training Loss: 24.6042, Validation Loss: 24.5641\n",
      "Epoch 1384/6000, Training Loss: 24.5843, Validation Loss: 24.5441\n",
      "Epoch 1385/6000, Training Loss: 24.5644, Validation Loss: 24.5242\n",
      "Epoch 1386/6000, Training Loss: 24.5445, Validation Loss: 24.5042\n",
      "Epoch 1387/6000, Training Loss: 24.5246, Validation Loss: 24.4843\n",
      "Epoch 1388/6000, Training Loss: 24.5048, Validation Loss: 24.4643\n",
      "Epoch 1389/6000, Training Loss: 24.4849, Validation Loss: 24.4444\n",
      "Epoch 1390/6000, Training Loss: 24.4650, Validation Loss: 24.4244\n",
      "Epoch 1391/6000, Training Loss: 24.4451, Validation Loss: 24.4045\n",
      "Epoch 1392/6000, Training Loss: 24.4253, Validation Loss: 24.3845\n",
      "Epoch 1393/6000, Training Loss: 24.4054, Validation Loss: 24.3646\n",
      "Epoch 1394/6000, Training Loss: 24.3855, Validation Loss: 24.3447\n",
      "Epoch 1395/6000, Training Loss: 24.3657, Validation Loss: 24.3247\n",
      "Epoch 1396/6000, Training Loss: 24.3458, Validation Loss: 24.3048\n",
      "Epoch 1397/6000, Training Loss: 24.3259, Validation Loss: 24.2848\n",
      "Epoch 1398/6000, Training Loss: 24.3061, Validation Loss: 24.2649\n",
      "Epoch 1399/6000, Training Loss: 24.2862, Validation Loss: 24.2449\n",
      "Epoch 1400/6000, Training Loss: 24.2663, Validation Loss: 24.2250\n",
      "Epoch 1401/6000, Training Loss: 24.2465, Validation Loss: 24.2051\n",
      "Epoch 1402/6000, Training Loss: 24.2266, Validation Loss: 24.1851\n",
      "Epoch 1403/6000, Training Loss: 24.2068, Validation Loss: 24.1652\n",
      "Epoch 1404/6000, Training Loss: 24.1869, Validation Loss: 24.1453\n",
      "Epoch 1405/6000, Training Loss: 24.1670, Validation Loss: 24.1253\n",
      "Epoch 1406/6000, Training Loss: 24.1472, Validation Loss: 24.1054\n",
      "Epoch 1407/6000, Training Loss: 24.1273, Validation Loss: 24.0855\n",
      "Epoch 1408/6000, Training Loss: 24.1075, Validation Loss: 24.0655\n",
      "Epoch 1409/6000, Training Loss: 24.0876, Validation Loss: 24.0456\n",
      "Epoch 1410/6000, Training Loss: 24.0678, Validation Loss: 24.0257\n",
      "Epoch 1411/6000, Training Loss: 24.0479, Validation Loss: 24.0058\n",
      "Epoch 1412/6000, Training Loss: 24.0281, Validation Loss: 23.9858\n",
      "Epoch 1413/6000, Training Loss: 24.0082, Validation Loss: 23.9659\n",
      "Epoch 1414/6000, Training Loss: 23.9884, Validation Loss: 23.9460\n",
      "Epoch 1415/6000, Training Loss: 23.9685, Validation Loss: 23.9261\n",
      "Epoch 1416/6000, Training Loss: 23.9487, Validation Loss: 23.9062\n",
      "Epoch 1417/6000, Training Loss: 23.9288, Validation Loss: 23.8862\n",
      "Epoch 1418/6000, Training Loss: 23.9090, Validation Loss: 23.8663\n",
      "Epoch 1419/6000, Training Loss: 23.8891, Validation Loss: 23.8464\n",
      "Epoch 1420/6000, Training Loss: 23.8693, Validation Loss: 23.8265\n",
      "Epoch 1421/6000, Training Loss: 23.8494, Validation Loss: 23.8066\n",
      "Epoch 1422/6000, Training Loss: 23.8296, Validation Loss: 23.7867\n",
      "Epoch 1423/6000, Training Loss: 23.8098, Validation Loss: 23.7668\n",
      "Epoch 1424/6000, Training Loss: 23.7899, Validation Loss: 23.7469\n",
      "Epoch 1425/6000, Training Loss: 23.7701, Validation Loss: 23.7270\n",
      "Epoch 1426/6000, Training Loss: 23.7503, Validation Loss: 23.7071\n",
      "Epoch 1427/6000, Training Loss: 23.7304, Validation Loss: 23.6871\n",
      "Epoch 1428/6000, Training Loss: 23.7106, Validation Loss: 23.6672\n",
      "Epoch 1429/6000, Training Loss: 23.6908, Validation Loss: 23.6473\n",
      "Epoch 1430/6000, Training Loss: 23.6709, Validation Loss: 23.6274\n",
      "Epoch 1431/6000, Training Loss: 23.6511, Validation Loss: 23.6075\n",
      "Epoch 1432/6000, Training Loss: 23.6313, Validation Loss: 23.5876\n",
      "Epoch 1433/6000, Training Loss: 23.6114, Validation Loss: 23.5677\n",
      "Epoch 1434/6000, Training Loss: 23.5916, Validation Loss: 23.5479\n",
      "Epoch 1435/6000, Training Loss: 23.5718, Validation Loss: 23.5280\n",
      "Epoch 1436/6000, Training Loss: 23.5520, Validation Loss: 23.5081\n",
      "Epoch 1437/6000, Training Loss: 23.5321, Validation Loss: 23.4882\n",
      "Epoch 1438/6000, Training Loss: 23.5123, Validation Loss: 23.4683\n",
      "Epoch 1439/6000, Training Loss: 23.4925, Validation Loss: 23.4484\n",
      "Epoch 1440/6000, Training Loss: 23.4727, Validation Loss: 23.4285\n",
      "Epoch 1441/6000, Training Loss: 23.4529, Validation Loss: 23.4086\n",
      "Epoch 1442/6000, Training Loss: 23.4331, Validation Loss: 23.3887\n",
      "Epoch 1443/6000, Training Loss: 23.4132, Validation Loss: 23.3688\n",
      "Epoch 1444/6000, Training Loss: 23.3934, Validation Loss: 23.3490\n",
      "Epoch 1445/6000, Training Loss: 23.3736, Validation Loss: 23.3291\n",
      "Epoch 1446/6000, Training Loss: 23.3538, Validation Loss: 23.3092\n",
      "Epoch 1447/6000, Training Loss: 23.3340, Validation Loss: 23.2893\n",
      "Epoch 1448/6000, Training Loss: 23.3142, Validation Loss: 23.2694\n",
      "Epoch 1449/6000, Training Loss: 23.2944, Validation Loss: 23.2496\n",
      "Epoch 1450/6000, Training Loss: 23.2746, Validation Loss: 23.2297\n",
      "Epoch 1451/6000, Training Loss: 23.2548, Validation Loss: 23.2098\n",
      "Epoch 1452/6000, Training Loss: 23.2350, Validation Loss: 23.1900\n",
      "Epoch 1453/6000, Training Loss: 23.2152, Validation Loss: 23.1701\n",
      "Epoch 1454/6000, Training Loss: 23.1954, Validation Loss: 23.1502\n",
      "Epoch 1455/6000, Training Loss: 23.1756, Validation Loss: 23.1304\n",
      "Epoch 1456/6000, Training Loss: 23.1558, Validation Loss: 23.1105\n",
      "Epoch 1457/6000, Training Loss: 23.1360, Validation Loss: 23.0906\n",
      "Epoch 1458/6000, Training Loss: 23.1162, Validation Loss: 23.0708\n",
      "Epoch 1459/6000, Training Loss: 23.0964, Validation Loss: 23.0509\n",
      "Epoch 1460/6000, Training Loss: 23.0766, Validation Loss: 23.0310\n",
      "Epoch 1461/6000, Training Loss: 23.0568, Validation Loss: 23.0112\n",
      "Epoch 1462/6000, Training Loss: 23.0370, Validation Loss: 22.9913\n",
      "Epoch 1463/6000, Training Loss: 23.0172, Validation Loss: 22.9715\n",
      "Epoch 1464/6000, Training Loss: 22.9974, Validation Loss: 22.9516\n",
      "Epoch 1465/6000, Training Loss: 22.9776, Validation Loss: 22.9318\n",
      "Epoch 1466/6000, Training Loss: 22.9578, Validation Loss: 22.9119\n",
      "Epoch 1467/6000, Training Loss: 22.9381, Validation Loss: 22.8921\n",
      "Epoch 1468/6000, Training Loss: 22.9183, Validation Loss: 22.8722\n",
      "Epoch 1469/6000, Training Loss: 22.8985, Validation Loss: 22.8524\n",
      "Epoch 1470/6000, Training Loss: 22.8787, Validation Loss: 22.8325\n",
      "Epoch 1471/6000, Training Loss: 22.8589, Validation Loss: 22.8127\n",
      "Epoch 1472/6000, Training Loss: 22.8391, Validation Loss: 22.7929\n",
      "Epoch 1473/6000, Training Loss: 22.8194, Validation Loss: 22.7730\n",
      "Epoch 1474/6000, Training Loss: 22.7996, Validation Loss: 22.7532\n",
      "Epoch 1475/6000, Training Loss: 22.7798, Validation Loss: 22.7334\n",
      "Epoch 1476/6000, Training Loss: 22.7600, Validation Loss: 22.7135\n",
      "Epoch 1477/6000, Training Loss: 22.7403, Validation Loss: 22.6937\n",
      "Epoch 1478/6000, Training Loss: 22.7205, Validation Loss: 22.6738\n",
      "Epoch 1479/6000, Training Loss: 22.7007, Validation Loss: 22.6540\n",
      "Epoch 1480/6000, Training Loss: 22.6809, Validation Loss: 22.6342\n",
      "Epoch 1481/6000, Training Loss: 22.6612, Validation Loss: 22.6144\n",
      "Epoch 1482/6000, Training Loss: 22.6414, Validation Loss: 22.5945\n",
      "Epoch 1483/6000, Training Loss: 22.6216, Validation Loss: 22.5747\n",
      "Epoch 1484/6000, Training Loss: 22.6019, Validation Loss: 22.5549\n",
      "Epoch 1485/6000, Training Loss: 22.5821, Validation Loss: 22.5350\n",
      "Epoch 1486/6000, Training Loss: 22.5623, Validation Loss: 22.5152\n",
      "Epoch 1487/6000, Training Loss: 22.5426, Validation Loss: 22.4954\n",
      "Epoch 1488/6000, Training Loss: 22.5228, Validation Loss: 22.4756\n",
      "Epoch 1489/6000, Training Loss: 22.5030, Validation Loss: 22.4557\n",
      "Epoch 1490/6000, Training Loss: 22.4833, Validation Loss: 22.4359\n",
      "Epoch 1491/6000, Training Loss: 22.4635, Validation Loss: 22.4161\n",
      "Epoch 1492/6000, Training Loss: 22.4438, Validation Loss: 22.3963\n",
      "Epoch 1493/6000, Training Loss: 22.4240, Validation Loss: 22.3765\n",
      "Epoch 1494/6000, Training Loss: 22.4043, Validation Loss: 22.3566\n",
      "Epoch 1495/6000, Training Loss: 22.3845, Validation Loss: 22.3368\n",
      "Epoch 1496/6000, Training Loss: 22.3648, Validation Loss: 22.3170\n",
      "Epoch 1497/6000, Training Loss: 22.3450, Validation Loss: 22.2972\n",
      "Epoch 1498/6000, Training Loss: 22.3253, Validation Loss: 22.2774\n",
      "Epoch 1499/6000, Training Loss: 22.3055, Validation Loss: 22.2576\n",
      "Epoch 1500/6000, Training Loss: 22.2858, Validation Loss: 22.2378\n",
      "Epoch 1501/6000, Training Loss: 22.2661, Validation Loss: 22.2181\n",
      "Epoch 1502/6000, Training Loss: 22.2464, Validation Loss: 22.1984\n",
      "Epoch 1503/6000, Training Loss: 22.2268, Validation Loss: 22.1787\n",
      "Epoch 1504/6000, Training Loss: 22.2072, Validation Loss: 22.1591\n",
      "Epoch 1505/6000, Training Loss: 22.1877, Validation Loss: 22.1395\n",
      "Epoch 1506/6000, Training Loss: 22.1682, Validation Loss: 22.1200\n",
      "Epoch 1507/6000, Training Loss: 22.1488, Validation Loss: 22.1005\n",
      "Epoch 1508/6000, Training Loss: 22.1294, Validation Loss: 22.0811\n",
      "Epoch 1509/6000, Training Loss: 22.1100, Validation Loss: 22.0618\n",
      "Epoch 1510/6000, Training Loss: 22.0907, Validation Loss: 22.0425\n",
      "Epoch 1511/6000, Training Loss: 22.0715, Validation Loss: 22.0232\n",
      "Epoch 1512/6000, Training Loss: 22.0522, Validation Loss: 22.0039\n",
      "Epoch 1513/6000, Training Loss: 22.0330, Validation Loss: 21.9847\n",
      "Epoch 1514/6000, Training Loss: 22.0138, Validation Loss: 21.9655\n",
      "Epoch 1515/6000, Training Loss: 21.9946, Validation Loss: 21.9463\n",
      "Epoch 1516/6000, Training Loss: 21.9755, Validation Loss: 21.9272\n",
      "Epoch 1517/6000, Training Loss: 21.9563, Validation Loss: 21.9081\n",
      "Epoch 1518/6000, Training Loss: 21.9373, Validation Loss: 21.8890\n",
      "Epoch 1519/6000, Training Loss: 21.9182, Validation Loss: 21.8700\n",
      "Epoch 1520/6000, Training Loss: 21.8992, Validation Loss: 21.8510\n",
      "Epoch 1521/6000, Training Loss: 21.8803, Validation Loss: 21.8321\n",
      "Epoch 1522/6000, Training Loss: 21.8614, Validation Loss: 21.8132\n",
      "Epoch 1523/6000, Training Loss: 21.8425, Validation Loss: 21.7944\n",
      "Epoch 1524/6000, Training Loss: 21.8237, Validation Loss: 21.7756\n",
      "Epoch 1525/6000, Training Loss: 21.8049, Validation Loss: 21.7569\n",
      "Epoch 1526/6000, Training Loss: 21.7862, Validation Loss: 21.7382\n",
      "Epoch 1527/6000, Training Loss: 21.7675, Validation Loss: 21.7195\n",
      "Epoch 1528/6000, Training Loss: 21.7488, Validation Loss: 21.7009\n",
      "Epoch 1529/6000, Training Loss: 21.7302, Validation Loss: 21.6823\n",
      "Epoch 1530/6000, Training Loss: 21.7116, Validation Loss: 21.6637\n",
      "Epoch 1531/6000, Training Loss: 21.6930, Validation Loss: 21.6452\n",
      "Epoch 1532/6000, Training Loss: 21.6744, Validation Loss: 21.6267\n",
      "Epoch 1533/6000, Training Loss: 21.6559, Validation Loss: 21.6082\n",
      "Epoch 1534/6000, Training Loss: 21.6374, Validation Loss: 21.5897\n",
      "Epoch 1535/6000, Training Loss: 21.6189, Validation Loss: 21.5713\n",
      "Epoch 1536/6000, Training Loss: 21.6005, Validation Loss: 21.5529\n",
      "Epoch 1537/6000, Training Loss: 21.5820, Validation Loss: 21.5345\n",
      "Epoch 1538/6000, Training Loss: 21.5636, Validation Loss: 21.5161\n",
      "Epoch 1539/6000, Training Loss: 21.5452, Validation Loss: 21.4977\n",
      "Epoch 1540/6000, Training Loss: 21.5268, Validation Loss: 21.4794\n",
      "Epoch 1541/6000, Training Loss: 21.5084, Validation Loss: 21.4611\n",
      "Epoch 1542/6000, Training Loss: 21.4901, Validation Loss: 21.4428\n",
      "Epoch 1543/6000, Training Loss: 21.4718, Validation Loss: 21.4246\n",
      "Epoch 1544/6000, Training Loss: 21.4536, Validation Loss: 21.4064\n",
      "Epoch 1545/6000, Training Loss: 21.4354, Validation Loss: 21.3882\n",
      "Epoch 1546/6000, Training Loss: 21.4172, Validation Loss: 21.3700\n",
      "Epoch 1547/6000, Training Loss: 21.3990, Validation Loss: 21.3519\n",
      "Epoch 1548/6000, Training Loss: 21.3809, Validation Loss: 21.3339\n",
      "Epoch 1549/6000, Training Loss: 21.3629, Validation Loss: 21.3159\n",
      "Epoch 1550/6000, Training Loss: 21.3449, Validation Loss: 21.2979\n",
      "Epoch 1551/6000, Training Loss: 21.3269, Validation Loss: 21.2800\n",
      "Epoch 1552/6000, Training Loss: 21.3090, Validation Loss: 21.2622\n",
      "Epoch 1553/6000, Training Loss: 21.2911, Validation Loss: 21.2443\n",
      "Epoch 1554/6000, Training Loss: 21.2733, Validation Loss: 21.2265\n",
      "Epoch 1555/6000, Training Loss: 21.2555, Validation Loss: 21.2088\n",
      "Epoch 1556/6000, Training Loss: 21.2377, Validation Loss: 21.1911\n",
      "Epoch 1557/6000, Training Loss: 21.2199, Validation Loss: 21.1734\n",
      "Epoch 1558/6000, Training Loss: 21.2022, Validation Loss: 21.1557\n",
      "Epoch 1559/6000, Training Loss: 21.1845, Validation Loss: 21.1380\n",
      "Epoch 1560/6000, Training Loss: 21.1668, Validation Loss: 21.1204\n",
      "Epoch 1561/6000, Training Loss: 21.1491, Validation Loss: 21.1028\n",
      "Epoch 1562/6000, Training Loss: 21.1314, Validation Loss: 21.0852\n",
      "Epoch 1563/6000, Training Loss: 21.1138, Validation Loss: 21.0676\n",
      "Epoch 1564/6000, Training Loss: 21.0961, Validation Loss: 21.0500\n",
      "Epoch 1565/6000, Training Loss: 21.0785, Validation Loss: 21.0325\n",
      "Epoch 1566/6000, Training Loss: 21.0609, Validation Loss: 21.0150\n",
      "Epoch 1567/6000, Training Loss: 21.0434, Validation Loss: 20.9975\n",
      "Epoch 1568/6000, Training Loss: 21.0258, Validation Loss: 20.9800\n",
      "Epoch 1569/6000, Training Loss: 21.0083, Validation Loss: 20.9626\n",
      "Epoch 1570/6000, Training Loss: 20.9909, Validation Loss: 20.9452\n",
      "Epoch 1571/6000, Training Loss: 20.9734, Validation Loss: 20.9278\n",
      "Epoch 1572/6000, Training Loss: 20.9560, Validation Loss: 20.9105\n",
      "Epoch 1573/6000, Training Loss: 20.9387, Validation Loss: 20.8932\n",
      "Epoch 1574/6000, Training Loss: 20.9214, Validation Loss: 20.8760\n",
      "Epoch 1575/6000, Training Loss: 20.9041, Validation Loss: 20.8588\n",
      "Epoch 1576/6000, Training Loss: 20.8869, Validation Loss: 20.8416\n",
      "Epoch 1577/6000, Training Loss: 20.8697, Validation Loss: 20.8245\n",
      "Epoch 1578/6000, Training Loss: 20.8525, Validation Loss: 20.8074\n",
      "Epoch 1579/6000, Training Loss: 20.8354, Validation Loss: 20.7904\n",
      "Epoch 1580/6000, Training Loss: 20.8183, Validation Loss: 20.7734\n",
      "Epoch 1581/6000, Training Loss: 20.8013, Validation Loss: 20.7564\n",
      "Epoch 1582/6000, Training Loss: 20.7842, Validation Loss: 20.7394\n",
      "Epoch 1583/6000, Training Loss: 20.7672, Validation Loss: 20.7225\n",
      "Epoch 1584/6000, Training Loss: 20.7503, Validation Loss: 20.7056\n",
      "Epoch 1585/6000, Training Loss: 20.7333, Validation Loss: 20.6887\n",
      "Epoch 1586/6000, Training Loss: 20.7164, Validation Loss: 20.6718\n",
      "Epoch 1587/6000, Training Loss: 20.6995, Validation Loss: 20.6550\n",
      "Epoch 1588/6000, Training Loss: 20.6826, Validation Loss: 20.6382\n",
      "Epoch 1589/6000, Training Loss: 20.6657, Validation Loss: 20.6213\n",
      "Epoch 1590/6000, Training Loss: 20.6488, Validation Loss: 20.6045\n",
      "Epoch 1591/6000, Training Loss: 20.6320, Validation Loss: 20.5878\n",
      "Epoch 1592/6000, Training Loss: 20.6152, Validation Loss: 20.5710\n",
      "Epoch 1593/6000, Training Loss: 20.5984, Validation Loss: 20.5542\n",
      "Epoch 1594/6000, Training Loss: 20.5816, Validation Loss: 20.5375\n",
      "Epoch 1595/6000, Training Loss: 20.5648, Validation Loss: 20.5208\n",
      "Epoch 1596/6000, Training Loss: 20.5481, Validation Loss: 20.5042\n",
      "Epoch 1597/6000, Training Loss: 20.5314, Validation Loss: 20.4876\n",
      "Epoch 1598/6000, Training Loss: 20.5147, Validation Loss: 20.4710\n",
      "Epoch 1599/6000, Training Loss: 20.4981, Validation Loss: 20.4545\n",
      "Epoch 1600/6000, Training Loss: 20.4815, Validation Loss: 20.4380\n",
      "Epoch 1601/6000, Training Loss: 20.4650, Validation Loss: 20.4216\n",
      "Epoch 1602/6000, Training Loss: 20.4485, Validation Loss: 20.4052\n",
      "Epoch 1603/6000, Training Loss: 20.4320, Validation Loss: 20.3888\n",
      "Epoch 1604/6000, Training Loss: 20.4156, Validation Loss: 20.3724\n",
      "Epoch 1605/6000, Training Loss: 20.3992, Validation Loss: 20.3561\n",
      "Epoch 1606/6000, Training Loss: 20.3829, Validation Loss: 20.3399\n",
      "Epoch 1607/6000, Training Loss: 20.3665, Validation Loss: 20.3237\n",
      "Epoch 1608/6000, Training Loss: 20.3503, Validation Loss: 20.3075\n",
      "Epoch 1609/6000, Training Loss: 20.3340, Validation Loss: 20.2913\n",
      "Epoch 1610/6000, Training Loss: 20.3178, Validation Loss: 20.2751\n",
      "Epoch 1611/6000, Training Loss: 20.3015, Validation Loss: 20.2590\n",
      "Epoch 1612/6000, Training Loss: 20.2854, Validation Loss: 20.2429\n",
      "Epoch 1613/6000, Training Loss: 20.2692, Validation Loss: 20.2268\n",
      "Epoch 1614/6000, Training Loss: 20.2530, Validation Loss: 20.2108\n",
      "Epoch 1615/6000, Training Loss: 20.2369, Validation Loss: 20.1947\n",
      "Epoch 1616/6000, Training Loss: 20.2208, Validation Loss: 20.1787\n",
      "Epoch 1617/6000, Training Loss: 20.2047, Validation Loss: 20.1627\n",
      "Epoch 1618/6000, Training Loss: 20.1886, Validation Loss: 20.1467\n",
      "Epoch 1619/6000, Training Loss: 20.1725, Validation Loss: 20.1307\n",
      "Epoch 1620/6000, Training Loss: 20.1565, Validation Loss: 20.1147\n",
      "Epoch 1621/6000, Training Loss: 20.1405, Validation Loss: 20.0987\n",
      "Epoch 1622/6000, Training Loss: 20.1245, Validation Loss: 20.0828\n",
      "Epoch 1623/6000, Training Loss: 20.1085, Validation Loss: 20.0669\n",
      "Epoch 1624/6000, Training Loss: 20.0926, Validation Loss: 20.0510\n",
      "Epoch 1625/6000, Training Loss: 20.0767, Validation Loss: 20.0352\n",
      "Epoch 1626/6000, Training Loss: 20.0608, Validation Loss: 20.0194\n",
      "Epoch 1627/6000, Training Loss: 20.0449, Validation Loss: 20.0036\n",
      "Epoch 1628/6000, Training Loss: 20.0291, Validation Loss: 19.9879\n",
      "Epoch 1629/6000, Training Loss: 20.0134, Validation Loss: 19.9722\n",
      "Epoch 1630/6000, Training Loss: 19.9977, Validation Loss: 19.9566\n",
      "Epoch 1631/6000, Training Loss: 19.9820, Validation Loss: 19.9409\n",
      "Epoch 1632/6000, Training Loss: 19.9663, Validation Loss: 19.9253\n",
      "Epoch 1633/6000, Training Loss: 19.9507, Validation Loss: 19.9098\n",
      "Epoch 1634/6000, Training Loss: 19.9351, Validation Loss: 19.8943\n",
      "Epoch 1635/6000, Training Loss: 19.9195, Validation Loss: 19.8788\n",
      "Epoch 1636/6000, Training Loss: 19.9040, Validation Loss: 19.8633\n",
      "Epoch 1637/6000, Training Loss: 19.8885, Validation Loss: 19.8478\n",
      "Epoch 1638/6000, Training Loss: 19.8730, Validation Loss: 19.8324\n",
      "Epoch 1639/6000, Training Loss: 19.8576, Validation Loss: 19.8170\n",
      "Epoch 1640/6000, Training Loss: 19.8421, Validation Loss: 19.8017\n",
      "Epoch 1641/6000, Training Loss: 19.8267, Validation Loss: 19.7863\n",
      "Epoch 1642/6000, Training Loss: 19.8113, Validation Loss: 19.7710\n",
      "Epoch 1643/6000, Training Loss: 19.7959, Validation Loss: 19.7556\n",
      "Epoch 1644/6000, Training Loss: 19.7805, Validation Loss: 19.7403\n",
      "Epoch 1645/6000, Training Loss: 19.7651, Validation Loss: 19.7251\n",
      "Epoch 1646/6000, Training Loss: 19.7498, Validation Loss: 19.7098\n",
      "Epoch 1647/6000, Training Loss: 19.7345, Validation Loss: 19.6945\n",
      "Epoch 1648/6000, Training Loss: 19.7191, Validation Loss: 19.6793\n",
      "Epoch 1649/6000, Training Loss: 19.7039, Validation Loss: 19.6640\n",
      "Epoch 1650/6000, Training Loss: 19.6886, Validation Loss: 19.6488\n",
      "Epoch 1651/6000, Training Loss: 19.6733, Validation Loss: 19.6336\n",
      "Epoch 1652/6000, Training Loss: 19.6581, Validation Loss: 19.6184\n",
      "Epoch 1653/6000, Training Loss: 19.6429, Validation Loss: 19.6033\n",
      "Epoch 1654/6000, Training Loss: 19.6278, Validation Loss: 19.5882\n",
      "Epoch 1655/6000, Training Loss: 19.6127, Validation Loss: 19.5732\n",
      "Epoch 1656/6000, Training Loss: 19.5976, Validation Loss: 19.5581\n",
      "Epoch 1657/6000, Training Loss: 19.5825, Validation Loss: 19.5432\n",
      "Epoch 1658/6000, Training Loss: 19.5675, Validation Loss: 19.5282\n",
      "Epoch 1659/6000, Training Loss: 19.5525, Validation Loss: 19.5133\n",
      "Epoch 1660/6000, Training Loss: 19.5376, Validation Loss: 19.4984\n",
      "Epoch 1661/6000, Training Loss: 19.5226, Validation Loss: 19.4836\n",
      "Epoch 1662/6000, Training Loss: 19.5078, Validation Loss: 19.4688\n",
      "Epoch 1663/6000, Training Loss: 19.4929, Validation Loss: 19.4540\n",
      "Epoch 1664/6000, Training Loss: 19.4781, Validation Loss: 19.4392\n",
      "Epoch 1665/6000, Training Loss: 19.4633, Validation Loss: 19.4245\n",
      "Epoch 1666/6000, Training Loss: 19.4485, Validation Loss: 19.4098\n",
      "Epoch 1667/6000, Training Loss: 19.4337, Validation Loss: 19.3951\n",
      "Epoch 1668/6000, Training Loss: 19.4190, Validation Loss: 19.3804\n",
      "Epoch 1669/6000, Training Loss: 19.4042, Validation Loss: 19.3657\n",
      "Epoch 1670/6000, Training Loss: 19.3895, Validation Loss: 19.3510\n",
      "Epoch 1671/6000, Training Loss: 19.3748, Validation Loss: 19.3364\n",
      "Epoch 1672/6000, Training Loss: 19.3601, Validation Loss: 19.3218\n",
      "Epoch 1673/6000, Training Loss: 19.3455, Validation Loss: 19.3072\n",
      "Epoch 1674/6000, Training Loss: 19.3308, Validation Loss: 19.2926\n",
      "Epoch 1675/6000, Training Loss: 19.3162, Validation Loss: 19.2780\n",
      "Epoch 1676/6000, Training Loss: 19.3016, Validation Loss: 19.2634\n",
      "Epoch 1677/6000, Training Loss: 19.2870, Validation Loss: 19.2489\n",
      "Epoch 1678/6000, Training Loss: 19.2724, Validation Loss: 19.2344\n",
      "Epoch 1679/6000, Training Loss: 19.2579, Validation Loss: 19.2199\n",
      "Epoch 1680/6000, Training Loss: 19.2433, Validation Loss: 19.2054\n",
      "Epoch 1681/6000, Training Loss: 19.2288, Validation Loss: 19.1909\n",
      "Epoch 1682/6000, Training Loss: 19.2143, Validation Loss: 19.1765\n",
      "Epoch 1683/6000, Training Loss: 19.1999, Validation Loss: 19.1621\n",
      "Epoch 1684/6000, Training Loss: 19.1854, Validation Loss: 19.1477\n",
      "Epoch 1685/6000, Training Loss: 19.1710, Validation Loss: 19.1334\n",
      "Epoch 1686/6000, Training Loss: 19.1567, Validation Loss: 19.1191\n",
      "Epoch 1687/6000, Training Loss: 19.1424, Validation Loss: 19.1049\n",
      "Epoch 1688/6000, Training Loss: 19.1281, Validation Loss: 19.0907\n",
      "Epoch 1689/6000, Training Loss: 19.1139, Validation Loss: 19.0765\n",
      "Epoch 1690/6000, Training Loss: 19.0997, Validation Loss: 19.0624\n",
      "Epoch 1691/6000, Training Loss: 19.0855, Validation Loss: 19.0483\n",
      "Epoch 1692/6000, Training Loss: 19.0713, Validation Loss: 19.0342\n",
      "Epoch 1693/6000, Training Loss: 19.0572, Validation Loss: 19.0201\n",
      "Epoch 1694/6000, Training Loss: 19.0431, Validation Loss: 19.0060\n",
      "Epoch 1695/6000, Training Loss: 19.0290, Validation Loss: 18.9920\n",
      "Epoch 1696/6000, Training Loss: 19.0149, Validation Loss: 18.9780\n",
      "Epoch 1697/6000, Training Loss: 19.0009, Validation Loss: 18.9640\n",
      "Epoch 1698/6000, Training Loss: 18.9868, Validation Loss: 18.9500\n",
      "Epoch 1699/6000, Training Loss: 18.9728, Validation Loss: 18.9360\n",
      "Epoch 1700/6000, Training Loss: 18.9588, Validation Loss: 18.9220\n",
      "Epoch 1701/6000, Training Loss: 18.9448, Validation Loss: 18.9081\n",
      "Epoch 1702/6000, Training Loss: 18.9308, Validation Loss: 18.8941\n",
      "Epoch 1703/6000, Training Loss: 18.9168, Validation Loss: 18.8802\n",
      "Epoch 1704/6000, Training Loss: 18.9029, Validation Loss: 18.8663\n",
      "Epoch 1705/6000, Training Loss: 18.8889, Validation Loss: 18.8524\n",
      "Epoch 1706/6000, Training Loss: 18.8750, Validation Loss: 18.8386\n",
      "Epoch 1707/6000, Training Loss: 18.8611, Validation Loss: 18.8247\n",
      "Epoch 1708/6000, Training Loss: 18.8473, Validation Loss: 18.8109\n",
      "Epoch 1709/6000, Training Loss: 18.8334, Validation Loss: 18.7971\n",
      "Epoch 1710/6000, Training Loss: 18.8196, Validation Loss: 18.7833\n",
      "Epoch 1711/6000, Training Loss: 18.8058, Validation Loss: 18.7695\n",
      "Epoch 1712/6000, Training Loss: 18.7920, Validation Loss: 18.7558\n",
      "Epoch 1713/6000, Training Loss: 18.7782, Validation Loss: 18.7421\n",
      "Epoch 1714/6000, Training Loss: 18.7645, Validation Loss: 18.7285\n",
      "Epoch 1715/6000, Training Loss: 18.7508, Validation Loss: 18.7149\n",
      "Epoch 1716/6000, Training Loss: 18.7371, Validation Loss: 18.7013\n",
      "Epoch 1717/6000, Training Loss: 18.7235, Validation Loss: 18.6878\n",
      "Epoch 1718/6000, Training Loss: 18.7099, Validation Loss: 18.6743\n",
      "Epoch 1719/6000, Training Loss: 18.6964, Validation Loss: 18.6608\n",
      "Epoch 1720/6000, Training Loss: 18.6829, Validation Loss: 18.6474\n",
      "Epoch 1721/6000, Training Loss: 18.6694, Validation Loss: 18.6339\n",
      "Epoch 1722/6000, Training Loss: 18.6559, Validation Loss: 18.6205\n",
      "Epoch 1723/6000, Training Loss: 18.6424, Validation Loss: 18.6071\n",
      "Epoch 1724/6000, Training Loss: 18.6290, Validation Loss: 18.5937\n",
      "Epoch 1725/6000, Training Loss: 18.6156, Validation Loss: 18.5804\n",
      "Epoch 1726/6000, Training Loss: 18.6022, Validation Loss: 18.5670\n",
      "Epoch 1727/6000, Training Loss: 18.5888, Validation Loss: 18.5537\n",
      "Epoch 1728/6000, Training Loss: 18.5755, Validation Loss: 18.5404\n",
      "Epoch 1729/6000, Training Loss: 18.5621, Validation Loss: 18.5270\n",
      "Epoch 1730/6000, Training Loss: 18.5488, Validation Loss: 18.5137\n",
      "Epoch 1731/6000, Training Loss: 18.5354, Validation Loss: 18.5005\n",
      "Epoch 1732/6000, Training Loss: 18.5221, Validation Loss: 18.4872\n",
      "Epoch 1733/6000, Training Loss: 18.5088, Validation Loss: 18.4739\n",
      "Epoch 1734/6000, Training Loss: 18.4955, Validation Loss: 18.4607\n",
      "Epoch 1735/6000, Training Loss: 18.4823, Validation Loss: 18.4475\n",
      "Epoch 1736/6000, Training Loss: 18.4691, Validation Loss: 18.4343\n",
      "Epoch 1737/6000, Training Loss: 18.4558, Validation Loss: 18.4212\n",
      "Epoch 1738/6000, Training Loss: 18.4426, Validation Loss: 18.4080\n",
      "Epoch 1739/6000, Training Loss: 18.4295, Validation Loss: 18.3949\n",
      "Epoch 1740/6000, Training Loss: 18.4163, Validation Loss: 18.3817\n",
      "Epoch 1741/6000, Training Loss: 18.4032, Validation Loss: 18.3686\n",
      "Epoch 1742/6000, Training Loss: 18.3900, Validation Loss: 18.3556\n",
      "Epoch 1743/6000, Training Loss: 18.3769, Validation Loss: 18.3425\n",
      "Epoch 1744/6000, Training Loss: 18.3639, Validation Loss: 18.3295\n",
      "Epoch 1745/6000, Training Loss: 18.3508, Validation Loss: 18.3166\n",
      "Epoch 1746/6000, Training Loss: 18.3378, Validation Loss: 18.3037\n",
      "Epoch 1747/6000, Training Loss: 18.3249, Validation Loss: 18.2908\n",
      "Epoch 1748/6000, Training Loss: 18.3120, Validation Loss: 18.2779\n",
      "Epoch 1749/6000, Training Loss: 18.2991, Validation Loss: 18.2651\n",
      "Epoch 1750/6000, Training Loss: 18.2862, Validation Loss: 18.2523\n",
      "Epoch 1751/6000, Training Loss: 18.2734, Validation Loss: 18.2396\n",
      "Epoch 1752/6000, Training Loss: 18.2606, Validation Loss: 18.2268\n",
      "Epoch 1753/6000, Training Loss: 18.2478, Validation Loss: 18.2141\n",
      "Epoch 1754/6000, Training Loss: 18.2350, Validation Loss: 18.2013\n",
      "Epoch 1755/6000, Training Loss: 18.2222, Validation Loss: 18.1886\n",
      "Epoch 1756/6000, Training Loss: 18.2095, Validation Loss: 18.1759\n",
      "Epoch 1757/6000, Training Loss: 18.1968, Validation Loss: 18.1633\n",
      "Epoch 1758/6000, Training Loss: 18.1841, Validation Loss: 18.1506\n",
      "Epoch 1759/6000, Training Loss: 18.1714, Validation Loss: 18.1379\n",
      "Epoch 1760/6000, Training Loss: 18.1587, Validation Loss: 18.1253\n",
      "Epoch 1761/6000, Training Loss: 18.1460, Validation Loss: 18.1127\n",
      "Epoch 1762/6000, Training Loss: 18.1333, Validation Loss: 18.1000\n",
      "Epoch 1763/6000, Training Loss: 18.1207, Validation Loss: 18.0875\n",
      "Epoch 1764/6000, Training Loss: 18.1080, Validation Loss: 18.0749\n",
      "Epoch 1765/6000, Training Loss: 18.0954, Validation Loss: 18.0623\n",
      "Epoch 1766/6000, Training Loss: 18.0828, Validation Loss: 18.0498\n",
      "Epoch 1767/6000, Training Loss: 18.0702, Validation Loss: 18.0372\n",
      "Epoch 1768/6000, Training Loss: 18.0577, Validation Loss: 18.0247\n",
      "Epoch 1769/6000, Training Loss: 18.0451, Validation Loss: 18.0123\n",
      "Epoch 1770/6000, Training Loss: 18.0326, Validation Loss: 17.9998\n",
      "Epoch 1771/6000, Training Loss: 18.0201, Validation Loss: 17.9873\n",
      "Epoch 1772/6000, Training Loss: 18.0076, Validation Loss: 17.9749\n",
      "Epoch 1773/6000, Training Loss: 17.9951, Validation Loss: 17.9625\n",
      "Epoch 1774/6000, Training Loss: 17.9827, Validation Loss: 17.9502\n",
      "Epoch 1775/6000, Training Loss: 17.9703, Validation Loss: 17.9378\n",
      "Epoch 1776/6000, Training Loss: 17.9579, Validation Loss: 17.9255\n",
      "Epoch 1777/6000, Training Loss: 17.9456, Validation Loss: 17.9133\n",
      "Epoch 1778/6000, Training Loss: 17.9333, Validation Loss: 17.9010\n",
      "Epoch 1779/6000, Training Loss: 17.9210, Validation Loss: 17.8888\n",
      "Epoch 1780/6000, Training Loss: 17.9087, Validation Loss: 17.8766\n",
      "Epoch 1781/6000, Training Loss: 17.8965, Validation Loss: 17.8645\n",
      "Epoch 1782/6000, Training Loss: 17.8843, Validation Loss: 17.8524\n",
      "Epoch 1783/6000, Training Loss: 17.8721, Validation Loss: 17.8402\n",
      "Epoch 1784/6000, Training Loss: 17.8600, Validation Loss: 17.8281\n",
      "Epoch 1785/6000, Training Loss: 17.8479, Validation Loss: 17.8161\n",
      "Epoch 1786/6000, Training Loss: 17.8358, Validation Loss: 17.8040\n",
      "Epoch 1787/6000, Training Loss: 17.8237, Validation Loss: 17.7920\n",
      "Epoch 1788/6000, Training Loss: 17.8116, Validation Loss: 17.7799\n",
      "Epoch 1789/6000, Training Loss: 17.7995, Validation Loss: 17.7679\n",
      "Epoch 1790/6000, Training Loss: 17.7874, Validation Loss: 17.7559\n",
      "Epoch 1791/6000, Training Loss: 17.7754, Validation Loss: 17.7439\n",
      "Epoch 1792/6000, Training Loss: 17.7633, Validation Loss: 17.7319\n",
      "Epoch 1793/6000, Training Loss: 17.7513, Validation Loss: 17.7199\n",
      "Epoch 1794/6000, Training Loss: 17.7393, Validation Loss: 17.7079\n",
      "Epoch 1795/6000, Training Loss: 17.7273, Validation Loss: 17.6960\n",
      "Epoch 1796/6000, Training Loss: 17.7153, Validation Loss: 17.6840\n",
      "Epoch 1797/6000, Training Loss: 17.7033, Validation Loss: 17.6721\n",
      "Epoch 1798/6000, Training Loss: 17.6914, Validation Loss: 17.6602\n",
      "Epoch 1799/6000, Training Loss: 17.6795, Validation Loss: 17.6483\n",
      "Epoch 1800/6000, Training Loss: 17.6676, Validation Loss: 17.6364\n",
      "Epoch 1801/6000, Training Loss: 17.6557, Validation Loss: 17.6246\n",
      "Epoch 1802/6000, Training Loss: 17.6438, Validation Loss: 17.6128\n",
      "Epoch 1803/6000, Training Loss: 17.6319, Validation Loss: 17.6009\n",
      "Epoch 1804/6000, Training Loss: 17.6201, Validation Loss: 17.5892\n",
      "Epoch 1805/6000, Training Loss: 17.6083, Validation Loss: 17.5774\n",
      "Epoch 1806/6000, Training Loss: 17.5965, Validation Loss: 17.5657\n",
      "Epoch 1807/6000, Training Loss: 17.5847, Validation Loss: 17.5540\n",
      "Epoch 1808/6000, Training Loss: 17.5730, Validation Loss: 17.5424\n",
      "Epoch 1809/6000, Training Loss: 17.5613, Validation Loss: 17.5308\n",
      "Epoch 1810/6000, Training Loss: 17.5496, Validation Loss: 17.5192\n",
      "Epoch 1811/6000, Training Loss: 17.5380, Validation Loss: 17.5076\n",
      "Epoch 1812/6000, Training Loss: 17.5264, Validation Loss: 17.4961\n",
      "Epoch 1813/6000, Training Loss: 17.5148, Validation Loss: 17.4846\n",
      "Epoch 1814/6000, Training Loss: 17.5032, Validation Loss: 17.4731\n",
      "Epoch 1815/6000, Training Loss: 17.4916, Validation Loss: 17.4616\n",
      "Epoch 1816/6000, Training Loss: 17.4801, Validation Loss: 17.4502\n",
      "Epoch 1817/6000, Training Loss: 17.4686, Validation Loss: 17.4387\n",
      "Epoch 1818/6000, Training Loss: 17.4571, Validation Loss: 17.4273\n",
      "Epoch 1819/6000, Training Loss: 17.4456, Validation Loss: 17.4159\n",
      "Epoch 1820/6000, Training Loss: 17.4341, Validation Loss: 17.4045\n",
      "Epoch 1821/6000, Training Loss: 17.4227, Validation Loss: 17.3931\n",
      "Epoch 1822/6000, Training Loss: 17.4112, Validation Loss: 17.3817\n",
      "Epoch 1823/6000, Training Loss: 17.3997, Validation Loss: 17.3703\n",
      "Epoch 1824/6000, Training Loss: 17.3883, Validation Loss: 17.3589\n",
      "Epoch 1825/6000, Training Loss: 17.3769, Validation Loss: 17.3476\n",
      "Epoch 1826/6000, Training Loss: 17.3655, Validation Loss: 17.3362\n",
      "Epoch 1827/6000, Training Loss: 17.3541, Validation Loss: 17.3249\n",
      "Epoch 1828/6000, Training Loss: 17.3427, Validation Loss: 17.3136\n",
      "Epoch 1829/6000, Training Loss: 17.3313, Validation Loss: 17.3022\n",
      "Epoch 1830/6000, Training Loss: 17.3200, Validation Loss: 17.2909\n",
      "Epoch 1831/6000, Training Loss: 17.3086, Validation Loss: 17.2797\n",
      "Epoch 1832/6000, Training Loss: 17.2973, Validation Loss: 17.2684\n",
      "Epoch 1833/6000, Training Loss: 17.2860, Validation Loss: 17.2571\n",
      "Epoch 1834/6000, Training Loss: 17.2747, Validation Loss: 17.2459\n",
      "Epoch 1835/6000, Training Loss: 17.2634, Validation Loss: 17.2347\n",
      "Epoch 1836/6000, Training Loss: 17.2522, Validation Loss: 17.2235\n",
      "Epoch 1837/6000, Training Loss: 17.2410, Validation Loss: 17.2124\n",
      "Epoch 1838/6000, Training Loss: 17.2298, Validation Loss: 17.2013\n",
      "Epoch 1839/6000, Training Loss: 17.2186, Validation Loss: 17.1902\n",
      "Epoch 1840/6000, Training Loss: 17.2074, Validation Loss: 17.1792\n",
      "Epoch 1841/6000, Training Loss: 17.1963, Validation Loss: 17.1682\n",
      "Epoch 1842/6000, Training Loss: 17.1852, Validation Loss: 17.1572\n",
      "Epoch 1843/6000, Training Loss: 17.1742, Validation Loss: 17.1463\n",
      "Epoch 1844/6000, Training Loss: 17.1631, Validation Loss: 17.1353\n",
      "Epoch 1845/6000, Training Loss: 17.1521, Validation Loss: 17.1244\n",
      "Epoch 1846/6000, Training Loss: 17.1412, Validation Loss: 17.1135\n",
      "Epoch 1847/6000, Training Loss: 17.1302, Validation Loss: 17.1027\n",
      "Epoch 1848/6000, Training Loss: 17.1192, Validation Loss: 17.0918\n",
      "Epoch 1849/6000, Training Loss: 17.1083, Validation Loss: 17.0809\n",
      "Epoch 1850/6000, Training Loss: 17.0974, Validation Loss: 17.0701\n",
      "Epoch 1851/6000, Training Loss: 17.0865, Validation Loss: 17.0593\n",
      "Epoch 1852/6000, Training Loss: 17.0756, Validation Loss: 17.0485\n",
      "Epoch 1853/6000, Training Loss: 17.0647, Validation Loss: 17.0377\n",
      "Epoch 1854/6000, Training Loss: 17.0539, Validation Loss: 17.0269\n",
      "Epoch 1855/6000, Training Loss: 17.0430, Validation Loss: 17.0161\n",
      "Epoch 1856/6000, Training Loss: 17.0322, Validation Loss: 17.0053\n",
      "Epoch 1857/6000, Training Loss: 17.0213, Validation Loss: 16.9946\n",
      "Epoch 1858/6000, Training Loss: 17.0105, Validation Loss: 16.9838\n",
      "Epoch 1859/6000, Training Loss: 16.9997, Validation Loss: 16.9731\n",
      "Epoch 1860/6000, Training Loss: 16.9889, Validation Loss: 16.9623\n",
      "Epoch 1861/6000, Training Loss: 16.9781, Validation Loss: 16.9516\n",
      "Epoch 1862/6000, Training Loss: 16.9674, Validation Loss: 16.9409\n",
      "Epoch 1863/6000, Training Loss: 16.9566, Validation Loss: 16.9302\n",
      "Epoch 1864/6000, Training Loss: 16.9459, Validation Loss: 16.9195\n",
      "Epoch 1865/6000, Training Loss: 16.9351, Validation Loss: 16.9088\n",
      "Epoch 1866/6000, Training Loss: 16.9244, Validation Loss: 16.8982\n",
      "Epoch 1867/6000, Training Loss: 16.9137, Validation Loss: 16.8875\n",
      "Epoch 1868/6000, Training Loss: 16.9030, Validation Loss: 16.8769\n",
      "Epoch 1869/6000, Training Loss: 16.8924, Validation Loss: 16.8664\n",
      "Epoch 1870/6000, Training Loss: 16.8818, Validation Loss: 16.8558\n",
      "Epoch 1871/6000, Training Loss: 16.8712, Validation Loss: 16.8453\n",
      "Epoch 1872/6000, Training Loss: 16.8606, Validation Loss: 16.8348\n",
      "Epoch 1873/6000, Training Loss: 16.8501, Validation Loss: 16.8244\n",
      "Epoch 1874/6000, Training Loss: 16.8395, Validation Loss: 16.8140\n",
      "Epoch 1875/6000, Training Loss: 16.8290, Validation Loss: 16.8036\n",
      "Epoch 1876/6000, Training Loss: 16.8186, Validation Loss: 16.7932\n",
      "Epoch 1877/6000, Training Loss: 16.8081, Validation Loss: 16.7829\n",
      "Epoch 1878/6000, Training Loss: 16.7977, Validation Loss: 16.7726\n",
      "Epoch 1879/6000, Training Loss: 16.7873, Validation Loss: 16.7622\n",
      "Epoch 1880/6000, Training Loss: 16.7769, Validation Loss: 16.7519\n",
      "Epoch 1881/6000, Training Loss: 16.7666, Validation Loss: 16.7417\n",
      "Epoch 1882/6000, Training Loss: 16.7562, Validation Loss: 16.7314\n",
      "Epoch 1883/6000, Training Loss: 16.7459, Validation Loss: 16.7211\n",
      "Epoch 1884/6000, Training Loss: 16.7356, Validation Loss: 16.7109\n",
      "Epoch 1885/6000, Training Loss: 16.7253, Validation Loss: 16.7006\n",
      "Epoch 1886/6000, Training Loss: 16.7150, Validation Loss: 16.6904\n",
      "Epoch 1887/6000, Training Loss: 16.7047, Validation Loss: 16.6802\n",
      "Epoch 1888/6000, Training Loss: 16.6944, Validation Loss: 16.6700\n",
      "Epoch 1889/6000, Training Loss: 16.6842, Validation Loss: 16.6598\n",
      "Epoch 1890/6000, Training Loss: 16.6739, Validation Loss: 16.6496\n",
      "Epoch 1891/6000, Training Loss: 16.6637, Validation Loss: 16.6394\n",
      "Epoch 1892/6000, Training Loss: 16.6534, Validation Loss: 16.6293\n",
      "Epoch 1893/6000, Training Loss: 16.6432, Validation Loss: 16.6191\n",
      "Epoch 1894/6000, Training Loss: 16.6330, Validation Loss: 16.6090\n",
      "Epoch 1895/6000, Training Loss: 16.6228, Validation Loss: 16.5988\n",
      "Epoch 1896/6000, Training Loss: 16.6126, Validation Loss: 16.5887\n",
      "Epoch 1897/6000, Training Loss: 16.6025, Validation Loss: 16.5786\n",
      "Epoch 1898/6000, Training Loss: 16.5923, Validation Loss: 16.5685\n",
      "Epoch 1899/6000, Training Loss: 16.5822, Validation Loss: 16.5584\n",
      "Epoch 1900/6000, Training Loss: 16.5721, Validation Loss: 16.5483\n",
      "Epoch 1901/6000, Training Loss: 16.5620, Validation Loss: 16.5383\n",
      "Epoch 1902/6000, Training Loss: 16.5519, Validation Loss: 16.5283\n",
      "Epoch 1903/6000, Training Loss: 16.5418, Validation Loss: 16.5183\n",
      "Epoch 1904/6000, Training Loss: 16.5317, Validation Loss: 16.5083\n",
      "Epoch 1905/6000, Training Loss: 16.5217, Validation Loss: 16.4984\n",
      "Epoch 1906/6000, Training Loss: 16.5117, Validation Loss: 16.4885\n",
      "Epoch 1907/6000, Training Loss: 16.5017, Validation Loss: 16.4786\n",
      "Epoch 1908/6000, Training Loss: 16.4918, Validation Loss: 16.4687\n",
      "Epoch 1909/6000, Training Loss: 16.4819, Validation Loss: 16.4589\n",
      "Epoch 1910/6000, Training Loss: 16.4720, Validation Loss: 16.4491\n",
      "Epoch 1911/6000, Training Loss: 16.4621, Validation Loss: 16.4393\n",
      "Epoch 1912/6000, Training Loss: 16.4522, Validation Loss: 16.4296\n",
      "Epoch 1913/6000, Training Loss: 16.4424, Validation Loss: 16.4198\n",
      "Epoch 1914/6000, Training Loss: 16.4326, Validation Loss: 16.4101\n",
      "Epoch 1915/6000, Training Loss: 16.4228, Validation Loss: 16.4003\n",
      "Epoch 1916/6000, Training Loss: 16.4130, Validation Loss: 16.3906\n",
      "Epoch 1917/6000, Training Loss: 16.4032, Validation Loss: 16.3809\n",
      "Epoch 1918/6000, Training Loss: 16.3935, Validation Loss: 16.3712\n",
      "Epoch 1919/6000, Training Loss: 16.3837, Validation Loss: 16.3616\n",
      "Epoch 1920/6000, Training Loss: 16.3740, Validation Loss: 16.3519\n",
      "Epoch 1921/6000, Training Loss: 16.3642, Validation Loss: 16.3422\n",
      "Epoch 1922/6000, Training Loss: 16.3545, Validation Loss: 16.3326\n",
      "Epoch 1923/6000, Training Loss: 16.3448, Validation Loss: 16.3229\n",
      "Epoch 1924/6000, Training Loss: 16.3351, Validation Loss: 16.3133\n",
      "Epoch 1925/6000, Training Loss: 16.3254, Validation Loss: 16.3037\n",
      "Epoch 1926/6000, Training Loss: 16.3157, Validation Loss: 16.2941\n",
      "Epoch 1927/6000, Training Loss: 16.3061, Validation Loss: 16.2845\n",
      "Epoch 1928/6000, Training Loss: 16.2964, Validation Loss: 16.2749\n",
      "Epoch 1929/6000, Training Loss: 16.2868, Validation Loss: 16.2653\n",
      "Epoch 1930/6000, Training Loss: 16.2772, Validation Loss: 16.2557\n",
      "Epoch 1931/6000, Training Loss: 16.2676, Validation Loss: 16.2462\n",
      "Epoch 1932/6000, Training Loss: 16.2580, Validation Loss: 16.2366\n",
      "Epoch 1933/6000, Training Loss: 16.2484, Validation Loss: 16.2271\n",
      "Epoch 1934/6000, Training Loss: 16.2388, Validation Loss: 16.2176\n",
      "Epoch 1935/6000, Training Loss: 16.2293, Validation Loss: 16.2081\n",
      "Epoch 1936/6000, Training Loss: 16.2197, Validation Loss: 16.1986\n",
      "Epoch 1937/6000, Training Loss: 16.2102, Validation Loss: 16.1892\n",
      "Epoch 1938/6000, Training Loss: 16.2007, Validation Loss: 16.1798\n",
      "Epoch 1939/6000, Training Loss: 16.1912, Validation Loss: 16.1704\n",
      "Epoch 1940/6000, Training Loss: 16.1818, Validation Loss: 16.1610\n",
      "Epoch 1941/6000, Training Loss: 16.1723, Validation Loss: 16.1516\n",
      "Epoch 1942/6000, Training Loss: 16.1629, Validation Loss: 16.1423\n",
      "Epoch 1943/6000, Training Loss: 16.1536, Validation Loss: 16.1330\n",
      "Epoch 1944/6000, Training Loss: 16.1442, Validation Loss: 16.1238\n",
      "Epoch 1945/6000, Training Loss: 16.1349, Validation Loss: 16.1145\n",
      "Epoch 1946/6000, Training Loss: 16.1256, Validation Loss: 16.1053\n",
      "Epoch 1947/6000, Training Loss: 16.1163, Validation Loss: 16.0961\n",
      "Epoch 1948/6000, Training Loss: 16.1070, Validation Loss: 16.0869\n",
      "Epoch 1949/6000, Training Loss: 16.0977, Validation Loss: 16.0777\n",
      "Epoch 1950/6000, Training Loss: 16.0885, Validation Loss: 16.0685\n",
      "Epoch 1951/6000, Training Loss: 16.0793, Validation Loss: 16.0593\n",
      "Epoch 1952/6000, Training Loss: 16.0701, Validation Loss: 16.0502\n",
      "Epoch 1953/6000, Training Loss: 16.0608, Validation Loss: 16.0410\n",
      "Epoch 1954/6000, Training Loss: 16.0516, Validation Loss: 16.0319\n",
      "Epoch 1955/6000, Training Loss: 16.0425, Validation Loss: 16.0227\n",
      "Epoch 1956/6000, Training Loss: 16.0333, Validation Loss: 16.0136\n",
      "Epoch 1957/6000, Training Loss: 16.0241, Validation Loss: 16.0045\n",
      "Epoch 1958/6000, Training Loss: 16.0149, Validation Loss: 15.9954\n",
      "Epoch 1959/6000, Training Loss: 16.0058, Validation Loss: 15.9863\n",
      "Epoch 1960/6000, Training Loss: 15.9966, Validation Loss: 15.9772\n",
      "Epoch 1961/6000, Training Loss: 15.9875, Validation Loss: 15.9681\n",
      "Epoch 1962/6000, Training Loss: 15.9784, Validation Loss: 15.9590\n",
      "Epoch 1963/6000, Training Loss: 15.9693, Validation Loss: 15.9500\n",
      "Epoch 1964/6000, Training Loss: 15.9602, Validation Loss: 15.9409\n",
      "Epoch 1965/6000, Training Loss: 15.9511, Validation Loss: 15.9319\n",
      "Epoch 1966/6000, Training Loss: 15.9420, Validation Loss: 15.9229\n",
      "Epoch 1967/6000, Training Loss: 15.9330, Validation Loss: 15.9139\n",
      "Epoch 1968/6000, Training Loss: 15.9239, Validation Loss: 15.9049\n",
      "Epoch 1969/6000, Training Loss: 15.9149, Validation Loss: 15.8959\n",
      "Epoch 1970/6000, Training Loss: 15.9059, Validation Loss: 15.8869\n",
      "Epoch 1971/6000, Training Loss: 15.8969, Validation Loss: 15.8780\n",
      "Epoch 1972/6000, Training Loss: 15.8879, Validation Loss: 15.8690\n",
      "Epoch 1973/6000, Training Loss: 15.8789, Validation Loss: 15.8601\n",
      "Epoch 1974/6000, Training Loss: 15.8699, Validation Loss: 15.8513\n",
      "Epoch 1975/6000, Training Loss: 15.8610, Validation Loss: 15.8424\n",
      "Epoch 1976/6000, Training Loss: 15.8521, Validation Loss: 15.8336\n",
      "Epoch 1977/6000, Training Loss: 15.8432, Validation Loss: 15.8248\n",
      "Epoch 1978/6000, Training Loss: 15.8344, Validation Loss: 15.8160\n",
      "Epoch 1979/6000, Training Loss: 15.8255, Validation Loss: 15.8073\n",
      "Epoch 1980/6000, Training Loss: 15.8167, Validation Loss: 15.7985\n",
      "Epoch 1981/6000, Training Loss: 15.8080, Validation Loss: 15.7898\n",
      "Epoch 1982/6000, Training Loss: 15.7992, Validation Loss: 15.7811\n",
      "Epoch 1983/6000, Training Loss: 15.7904, Validation Loss: 15.7724\n",
      "Epoch 1984/6000, Training Loss: 15.7817, Validation Loss: 15.7637\n",
      "Epoch 1985/6000, Training Loss: 15.7730, Validation Loss: 15.7551\n",
      "Epoch 1986/6000, Training Loss: 15.7643, Validation Loss: 15.7464\n",
      "Epoch 1987/6000, Training Loss: 15.7556, Validation Loss: 15.7378\n",
      "Epoch 1988/6000, Training Loss: 15.7469, Validation Loss: 15.7292\n",
      "Epoch 1989/6000, Training Loss: 15.7382, Validation Loss: 15.7205\n",
      "Epoch 1990/6000, Training Loss: 15.7295, Validation Loss: 15.7119\n",
      "Epoch 1991/6000, Training Loss: 15.7209, Validation Loss: 15.7033\n",
      "Epoch 1992/6000, Training Loss: 15.7122, Validation Loss: 15.6947\n",
      "Epoch 1993/6000, Training Loss: 15.7036, Validation Loss: 15.6862\n",
      "Epoch 1994/6000, Training Loss: 15.6950, Validation Loss: 15.6776\n",
      "Epoch 1995/6000, Training Loss: 15.6863, Validation Loss: 15.6690\n",
      "Epoch 1996/6000, Training Loss: 15.6777, Validation Loss: 15.6605\n",
      "Epoch 1997/6000, Training Loss: 15.6691, Validation Loss: 15.6519\n",
      "Epoch 1998/6000, Training Loss: 15.6605, Validation Loss: 15.6434\n",
      "Epoch 1999/6000, Training Loss: 15.6519, Validation Loss: 15.6348\n",
      "Epoch 2000/6000, Training Loss: 15.6434, Validation Loss: 15.6263\n",
      "Epoch 2001/6000, Training Loss: 15.6348, Validation Loss: 15.6178\n",
      "Epoch 2002/6000, Training Loss: 15.6262, Validation Loss: 15.6093\n",
      "Epoch 2003/6000, Training Loss: 15.6177, Validation Loss: 15.6008\n",
      "Epoch 2004/6000, Training Loss: 15.6092, Validation Loss: 15.5922\n",
      "Epoch 2005/6000, Training Loss: 15.6006, Validation Loss: 15.5838\n",
      "Epoch 2006/6000, Training Loss: 15.5921, Validation Loss: 15.5753\n",
      "Epoch 2007/6000, Training Loss: 15.5836, Validation Loss: 15.5668\n",
      "Epoch 2008/6000, Training Loss: 15.5752, Validation Loss: 15.5584\n",
      "Epoch 2009/6000, Training Loss: 15.5667, Validation Loss: 15.5500\n",
      "Epoch 2010/6000, Training Loss: 15.5583, Validation Loss: 15.5416\n",
      "Epoch 2011/6000, Training Loss: 15.5499, Validation Loss: 15.5333\n",
      "Epoch 2012/6000, Training Loss: 15.5415, Validation Loss: 15.5249\n",
      "Epoch 2013/6000, Training Loss: 15.5331, Validation Loss: 15.5166\n",
      "Epoch 2014/6000, Training Loss: 15.5248, Validation Loss: 15.5084\n",
      "Epoch 2015/6000, Training Loss: 15.5165, Validation Loss: 15.5001\n",
      "Epoch 2016/6000, Training Loss: 15.5082, Validation Loss: 15.4919\n",
      "Epoch 2017/6000, Training Loss: 15.4999, Validation Loss: 15.4837\n",
      "Epoch 2018/6000, Training Loss: 15.4916, Validation Loss: 15.4755\n",
      "Epoch 2019/6000, Training Loss: 15.4834, Validation Loss: 15.4673\n",
      "Epoch 2020/6000, Training Loss: 15.4751, Validation Loss: 15.4591\n",
      "Epoch 2021/6000, Training Loss: 15.4669, Validation Loss: 15.4510\n",
      "Epoch 2022/6000, Training Loss: 15.4587, Validation Loss: 15.4428\n",
      "Epoch 2023/6000, Training Loss: 15.4505, Validation Loss: 15.4347\n",
      "Epoch 2024/6000, Training Loss: 15.4423, Validation Loss: 15.4266\n",
      "Epoch 2025/6000, Training Loss: 15.4342, Validation Loss: 15.4184\n",
      "Epoch 2026/6000, Training Loss: 15.4260, Validation Loss: 15.4103\n",
      "Epoch 2027/6000, Training Loss: 15.4178, Validation Loss: 15.4022\n",
      "Epoch 2028/6000, Training Loss: 15.4097, Validation Loss: 15.3941\n",
      "Epoch 2029/6000, Training Loss: 15.4015, Validation Loss: 15.3860\n",
      "Epoch 2030/6000, Training Loss: 15.3934, Validation Loss: 15.3779\n",
      "Epoch 2031/6000, Training Loss: 15.3853, Validation Loss: 15.3699\n",
      "Epoch 2032/6000, Training Loss: 15.3772, Validation Loss: 15.3618\n",
      "Epoch 2033/6000, Training Loss: 15.3691, Validation Loss: 15.3538\n",
      "Epoch 2034/6000, Training Loss: 15.3610, Validation Loss: 15.3457\n",
      "Epoch 2035/6000, Training Loss: 15.3529, Validation Loss: 15.3377\n",
      "Epoch 2036/6000, Training Loss: 15.3448, Validation Loss: 15.3297\n",
      "Epoch 2037/6000, Training Loss: 15.3368, Validation Loss: 15.3217\n",
      "Epoch 2038/6000, Training Loss: 15.3287, Validation Loss: 15.3137\n",
      "Epoch 2039/6000, Training Loss: 15.3207, Validation Loss: 15.3057\n",
      "Epoch 2040/6000, Training Loss: 15.3127, Validation Loss: 15.2977\n",
      "Epoch 2041/6000, Training Loss: 15.3046, Validation Loss: 15.2897\n",
      "Epoch 2042/6000, Training Loss: 15.2966, Validation Loss: 15.2817\n",
      "Epoch 2043/6000, Training Loss: 15.2886, Validation Loss: 15.2738\n",
      "Epoch 2044/6000, Training Loss: 15.2806, Validation Loss: 15.2659\n",
      "Epoch 2045/6000, Training Loss: 15.2727, Validation Loss: 15.2579\n",
      "Epoch 2046/6000, Training Loss: 15.2647, Validation Loss: 15.2500\n",
      "Epoch 2047/6000, Training Loss: 15.2568, Validation Loss: 15.2422\n",
      "Epoch 2048/6000, Training Loss: 15.2489, Validation Loss: 15.2343\n",
      "Epoch 2049/6000, Training Loss: 15.2410, Validation Loss: 15.2265\n",
      "Epoch 2050/6000, Training Loss: 15.2331, Validation Loss: 15.2187\n",
      "Epoch 2051/6000, Training Loss: 15.2253, Validation Loss: 15.2109\n",
      "Epoch 2052/6000, Training Loss: 15.2175, Validation Loss: 15.2031\n",
      "Epoch 2053/6000, Training Loss: 15.2097, Validation Loss: 15.1954\n",
      "Epoch 2054/6000, Training Loss: 15.2019, Validation Loss: 15.1877\n",
      "Epoch 2055/6000, Training Loss: 15.1941, Validation Loss: 15.1800\n",
      "Epoch 2056/6000, Training Loss: 15.1864, Validation Loss: 15.1723\n",
      "Epoch 2057/6000, Training Loss: 15.1786, Validation Loss: 15.1646\n",
      "Epoch 2058/6000, Training Loss: 15.1709, Validation Loss: 15.1570\n",
      "Epoch 2059/6000, Training Loss: 15.1632, Validation Loss: 15.1493\n",
      "Epoch 2060/6000, Training Loss: 15.1555, Validation Loss: 15.1417\n",
      "Epoch 2061/6000, Training Loss: 15.1478, Validation Loss: 15.1341\n",
      "Epoch 2062/6000, Training Loss: 15.1401, Validation Loss: 15.1264\n",
      "Epoch 2063/6000, Training Loss: 15.1324, Validation Loss: 15.1188\n",
      "Epoch 2064/6000, Training Loss: 15.1248, Validation Loss: 15.1112\n",
      "Epoch 2065/6000, Training Loss: 15.1171, Validation Loss: 15.1036\n",
      "Epoch 2066/6000, Training Loss: 15.1094, Validation Loss: 15.0960\n",
      "Epoch 2067/6000, Training Loss: 15.1018, Validation Loss: 15.0884\n",
      "Epoch 2068/6000, Training Loss: 15.0941, Validation Loss: 15.0809\n",
      "Epoch 2069/6000, Training Loss: 15.0865, Validation Loss: 15.0733\n",
      "Epoch 2070/6000, Training Loss: 15.0789, Validation Loss: 15.0658\n",
      "Epoch 2071/6000, Training Loss: 15.0713, Validation Loss: 15.0582\n",
      "Epoch 2072/6000, Training Loss: 15.0637, Validation Loss: 15.0507\n",
      "Epoch 2073/6000, Training Loss: 15.0561, Validation Loss: 15.0432\n",
      "Epoch 2074/6000, Training Loss: 15.0485, Validation Loss: 15.0357\n",
      "Epoch 2075/6000, Training Loss: 15.0410, Validation Loss: 15.0282\n",
      "Epoch 2076/6000, Training Loss: 15.0334, Validation Loss: 15.0207\n",
      "Epoch 2077/6000, Training Loss: 15.0259, Validation Loss: 15.0132\n",
      "Epoch 2078/6000, Training Loss: 15.0183, Validation Loss: 15.0057\n",
      "Epoch 2079/6000, Training Loss: 15.0108, Validation Loss: 14.9982\n",
      "Epoch 2080/6000, Training Loss: 15.0033, Validation Loss: 14.9907\n",
      "Epoch 2081/6000, Training Loss: 14.9958, Validation Loss: 14.9833\n",
      "Epoch 2082/6000, Training Loss: 14.9883, Validation Loss: 14.9758\n",
      "Epoch 2083/6000, Training Loss: 14.9808, Validation Loss: 14.9684\n",
      "Epoch 2084/6000, Training Loss: 14.9734, Validation Loss: 14.9609\n",
      "Epoch 2085/6000, Training Loss: 14.9659, Validation Loss: 14.9535\n",
      "Epoch 2086/6000, Training Loss: 14.9585, Validation Loss: 14.9461\n",
      "Epoch 2087/6000, Training Loss: 14.9511, Validation Loss: 14.9388\n",
      "Epoch 2088/6000, Training Loss: 14.9437, Validation Loss: 14.9315\n",
      "Epoch 2089/6000, Training Loss: 14.9364, Validation Loss: 14.9242\n",
      "Epoch 2090/6000, Training Loss: 14.9290, Validation Loss: 14.9169\n",
      "Epoch 2091/6000, Training Loss: 14.9217, Validation Loss: 14.9096\n",
      "Epoch 2092/6000, Training Loss: 14.9144, Validation Loss: 14.9024\n",
      "Epoch 2093/6000, Training Loss: 14.9072, Validation Loss: 14.8951\n",
      "Epoch 2094/6000, Training Loss: 14.8999, Validation Loss: 14.8879\n",
      "Epoch 2095/6000, Training Loss: 14.8927, Validation Loss: 14.8807\n",
      "Epoch 2096/6000, Training Loss: 14.8854, Validation Loss: 14.8735\n",
      "Epoch 2097/6000, Training Loss: 14.8782, Validation Loss: 14.8664\n",
      "Epoch 2098/6000, Training Loss: 14.8710, Validation Loss: 14.8592\n",
      "Epoch 2099/6000, Training Loss: 14.8638, Validation Loss: 14.8520\n",
      "Epoch 2100/6000, Training Loss: 14.8566, Validation Loss: 14.8449\n",
      "Epoch 2101/6000, Training Loss: 14.8494, Validation Loss: 14.8377\n",
      "Epoch 2102/6000, Training Loss: 14.8422, Validation Loss: 14.8306\n",
      "Epoch 2103/6000, Training Loss: 14.8350, Validation Loss: 14.8235\n",
      "Epoch 2104/6000, Training Loss: 14.8279, Validation Loss: 14.8163\n",
      "Epoch 2105/6000, Training Loss: 14.8207, Validation Loss: 14.8092\n",
      "Epoch 2106/6000, Training Loss: 14.8135, Validation Loss: 14.8021\n",
      "Epoch 2107/6000, Training Loss: 14.8064, Validation Loss: 14.7950\n",
      "Epoch 2108/6000, Training Loss: 14.7992, Validation Loss: 14.7879\n",
      "Epoch 2109/6000, Training Loss: 14.7921, Validation Loss: 14.7808\n",
      "Epoch 2110/6000, Training Loss: 14.7850, Validation Loss: 14.7737\n",
      "Epoch 2111/6000, Training Loss: 14.7779, Validation Loss: 14.7667\n",
      "Epoch 2112/6000, Training Loss: 14.7708, Validation Loss: 14.7596\n",
      "Epoch 2113/6000, Training Loss: 14.7637, Validation Loss: 14.7526\n",
      "Epoch 2114/6000, Training Loss: 14.7566, Validation Loss: 14.7455\n",
      "Epoch 2115/6000, Training Loss: 14.7496, Validation Loss: 14.7385\n",
      "Epoch 2116/6000, Training Loss: 14.7425, Validation Loss: 14.7315\n",
      "Epoch 2117/6000, Training Loss: 14.7355, Validation Loss: 14.7245\n",
      "Epoch 2118/6000, Training Loss: 14.7284, Validation Loss: 14.7175\n",
      "Epoch 2119/6000, Training Loss: 14.7214, Validation Loss: 14.7105\n",
      "Epoch 2120/6000, Training Loss: 14.7144, Validation Loss: 14.7035\n",
      "Epoch 2121/6000, Training Loss: 14.7074, Validation Loss: 14.6966\n",
      "Epoch 2122/6000, Training Loss: 14.7004, Validation Loss: 14.6896\n",
      "Epoch 2123/6000, Training Loss: 14.6934, Validation Loss: 14.6827\n",
      "Epoch 2124/6000, Training Loss: 14.6864, Validation Loss: 14.6758\n",
      "Epoch 2125/6000, Training Loss: 14.6795, Validation Loss: 14.6689\n",
      "Epoch 2126/6000, Training Loss: 14.6726, Validation Loss: 14.6620\n",
      "Epoch 2127/6000, Training Loss: 14.6657, Validation Loss: 14.6552\n",
      "Epoch 2128/6000, Training Loss: 14.6588, Validation Loss: 14.6484\n",
      "Epoch 2129/6000, Training Loss: 14.6519, Validation Loss: 14.6416\n",
      "Epoch 2130/6000, Training Loss: 14.6451, Validation Loss: 14.6348\n",
      "Epoch 2131/6000, Training Loss: 14.6383, Validation Loss: 14.6280\n",
      "Epoch 2132/6000, Training Loss: 14.6314, Validation Loss: 14.6212\n",
      "Epoch 2133/6000, Training Loss: 14.6247, Validation Loss: 14.6145\n",
      "Epoch 2134/6000, Training Loss: 14.6179, Validation Loss: 14.6078\n",
      "Epoch 2135/6000, Training Loss: 14.6111, Validation Loss: 14.6010\n",
      "Epoch 2136/6000, Training Loss: 14.6044, Validation Loss: 14.5943\n",
      "Epoch 2137/6000, Training Loss: 14.5976, Validation Loss: 14.5876\n",
      "Epoch 2138/6000, Training Loss: 14.5909, Validation Loss: 14.5809\n",
      "Epoch 2139/6000, Training Loss: 14.5841, Validation Loss: 14.5743\n",
      "Epoch 2140/6000, Training Loss: 14.5774, Validation Loss: 14.5676\n",
      "Epoch 2141/6000, Training Loss: 14.5707, Validation Loss: 14.5609\n",
      "Epoch 2142/6000, Training Loss: 14.5640, Validation Loss: 14.5542\n",
      "Epoch 2143/6000, Training Loss: 14.5573, Validation Loss: 14.5476\n",
      "Epoch 2144/6000, Training Loss: 14.5506, Validation Loss: 14.5409\n",
      "Epoch 2145/6000, Training Loss: 14.5439, Validation Loss: 14.5343\n",
      "Epoch 2146/6000, Training Loss: 14.5372, Validation Loss: 14.5276\n",
      "Epoch 2147/6000, Training Loss: 14.5305, Validation Loss: 14.5210\n",
      "Epoch 2148/6000, Training Loss: 14.5239, Validation Loss: 14.5144\n",
      "Epoch 2149/6000, Training Loss: 14.5172, Validation Loss: 14.5078\n",
      "Epoch 2150/6000, Training Loss: 14.5106, Validation Loss: 14.5012\n",
      "Epoch 2151/6000, Training Loss: 14.5039, Validation Loss: 14.4946\n",
      "Epoch 2152/6000, Training Loss: 14.4973, Validation Loss: 14.4880\n",
      "Epoch 2153/6000, Training Loss: 14.4907, Validation Loss: 14.4814\n",
      "Epoch 2154/6000, Training Loss: 14.4841, Validation Loss: 14.4749\n",
      "Epoch 2155/6000, Training Loss: 14.4775, Validation Loss: 14.4683\n",
      "Epoch 2156/6000, Training Loss: 14.4709, Validation Loss: 14.4618\n",
      "Epoch 2157/6000, Training Loss: 14.4643, Validation Loss: 14.4552\n",
      "Epoch 2158/6000, Training Loss: 14.4578, Validation Loss: 14.4487\n",
      "Epoch 2159/6000, Training Loss: 14.4512, Validation Loss: 14.4422\n",
      "Epoch 2160/6000, Training Loss: 14.4446, Validation Loss: 14.4357\n",
      "Epoch 2161/6000, Training Loss: 14.4381, Validation Loss: 14.4292\n",
      "Epoch 2162/6000, Training Loss: 14.4316, Validation Loss: 14.4227\n",
      "Epoch 2163/6000, Training Loss: 14.4250, Validation Loss: 14.4162\n",
      "Epoch 2164/6000, Training Loss: 14.4185, Validation Loss: 14.4098\n",
      "Epoch 2165/6000, Training Loss: 14.4120, Validation Loss: 14.4033\n",
      "Epoch 2166/6000, Training Loss: 14.4056, Validation Loss: 14.3969\n",
      "Epoch 2167/6000, Training Loss: 14.3991, Validation Loss: 14.3905\n",
      "Epoch 2168/6000, Training Loss: 14.3927, Validation Loss: 14.3841\n",
      "Epoch 2169/6000, Training Loss: 14.3863, Validation Loss: 14.3778\n",
      "Epoch 2170/6000, Training Loss: 14.3799, Validation Loss: 14.3715\n",
      "Epoch 2171/6000, Training Loss: 14.3736, Validation Loss: 14.3651\n",
      "Epoch 2172/6000, Training Loss: 14.3672, Validation Loss: 14.3588\n",
      "Epoch 2173/6000, Training Loss: 14.3609, Validation Loss: 14.3525\n",
      "Epoch 2174/6000, Training Loss: 14.3546, Validation Loss: 14.3463\n",
      "Epoch 2175/6000, Training Loss: 14.3483, Validation Loss: 14.3400\n",
      "Epoch 2176/6000, Training Loss: 14.3420, Validation Loss: 14.3337\n",
      "Epoch 2177/6000, Training Loss: 14.3357, Validation Loss: 14.3275\n",
      "Epoch 2178/6000, Training Loss: 14.3294, Validation Loss: 14.3213\n",
      "Epoch 2179/6000, Training Loss: 14.3231, Validation Loss: 14.3150\n",
      "Epoch 2180/6000, Training Loss: 14.3169, Validation Loss: 14.3088\n",
      "Epoch 2181/6000, Training Loss: 14.3106, Validation Loss: 14.3026\n",
      "Epoch 2182/6000, Training Loss: 14.3043, Validation Loss: 14.2964\n",
      "Epoch 2183/6000, Training Loss: 14.2981, Validation Loss: 14.2902\n",
      "Epoch 2184/6000, Training Loss: 14.2918, Validation Loss: 14.2840\n",
      "Epoch 2185/6000, Training Loss: 14.2856, Validation Loss: 14.2778\n",
      "Epoch 2186/6000, Training Loss: 14.2794, Validation Loss: 14.2716\n",
      "Epoch 2187/6000, Training Loss: 14.2731, Validation Loss: 14.2654\n",
      "Epoch 2188/6000, Training Loss: 14.2669, Validation Loss: 14.2592\n",
      "Epoch 2189/6000, Training Loss: 14.2607, Validation Loss: 14.2531\n",
      "Epoch 2190/6000, Training Loss: 14.2545, Validation Loss: 14.2469\n",
      "Epoch 2191/6000, Training Loss: 14.2483, Validation Loss: 14.2408\n",
      "Epoch 2192/6000, Training Loss: 14.2421, Validation Loss: 14.2346\n",
      "Epoch 2193/6000, Training Loss: 14.2359, Validation Loss: 14.2285\n",
      "Epoch 2194/6000, Training Loss: 14.2298, Validation Loss: 14.2224\n",
      "Epoch 2195/6000, Training Loss: 14.2236, Validation Loss: 14.2163\n",
      "Epoch 2196/6000, Training Loss: 14.2175, Validation Loss: 14.2102\n",
      "Epoch 2197/6000, Training Loss: 14.2113, Validation Loss: 14.2041\n",
      "Epoch 2198/6000, Training Loss: 14.2052, Validation Loss: 14.1980\n",
      "Epoch 2199/6000, Training Loss: 14.1991, Validation Loss: 14.1920\n",
      "Epoch 2200/6000, Training Loss: 14.1930, Validation Loss: 14.1859\n",
      "Epoch 2201/6000, Training Loss: 14.1868, Validation Loss: 14.1798\n",
      "Epoch 2202/6000, Training Loss: 14.1807, Validation Loss: 14.1738\n",
      "Epoch 2203/6000, Training Loss: 14.1747, Validation Loss: 14.1677\n",
      "Epoch 2204/6000, Training Loss: 14.1686, Validation Loss: 14.1617\n",
      "Epoch 2205/6000, Training Loss: 14.1625, Validation Loss: 14.1557\n",
      "Epoch 2206/6000, Training Loss: 14.1565, Validation Loss: 14.1497\n",
      "Epoch 2207/6000, Training Loss: 14.1504, Validation Loss: 14.1437\n",
      "Epoch 2208/6000, Training Loss: 14.1444, Validation Loss: 14.1377\n",
      "Epoch 2209/6000, Training Loss: 14.1384, Validation Loss: 14.1318\n",
      "Epoch 2210/6000, Training Loss: 14.1325, Validation Loss: 14.1258\n",
      "Epoch 2211/6000, Training Loss: 14.1265, Validation Loss: 14.1199\n",
      "Epoch 2212/6000, Training Loss: 14.1206, Validation Loss: 14.1140\n",
      "Epoch 2213/6000, Training Loss: 14.1147, Validation Loss: 14.1082\n",
      "Epoch 2214/6000, Training Loss: 14.1088, Validation Loss: 14.1023\n",
      "Epoch 2215/6000, Training Loss: 14.1029, Validation Loss: 14.0964\n",
      "Epoch 2216/6000, Training Loss: 14.0970, Validation Loss: 14.0906\n",
      "Epoch 2217/6000, Training Loss: 14.0911, Validation Loss: 14.0848\n",
      "Epoch 2218/6000, Training Loss: 14.0852, Validation Loss: 14.0789\n",
      "Epoch 2219/6000, Training Loss: 14.0794, Validation Loss: 14.0731\n",
      "Epoch 2220/6000, Training Loss: 14.0736, Validation Loss: 14.0673\n",
      "Epoch 2221/6000, Training Loss: 14.0677, Validation Loss: 14.0615\n",
      "Epoch 2222/6000, Training Loss: 14.0619, Validation Loss: 14.0557\n",
      "Epoch 2223/6000, Training Loss: 14.0561, Validation Loss: 14.0499\n",
      "Epoch 2224/6000, Training Loss: 14.0502, Validation Loss: 14.0441\n",
      "Epoch 2225/6000, Training Loss: 14.0444, Validation Loss: 14.0383\n",
      "Epoch 2226/6000, Training Loss: 14.0386, Validation Loss: 14.0326\n",
      "Epoch 2227/6000, Training Loss: 14.0328, Validation Loss: 14.0268\n",
      "Epoch 2228/6000, Training Loss: 14.0270, Validation Loss: 14.0210\n",
      "Epoch 2229/6000, Training Loss: 14.0212, Validation Loss: 14.0153\n",
      "Epoch 2230/6000, Training Loss: 14.0154, Validation Loss: 14.0095\n",
      "Epoch 2231/6000, Training Loss: 14.0097, Validation Loss: 14.0038\n",
      "Epoch 2232/6000, Training Loss: 14.0039, Validation Loss: 13.9981\n",
      "Epoch 2233/6000, Training Loss: 13.9981, Validation Loss: 13.9924\n",
      "Epoch 2234/6000, Training Loss: 13.9924, Validation Loss: 13.9867\n",
      "Epoch 2235/6000, Training Loss: 13.9867, Validation Loss: 13.9810\n",
      "Epoch 2236/6000, Training Loss: 13.9810, Validation Loss: 13.9754\n",
      "Epoch 2237/6000, Training Loss: 13.9753, Validation Loss: 13.9697\n",
      "Epoch 2238/6000, Training Loss: 13.9696, Validation Loss: 13.9641\n",
      "Epoch 2239/6000, Training Loss: 13.9639, Validation Loss: 13.9584\n",
      "Epoch 2240/6000, Training Loss: 13.9582, Validation Loss: 13.9528\n",
      "Epoch 2241/6000, Training Loss: 13.9525, Validation Loss: 13.9471\n",
      "Epoch 2242/6000, Training Loss: 13.9468, Validation Loss: 13.9415\n",
      "Epoch 2243/6000, Training Loss: 13.9412, Validation Loss: 13.9359\n",
      "Epoch 2244/6000, Training Loss: 13.9355, Validation Loss: 13.9302\n",
      "Epoch 2245/6000, Training Loss: 13.9299, Validation Loss: 13.9246\n",
      "Epoch 2246/6000, Training Loss: 13.9242, Validation Loss: 13.9190\n",
      "Epoch 2247/6000, Training Loss: 13.9186, Validation Loss: 13.9134\n",
      "Epoch 2248/6000, Training Loss: 13.9130, Validation Loss: 13.9078\n",
      "Epoch 2249/6000, Training Loss: 13.9074, Validation Loss: 13.9022\n",
      "Epoch 2250/6000, Training Loss: 13.9018, Validation Loss: 13.8966\n",
      "Epoch 2251/6000, Training Loss: 13.8962, Validation Loss: 13.8911\n",
      "Epoch 2252/6000, Training Loss: 13.8907, Validation Loss: 13.8856\n",
      "Epoch 2253/6000, Training Loss: 13.8852, Validation Loss: 13.8801\n",
      "Epoch 2254/6000, Training Loss: 13.8797, Validation Loss: 13.8746\n",
      "Epoch 2255/6000, Training Loss: 13.8742, Validation Loss: 13.8691\n",
      "Epoch 2256/6000, Training Loss: 13.8687, Validation Loss: 13.8637\n",
      "Epoch 2257/6000, Training Loss: 13.8632, Validation Loss: 13.8582\n",
      "Epoch 2258/6000, Training Loss: 13.8578, Validation Loss: 13.8528\n",
      "Epoch 2259/6000, Training Loss: 13.8524, Validation Loss: 13.8474\n",
      "Epoch 2260/6000, Training Loss: 13.8469, Validation Loss: 13.8420\n",
      "Epoch 2261/6000, Training Loss: 13.8415, Validation Loss: 13.8366\n",
      "Epoch 2262/6000, Training Loss: 13.8361, Validation Loss: 13.8312\n",
      "Epoch 2263/6000, Training Loss: 13.8307, Validation Loss: 13.8258\n",
      "Epoch 2264/6000, Training Loss: 13.8253, Validation Loss: 13.8204\n",
      "Epoch 2265/6000, Training Loss: 13.8199, Validation Loss: 13.8150\n",
      "Epoch 2266/6000, Training Loss: 13.8145, Validation Loss: 13.8097\n",
      "Epoch 2267/6000, Training Loss: 13.8091, Validation Loss: 13.8043\n",
      "Epoch 2268/6000, Training Loss: 13.8037, Validation Loss: 13.7989\n",
      "Epoch 2269/6000, Training Loss: 13.7983, Validation Loss: 13.7936\n",
      "Epoch 2270/6000, Training Loss: 13.7930, Validation Loss: 13.7882\n",
      "Epoch 2271/6000, Training Loss: 13.7876, Validation Loss: 13.7829\n",
      "Epoch 2272/6000, Training Loss: 13.7822, Validation Loss: 13.7776\n",
      "Epoch 2273/6000, Training Loss: 13.7769, Validation Loss: 13.7722\n",
      "Epoch 2274/6000, Training Loss: 13.7715, Validation Loss: 13.7669\n",
      "Epoch 2275/6000, Training Loss: 13.7662, Validation Loss: 13.7616\n",
      "Epoch 2276/6000, Training Loss: 13.7609, Validation Loss: 13.7564\n",
      "Epoch 2277/6000, Training Loss: 13.7556, Validation Loss: 13.7511\n",
      "Epoch 2278/6000, Training Loss: 13.7503, Validation Loss: 13.7458\n",
      "Epoch 2279/6000, Training Loss: 13.7450, Validation Loss: 13.7406\n",
      "Epoch 2280/6000, Training Loss: 13.7397, Validation Loss: 13.7353\n",
      "Epoch 2281/6000, Training Loss: 13.7344, Validation Loss: 13.7301\n",
      "Epoch 2282/6000, Training Loss: 13.7292, Validation Loss: 13.7249\n",
      "Epoch 2283/6000, Training Loss: 13.7239, Validation Loss: 13.7196\n",
      "Epoch 2284/6000, Training Loss: 13.7186, Validation Loss: 13.7144\n",
      "Epoch 2285/6000, Training Loss: 13.7134, Validation Loss: 13.7092\n",
      "Epoch 2286/6000, Training Loss: 13.7081, Validation Loss: 13.7040\n",
      "Epoch 2287/6000, Training Loss: 13.7029, Validation Loss: 13.6987\n",
      "Epoch 2288/6000, Training Loss: 13.6977, Validation Loss: 13.6935\n",
      "Epoch 2289/6000, Training Loss: 13.6924, Validation Loss: 13.6883\n",
      "Epoch 2290/6000, Training Loss: 13.6872, Validation Loss: 13.6831\n",
      "Epoch 2291/6000, Training Loss: 13.6820, Validation Loss: 13.6779\n",
      "Epoch 2292/6000, Training Loss: 13.6769, Validation Loss: 13.6727\n",
      "Epoch 2293/6000, Training Loss: 13.6717, Validation Loss: 13.6676\n",
      "Epoch 2294/6000, Training Loss: 13.6665, Validation Loss: 13.6624\n",
      "Epoch 2295/6000, Training Loss: 13.6614, Validation Loss: 13.6573\n",
      "Epoch 2296/6000, Training Loss: 13.6563, Validation Loss: 13.6522\n",
      "Epoch 2297/6000, Training Loss: 13.6512, Validation Loss: 13.6471\n",
      "Epoch 2298/6000, Training Loss: 13.6461, Validation Loss: 13.6420\n",
      "Epoch 2299/6000, Training Loss: 13.6411, Validation Loss: 13.6370\n",
      "Epoch 2300/6000, Training Loss: 13.6360, Validation Loss: 13.6319\n",
      "Epoch 2301/6000, Training Loss: 13.6310, Validation Loss: 13.6269\n",
      "Epoch 2302/6000, Training Loss: 13.6259, Validation Loss: 13.6219\n",
      "Epoch 2303/6000, Training Loss: 13.6209, Validation Loss: 13.6169\n",
      "Epoch 2304/6000, Training Loss: 13.6159, Validation Loss: 13.6119\n",
      "Epoch 2305/6000, Training Loss: 13.6109, Validation Loss: 13.6069\n",
      "Epoch 2306/6000, Training Loss: 13.6059, Validation Loss: 13.6019\n",
      "Epoch 2307/6000, Training Loss: 13.6009, Validation Loss: 13.5969\n",
      "Epoch 2308/6000, Training Loss: 13.5959, Validation Loss: 13.5920\n",
      "Epoch 2309/6000, Training Loss: 13.5910, Validation Loss: 13.5870\n",
      "Epoch 2310/6000, Training Loss: 13.5860, Validation Loss: 13.5821\n",
      "Epoch 2311/6000, Training Loss: 13.5810, Validation Loss: 13.5771\n",
      "Epoch 2312/6000, Training Loss: 13.5760, Validation Loss: 13.5722\n",
      "Epoch 2313/6000, Training Loss: 13.5711, Validation Loss: 13.5672\n",
      "Epoch 2314/6000, Training Loss: 13.5661, Validation Loss: 13.5623\n",
      "Epoch 2315/6000, Training Loss: 13.5612, Validation Loss: 13.5574\n",
      "Epoch 2316/6000, Training Loss: 13.5562, Validation Loss: 13.5524\n",
      "Epoch 2317/6000, Training Loss: 13.5513, Validation Loss: 13.5475\n",
      "Epoch 2318/6000, Training Loss: 13.5464, Validation Loss: 13.5426\n",
      "Epoch 2319/6000, Training Loss: 13.5414, Validation Loss: 13.5378\n",
      "Epoch 2320/6000, Training Loss: 13.5365, Validation Loss: 13.5329\n",
      "Epoch 2321/6000, Training Loss: 13.5316, Validation Loss: 13.5280\n",
      "Epoch 2322/6000, Training Loss: 13.5267, Validation Loss: 13.5232\n",
      "Epoch 2323/6000, Training Loss: 13.5219, Validation Loss: 13.5183\n",
      "Epoch 2324/6000, Training Loss: 13.5170, Validation Loss: 13.5135\n",
      "Epoch 2325/6000, Training Loss: 13.5121, Validation Loss: 13.5087\n",
      "Epoch 2326/6000, Training Loss: 13.5072, Validation Loss: 13.5038\n",
      "Epoch 2327/6000, Training Loss: 13.5024, Validation Loss: 13.4990\n",
      "Epoch 2328/6000, Training Loss: 13.4975, Validation Loss: 13.4942\n",
      "Epoch 2329/6000, Training Loss: 13.4927, Validation Loss: 13.4894\n",
      "Epoch 2330/6000, Training Loss: 13.4879, Validation Loss: 13.4846\n",
      "Epoch 2331/6000, Training Loss: 13.4830, Validation Loss: 13.4797\n",
      "Epoch 2332/6000, Training Loss: 13.4782, Validation Loss: 13.4749\n",
      "Epoch 2333/6000, Training Loss: 13.4734, Validation Loss: 13.4701\n",
      "Epoch 2334/6000, Training Loss: 13.4686, Validation Loss: 13.4653\n",
      "Epoch 2335/6000, Training Loss: 13.4638, Validation Loss: 13.4605\n",
      "Epoch 2336/6000, Training Loss: 13.4590, Validation Loss: 13.4557\n",
      "Epoch 2337/6000, Training Loss: 13.4542, Validation Loss: 13.4509\n",
      "Epoch 2338/6000, Training Loss: 13.4494, Validation Loss: 13.4461\n",
      "Epoch 2339/6000, Training Loss: 13.4447, Validation Loss: 13.4414\n",
      "Epoch 2340/6000, Training Loss: 13.4400, Validation Loss: 13.4367\n",
      "Epoch 2341/6000, Training Loss: 13.4353, Validation Loss: 13.4320\n",
      "Epoch 2342/6000, Training Loss: 13.4306, Validation Loss: 13.4273\n",
      "Epoch 2343/6000, Training Loss: 13.4259, Validation Loss: 13.4226\n",
      "Epoch 2344/6000, Training Loss: 13.4212, Validation Loss: 13.4179\n",
      "Epoch 2345/6000, Training Loss: 13.4166, Validation Loss: 13.4133\n",
      "Epoch 2346/6000, Training Loss: 13.4119, Validation Loss: 13.4087\n",
      "Epoch 2347/6000, Training Loss: 13.4073, Validation Loss: 13.4040\n",
      "Epoch 2348/6000, Training Loss: 13.4027, Validation Loss: 13.3994\n",
      "Epoch 2349/6000, Training Loss: 13.3981, Validation Loss: 13.3948\n",
      "Epoch 2350/6000, Training Loss: 13.3935, Validation Loss: 13.3902\n",
      "Epoch 2351/6000, Training Loss: 13.3889, Validation Loss: 13.3857\n",
      "Epoch 2352/6000, Training Loss: 13.3843, Validation Loss: 13.3811\n",
      "Epoch 2353/6000, Training Loss: 13.3797, Validation Loss: 13.3765\n",
      "Epoch 2354/6000, Training Loss: 13.3751, Validation Loss: 13.3720\n",
      "Epoch 2355/6000, Training Loss: 13.3705, Validation Loss: 13.3674\n",
      "Epoch 2356/6000, Training Loss: 13.3660, Validation Loss: 13.3629\n",
      "Epoch 2357/6000, Training Loss: 13.3614, Validation Loss: 13.3583\n",
      "Epoch 2358/6000, Training Loss: 13.3568, Validation Loss: 13.3538\n",
      "Epoch 2359/6000, Training Loss: 13.3523, Validation Loss: 13.3492\n",
      "Epoch 2360/6000, Training Loss: 13.3477, Validation Loss: 13.3447\n",
      "Epoch 2361/6000, Training Loss: 13.3432, Validation Loss: 13.3402\n",
      "Epoch 2362/6000, Training Loss: 13.3386, Validation Loss: 13.3357\n",
      "Epoch 2363/6000, Training Loss: 13.3341, Validation Loss: 13.3312\n",
      "Epoch 2364/6000, Training Loss: 13.3296, Validation Loss: 13.3267\n",
      "Epoch 2365/6000, Training Loss: 13.3250, Validation Loss: 13.3222\n",
      "Epoch 2366/6000, Training Loss: 13.3205, Validation Loss: 13.3178\n",
      "Epoch 2367/6000, Training Loss: 13.3160, Validation Loss: 13.3133\n",
      "Epoch 2368/6000, Training Loss: 13.3115, Validation Loss: 13.3089\n",
      "Epoch 2369/6000, Training Loss: 13.3070, Validation Loss: 13.3044\n",
      "Epoch 2370/6000, Training Loss: 13.3026, Validation Loss: 13.3000\n",
      "Epoch 2371/6000, Training Loss: 13.2981, Validation Loss: 13.2956\n",
      "Epoch 2372/6000, Training Loss: 13.2936, Validation Loss: 13.2911\n",
      "Epoch 2373/6000, Training Loss: 13.2892, Validation Loss: 13.2867\n",
      "Epoch 2374/6000, Training Loss: 13.2847, Validation Loss: 13.2823\n",
      "Epoch 2375/6000, Training Loss: 13.2803, Validation Loss: 13.2779\n",
      "Epoch 2376/6000, Training Loss: 13.2759, Validation Loss: 13.2734\n",
      "Epoch 2377/6000, Training Loss: 13.2714, Validation Loss: 13.2690\n",
      "Epoch 2378/6000, Training Loss: 13.2670, Validation Loss: 13.2646\n",
      "Epoch 2379/6000, Training Loss: 13.2626, Validation Loss: 13.2601\n",
      "Epoch 2380/6000, Training Loss: 13.2582, Validation Loss: 13.2557\n",
      "Epoch 2381/6000, Training Loss: 13.2538, Validation Loss: 13.2513\n",
      "Epoch 2382/6000, Training Loss: 13.2494, Validation Loss: 13.2469\n",
      "Epoch 2383/6000, Training Loss: 13.2450, Validation Loss: 13.2425\n",
      "Epoch 2384/6000, Training Loss: 13.2406, Validation Loss: 13.2381\n",
      "Epoch 2385/6000, Training Loss: 13.2362, Validation Loss: 13.2337\n",
      "Epoch 2386/6000, Training Loss: 13.2319, Validation Loss: 13.2294\n",
      "Epoch 2387/6000, Training Loss: 13.2276, Validation Loss: 13.2251\n",
      "Epoch 2388/6000, Training Loss: 13.2233, Validation Loss: 13.2208\n",
      "Epoch 2389/6000, Training Loss: 13.2190, Validation Loss: 13.2165\n",
      "Epoch 2390/6000, Training Loss: 13.2147, Validation Loss: 13.2122\n",
      "Epoch 2391/6000, Training Loss: 13.2104, Validation Loss: 13.2080\n",
      "Epoch 2392/6000, Training Loss: 13.2062, Validation Loss: 13.2037\n",
      "Epoch 2393/6000, Training Loss: 13.2019, Validation Loss: 13.1995\n",
      "Epoch 2394/6000, Training Loss: 13.1977, Validation Loss: 13.1953\n",
      "Epoch 2395/6000, Training Loss: 13.1935, Validation Loss: 13.1911\n",
      "Epoch 2396/6000, Training Loss: 13.1892, Validation Loss: 13.1869\n",
      "Epoch 2397/6000, Training Loss: 13.1850, Validation Loss: 13.1827\n",
      "Epoch 2398/6000, Training Loss: 13.1808, Validation Loss: 13.1785\n",
      "Epoch 2399/6000, Training Loss: 13.1766, Validation Loss: 13.1743\n",
      "Epoch 2400/6000, Training Loss: 13.1724, Validation Loss: 13.1701\n",
      "Epoch 2401/6000, Training Loss: 13.1682, Validation Loss: 13.1659\n",
      "Epoch 2402/6000, Training Loss: 13.1640, Validation Loss: 13.1618\n",
      "Epoch 2403/6000, Training Loss: 13.1598, Validation Loss: 13.1576\n",
      "Epoch 2404/6000, Training Loss: 13.1556, Validation Loss: 13.1534\n",
      "Epoch 2405/6000, Training Loss: 13.1515, Validation Loss: 13.1493\n",
      "Epoch 2406/6000, Training Loss: 13.1473, Validation Loss: 13.1451\n",
      "Epoch 2407/6000, Training Loss: 13.1431, Validation Loss: 13.1410\n",
      "Epoch 2408/6000, Training Loss: 13.1390, Validation Loss: 13.1368\n",
      "Epoch 2409/6000, Training Loss: 13.1348, Validation Loss: 13.1327\n",
      "Epoch 2410/6000, Training Loss: 13.1306, Validation Loss: 13.1286\n",
      "Epoch 2411/6000, Training Loss: 13.1265, Validation Loss: 13.1245\n",
      "Epoch 2412/6000, Training Loss: 13.1224, Validation Loss: 13.1203\n",
      "Epoch 2413/6000, Training Loss: 13.1182, Validation Loss: 13.1162\n",
      "Epoch 2414/6000, Training Loss: 13.1141, Validation Loss: 13.1122\n",
      "Epoch 2415/6000, Training Loss: 13.1100, Validation Loss: 13.1081\n",
      "Epoch 2416/6000, Training Loss: 13.1059, Validation Loss: 13.1040\n",
      "Epoch 2417/6000, Training Loss: 13.1018, Validation Loss: 13.0999\n",
      "Epoch 2418/6000, Training Loss: 13.0977, Validation Loss: 13.0959\n",
      "Epoch 2419/6000, Training Loss: 13.0936, Validation Loss: 13.0918\n",
      "Epoch 2420/6000, Training Loss: 13.0895, Validation Loss: 13.0877\n",
      "Epoch 2421/6000, Training Loss: 13.0854, Validation Loss: 13.0837\n",
      "Epoch 2422/6000, Training Loss: 13.0813, Validation Loss: 13.0796\n",
      "Epoch 2423/6000, Training Loss: 13.0773, Validation Loss: 13.0756\n",
      "Epoch 2424/6000, Training Loss: 13.0732, Validation Loss: 13.0716\n",
      "Epoch 2425/6000, Training Loss: 13.0692, Validation Loss: 13.0675\n",
      "Epoch 2426/6000, Training Loss: 13.0651, Validation Loss: 13.0635\n",
      "Epoch 2427/6000, Training Loss: 13.0611, Validation Loss: 13.0594\n",
      "Epoch 2428/6000, Training Loss: 13.0570, Validation Loss: 13.0554\n",
      "Epoch 2429/6000, Training Loss: 13.0530, Validation Loss: 13.0514\n",
      "Epoch 2430/6000, Training Loss: 13.0490, Validation Loss: 13.0474\n",
      "Epoch 2431/6000, Training Loss: 13.0450, Validation Loss: 13.0434\n",
      "Epoch 2432/6000, Training Loss: 13.0410, Validation Loss: 13.0394\n",
      "Epoch 2433/6000, Training Loss: 13.0370, Validation Loss: 13.0354\n",
      "Epoch 2434/6000, Training Loss: 13.0330, Validation Loss: 13.0314\n",
      "Epoch 2435/6000, Training Loss: 13.0290, Validation Loss: 13.0275\n",
      "Epoch 2436/6000, Training Loss: 13.0251, Validation Loss: 13.0236\n",
      "Epoch 2437/6000, Training Loss: 13.0212, Validation Loss: 13.0196\n",
      "Epoch 2438/6000, Training Loss: 13.0173, Validation Loss: 13.0157\n",
      "Epoch 2439/6000, Training Loss: 13.0134, Validation Loss: 13.0118\n",
      "Epoch 2440/6000, Training Loss: 13.0095, Validation Loss: 13.0080\n",
      "Epoch 2441/6000, Training Loss: 13.0056, Validation Loss: 13.0041\n",
      "Epoch 2442/6000, Training Loss: 13.0017, Validation Loss: 13.0002\n",
      "Epoch 2443/6000, Training Loss: 12.9979, Validation Loss: 12.9964\n",
      "Epoch 2444/6000, Training Loss: 12.9940, Validation Loss: 12.9925\n",
      "Epoch 2445/6000, Training Loss: 12.9902, Validation Loss: 12.9887\n",
      "Epoch 2446/6000, Training Loss: 12.9863, Validation Loss: 12.9849\n",
      "Epoch 2447/6000, Training Loss: 12.9825, Validation Loss: 12.9811\n",
      "Epoch 2448/6000, Training Loss: 12.9787, Validation Loss: 12.9772\n",
      "Epoch 2449/6000, Training Loss: 12.9748, Validation Loss: 12.9734\n",
      "Epoch 2450/6000, Training Loss: 12.9710, Validation Loss: 12.9696\n",
      "Epoch 2451/6000, Training Loss: 12.9672, Validation Loss: 12.9658\n",
      "Epoch 2452/6000, Training Loss: 12.9634, Validation Loss: 12.9620\n",
      "Epoch 2453/6000, Training Loss: 12.9595, Validation Loss: 12.9582\n",
      "Epoch 2454/6000, Training Loss: 12.9557, Validation Loss: 12.9544\n",
      "Epoch 2455/6000, Training Loss: 12.9519, Validation Loss: 12.9506\n",
      "Epoch 2456/6000, Training Loss: 12.9481, Validation Loss: 12.9468\n",
      "Epoch 2457/6000, Training Loss: 12.9444, Validation Loss: 12.9431\n",
      "Epoch 2458/6000, Training Loss: 12.9406, Validation Loss: 12.9393\n",
      "Epoch 2459/6000, Training Loss: 12.9368, Validation Loss: 12.9355\n",
      "Epoch 2460/6000, Training Loss: 12.9330, Validation Loss: 12.9318\n",
      "Epoch 2461/6000, Training Loss: 12.9293, Validation Loss: 12.9280\n",
      "Epoch 2462/6000, Training Loss: 12.9255, Validation Loss: 12.9243\n",
      "Epoch 2463/6000, Training Loss: 12.9217, Validation Loss: 12.9206\n",
      "Epoch 2464/6000, Training Loss: 12.9180, Validation Loss: 12.9169\n",
      "Epoch 2465/6000, Training Loss: 12.9143, Validation Loss: 12.9131\n",
      "Epoch 2466/6000, Training Loss: 12.9105, Validation Loss: 12.9094\n",
      "Epoch 2467/6000, Training Loss: 12.9068, Validation Loss: 12.9057\n",
      "Epoch 2468/6000, Training Loss: 12.9031, Validation Loss: 12.9020\n",
      "Epoch 2469/6000, Training Loss: 12.8993, Validation Loss: 12.8983\n",
      "Epoch 2470/6000, Training Loss: 12.8956, Validation Loss: 12.8947\n",
      "Epoch 2471/6000, Training Loss: 12.8919, Validation Loss: 12.8910\n",
      "Epoch 2472/6000, Training Loss: 12.8882, Validation Loss: 12.8873\n",
      "Epoch 2473/6000, Training Loss: 12.8845, Validation Loss: 12.8836\n",
      "Epoch 2474/6000, Training Loss: 12.8808, Validation Loss: 12.8799\n",
      "Epoch 2475/6000, Training Loss: 12.8771, Validation Loss: 12.8762\n",
      "Epoch 2476/6000, Training Loss: 12.8735, Validation Loss: 12.8726\n",
      "Epoch 2477/6000, Training Loss: 12.8698, Validation Loss: 12.8689\n",
      "Epoch 2478/6000, Training Loss: 12.8661, Validation Loss: 12.8652\n",
      "Epoch 2479/6000, Training Loss: 12.8625, Validation Loss: 12.8616\n",
      "Epoch 2480/6000, Training Loss: 12.8588, Validation Loss: 12.8579\n",
      "Epoch 2481/6000, Training Loss: 12.8552, Validation Loss: 12.8543\n",
      "Epoch 2482/6000, Training Loss: 12.8515, Validation Loss: 12.8506\n",
      "Epoch 2483/6000, Training Loss: 12.8479, Validation Loss: 12.8470\n",
      "Epoch 2484/6000, Training Loss: 12.8443, Validation Loss: 12.8434\n",
      "Epoch 2485/6000, Training Loss: 12.8407, Validation Loss: 12.8398\n",
      "Epoch 2486/6000, Training Loss: 12.8371, Validation Loss: 12.8363\n",
      "Epoch 2487/6000, Training Loss: 12.8336, Validation Loss: 12.8327\n",
      "Epoch 2488/6000, Training Loss: 12.8300, Validation Loss: 12.8292\n",
      "Epoch 2489/6000, Training Loss: 12.8265, Validation Loss: 12.8256\n",
      "Epoch 2490/6000, Training Loss: 12.8229, Validation Loss: 12.8221\n",
      "Epoch 2491/6000, Training Loss: 12.8194, Validation Loss: 12.8186\n",
      "Epoch 2492/6000, Training Loss: 12.8159, Validation Loss: 12.8151\n",
      "Epoch 2493/6000, Training Loss: 12.8124, Validation Loss: 12.8116\n",
      "Epoch 2494/6000, Training Loss: 12.8089, Validation Loss: 12.8081\n",
      "Epoch 2495/6000, Training Loss: 12.8054, Validation Loss: 12.8047\n",
      "Epoch 2496/6000, Training Loss: 12.8019, Validation Loss: 12.8012\n",
      "Epoch 2497/6000, Training Loss: 12.7984, Validation Loss: 12.7977\n",
      "Epoch 2498/6000, Training Loss: 12.7949, Validation Loss: 12.7943\n",
      "Epoch 2499/6000, Training Loss: 12.7915, Validation Loss: 12.7908\n",
      "Epoch 2500/6000, Training Loss: 12.7880, Validation Loss: 12.7873\n",
      "Epoch 2501/6000, Training Loss: 12.7845, Validation Loss: 12.7839\n",
      "Epoch 2502/6000, Training Loss: 12.7811, Validation Loss: 12.7804\n",
      "Epoch 2503/6000, Training Loss: 12.7776, Validation Loss: 12.7770\n",
      "Epoch 2504/6000, Training Loss: 12.7742, Validation Loss: 12.7736\n",
      "Epoch 2505/6000, Training Loss: 12.7707, Validation Loss: 12.7701\n",
      "Epoch 2506/6000, Training Loss: 12.7673, Validation Loss: 12.7667\n",
      "Epoch 2507/6000, Training Loss: 12.7638, Validation Loss: 12.7633\n",
      "Epoch 2508/6000, Training Loss: 12.7604, Validation Loss: 12.7599\n",
      "Epoch 2509/6000, Training Loss: 12.7570, Validation Loss: 12.7565\n",
      "Epoch 2510/6000, Training Loss: 12.7535, Validation Loss: 12.7531\n",
      "Epoch 2511/6000, Training Loss: 12.7501, Validation Loss: 12.7497\n",
      "Epoch 2512/6000, Training Loss: 12.7467, Validation Loss: 12.7463\n",
      "Epoch 2513/6000, Training Loss: 12.7433, Validation Loss: 12.7429\n",
      "Epoch 2514/6000, Training Loss: 12.7399, Validation Loss: 12.7395\n",
      "Epoch 2515/6000, Training Loss: 12.7365, Validation Loss: 12.7362\n",
      "Epoch 2516/6000, Training Loss: 12.7331, Validation Loss: 12.7328\n",
      "Epoch 2517/6000, Training Loss: 12.7297, Validation Loss: 12.7294\n",
      "Epoch 2518/6000, Training Loss: 12.7263, Validation Loss: 12.7261\n",
      "Epoch 2519/6000, Training Loss: 12.7229, Validation Loss: 12.7228\n",
      "Epoch 2520/6000, Training Loss: 12.7196, Validation Loss: 12.7194\n",
      "Epoch 2521/6000, Training Loss: 12.7162, Validation Loss: 12.7161\n",
      "Epoch 2522/6000, Training Loss: 12.7129, Validation Loss: 12.7127\n",
      "Epoch 2523/6000, Training Loss: 12.7095, Validation Loss: 12.7094\n",
      "Epoch 2524/6000, Training Loss: 12.7062, Validation Loss: 12.7061\n",
      "Epoch 2525/6000, Training Loss: 12.7028, Validation Loss: 12.7028\n",
      "Epoch 2526/6000, Training Loss: 12.6995, Validation Loss: 12.6994\n",
      "Epoch 2527/6000, Training Loss: 12.6961, Validation Loss: 12.6961\n",
      "Epoch 2528/6000, Training Loss: 12.6928, Validation Loss: 12.6928\n",
      "Epoch 2529/6000, Training Loss: 12.6895, Validation Loss: 12.6894\n",
      "Epoch 2530/6000, Training Loss: 12.6861, Validation Loss: 12.6861\n",
      "Epoch 2531/6000, Training Loss: 12.6828, Validation Loss: 12.6828\n",
      "Epoch 2532/6000, Training Loss: 12.6795, Validation Loss: 12.6795\n",
      "Epoch 2533/6000, Training Loss: 12.6762, Validation Loss: 12.6762\n",
      "Epoch 2534/6000, Training Loss: 12.6729, Validation Loss: 12.6730\n",
      "Epoch 2535/6000, Training Loss: 12.6697, Validation Loss: 12.6697\n",
      "Epoch 2536/6000, Training Loss: 12.6664, Validation Loss: 12.6665\n",
      "Epoch 2537/6000, Training Loss: 12.6632, Validation Loss: 12.6632\n",
      "Epoch 2538/6000, Training Loss: 12.6599, Validation Loss: 12.6600\n",
      "Epoch 2539/6000, Training Loss: 12.6567, Validation Loss: 12.6568\n",
      "Epoch 2540/6000, Training Loss: 12.6535, Validation Loss: 12.6536\n",
      "Epoch 2541/6000, Training Loss: 12.6503, Validation Loss: 12.6505\n",
      "Epoch 2542/6000, Training Loss: 12.6471, Validation Loss: 12.6473\n",
      "Epoch 2543/6000, Training Loss: 12.6439, Validation Loss: 12.6441\n",
      "Epoch 2544/6000, Training Loss: 12.6408, Validation Loss: 12.6410\n",
      "Epoch 2545/6000, Training Loss: 12.6376, Validation Loss: 12.6378\n",
      "Epoch 2546/6000, Training Loss: 12.6344, Validation Loss: 12.6347\n",
      "Epoch 2547/6000, Training Loss: 12.6313, Validation Loss: 12.6316\n",
      "Epoch 2548/6000, Training Loss: 12.6281, Validation Loss: 12.6284\n",
      "Epoch 2549/6000, Training Loss: 12.6250, Validation Loss: 12.6253\n",
      "Epoch 2550/6000, Training Loss: 12.6218, Validation Loss: 12.6222\n",
      "Epoch 2551/6000, Training Loss: 12.6187, Validation Loss: 12.6191\n",
      "Epoch 2552/6000, Training Loss: 12.6156, Validation Loss: 12.6160\n",
      "Epoch 2553/6000, Training Loss: 12.6124, Validation Loss: 12.6129\n",
      "Epoch 2554/6000, Training Loss: 12.6093, Validation Loss: 12.6098\n",
      "Epoch 2555/6000, Training Loss: 12.6062, Validation Loss: 12.6067\n",
      "Epoch 2556/6000, Training Loss: 12.6031, Validation Loss: 12.6036\n",
      "Epoch 2557/6000, Training Loss: 12.5999, Validation Loss: 12.6005\n",
      "Epoch 2558/6000, Training Loss: 12.5968, Validation Loss: 12.5974\n",
      "Epoch 2559/6000, Training Loss: 12.5937, Validation Loss: 12.5944\n",
      "Epoch 2560/6000, Training Loss: 12.5906, Validation Loss: 12.5913\n",
      "Epoch 2561/6000, Training Loss: 12.5875, Validation Loss: 12.5882\n",
      "Epoch 2562/6000, Training Loss: 12.5845, Validation Loss: 12.5852\n",
      "Epoch 2563/6000, Training Loss: 12.5814, Validation Loss: 12.5821\n",
      "Epoch 2564/6000, Training Loss: 12.5783, Validation Loss: 12.5791\n",
      "Epoch 2565/6000, Training Loss: 12.5752, Validation Loss: 12.5761\n",
      "Epoch 2566/6000, Training Loss: 12.5722, Validation Loss: 12.5730\n",
      "Epoch 2567/6000, Training Loss: 12.5691, Validation Loss: 12.5700\n",
      "Epoch 2568/6000, Training Loss: 12.5660, Validation Loss: 12.5670\n",
      "Epoch 2569/6000, Training Loss: 12.5630, Validation Loss: 12.5639\n",
      "Epoch 2570/6000, Training Loss: 12.5599, Validation Loss: 12.5609\n",
      "Epoch 2571/6000, Training Loss: 12.5569, Validation Loss: 12.5579\n",
      "Epoch 2572/6000, Training Loss: 12.5539, Validation Loss: 12.5549\n",
      "Epoch 2573/6000, Training Loss: 12.5508, Validation Loss: 12.5519\n",
      "Epoch 2574/6000, Training Loss: 12.5478, Validation Loss: 12.5489\n",
      "Epoch 2575/6000, Training Loss: 12.5448, Validation Loss: 12.5459\n",
      "Epoch 2576/6000, Training Loss: 12.5418, Validation Loss: 12.5429\n",
      "Epoch 2577/6000, Training Loss: 12.5388, Validation Loss: 12.5399\n",
      "Epoch 2578/6000, Training Loss: 12.5358, Validation Loss: 12.5369\n",
      "Epoch 2579/6000, Training Loss: 12.5328, Validation Loss: 12.5339\n",
      "Epoch 2580/6000, Training Loss: 12.5298, Validation Loss: 12.5309\n",
      "Epoch 2581/6000, Training Loss: 12.5268, Validation Loss: 12.5279\n",
      "Epoch 2582/6000, Training Loss: 12.5238, Validation Loss: 12.5249\n",
      "Epoch 2583/6000, Training Loss: 12.5208, Validation Loss: 12.5219\n",
      "Epoch 2584/6000, Training Loss: 12.5178, Validation Loss: 12.5190\n",
      "Epoch 2585/6000, Training Loss: 12.5148, Validation Loss: 12.5160\n",
      "Epoch 2586/6000, Training Loss: 12.5119, Validation Loss: 12.5130\n",
      "Epoch 2587/6000, Training Loss: 12.5089, Validation Loss: 12.5101\n",
      "Epoch 2588/6000, Training Loss: 12.5060, Validation Loss: 12.5071\n",
      "Epoch 2589/6000, Training Loss: 12.5030, Validation Loss: 12.5042\n",
      "Epoch 2590/6000, Training Loss: 12.5001, Validation Loss: 12.5013\n",
      "Epoch 2591/6000, Training Loss: 12.4972, Validation Loss: 12.4984\n",
      "Epoch 2592/6000, Training Loss: 12.4943, Validation Loss: 12.4955\n",
      "Epoch 2593/6000, Training Loss: 12.4914, Validation Loss: 12.4926\n",
      "Epoch 2594/6000, Training Loss: 12.4885, Validation Loss: 12.4898\n",
      "Epoch 2595/6000, Training Loss: 12.4857, Validation Loss: 12.4869\n",
      "Epoch 2596/6000, Training Loss: 12.4828, Validation Loss: 12.4841\n",
      "Epoch 2597/6000, Training Loss: 12.4799, Validation Loss: 12.4812\n",
      "Epoch 2598/6000, Training Loss: 12.4771, Validation Loss: 12.4784\n",
      "Epoch 2599/6000, Training Loss: 12.4743, Validation Loss: 12.4756\n",
      "Epoch 2600/6000, Training Loss: 12.4714, Validation Loss: 12.4728\n",
      "Epoch 2601/6000, Training Loss: 12.4686, Validation Loss: 12.4700\n",
      "Epoch 2602/6000, Training Loss: 12.4658, Validation Loss: 12.4672\n",
      "Epoch 2603/6000, Training Loss: 12.4630, Validation Loss: 12.4644\n",
      "Epoch 2604/6000, Training Loss: 12.4602, Validation Loss: 12.4616\n",
      "Epoch 2605/6000, Training Loss: 12.4574, Validation Loss: 12.4588\n",
      "Epoch 2606/6000, Training Loss: 12.4546, Validation Loss: 12.4560\n",
      "Epoch 2607/6000, Training Loss: 12.4518, Validation Loss: 12.4532\n",
      "Epoch 2608/6000, Training Loss: 12.4490, Validation Loss: 12.4505\n",
      "Epoch 2609/6000, Training Loss: 12.4462, Validation Loss: 12.4477\n",
      "Epoch 2610/6000, Training Loss: 12.4434, Validation Loss: 12.4449\n",
      "Epoch 2611/6000, Training Loss: 12.4406, Validation Loss: 12.4421\n",
      "Epoch 2612/6000, Training Loss: 12.4378, Validation Loss: 12.4394\n",
      "Epoch 2613/6000, Training Loss: 12.4350, Validation Loss: 12.4366\n",
      "Epoch 2614/6000, Training Loss: 12.4323, Validation Loss: 12.4339\n",
      "Epoch 2615/6000, Training Loss: 12.4295, Validation Loss: 12.4311\n",
      "Epoch 2616/6000, Training Loss: 12.4267, Validation Loss: 12.4284\n",
      "Epoch 2617/6000, Training Loss: 12.4240, Validation Loss: 12.4257\n",
      "Epoch 2618/6000, Training Loss: 12.4212, Validation Loss: 12.4229\n",
      "Epoch 2619/6000, Training Loss: 12.4185, Validation Loss: 12.4202\n",
      "Epoch 2620/6000, Training Loss: 12.4158, Validation Loss: 12.4175\n",
      "Epoch 2621/6000, Training Loss: 12.4130, Validation Loss: 12.4148\n",
      "Epoch 2622/6000, Training Loss: 12.4103, Validation Loss: 12.4121\n",
      "Epoch 2623/6000, Training Loss: 12.4075, Validation Loss: 12.4094\n",
      "Epoch 2624/6000, Training Loss: 12.4048, Validation Loss: 12.4067\n",
      "Epoch 2625/6000, Training Loss: 12.4021, Validation Loss: 12.4040\n",
      "Epoch 2626/6000, Training Loss: 12.3994, Validation Loss: 12.4013\n",
      "Epoch 2627/6000, Training Loss: 12.3967, Validation Loss: 12.3986\n",
      "Epoch 2628/6000, Training Loss: 12.3940, Validation Loss: 12.3959\n",
      "Epoch 2629/6000, Training Loss: 12.3913, Validation Loss: 12.3932\n",
      "Epoch 2630/6000, Training Loss: 12.3886, Validation Loss: 12.3905\n",
      "Epoch 2631/6000, Training Loss: 12.3859, Validation Loss: 12.3878\n",
      "Epoch 2632/6000, Training Loss: 12.3832, Validation Loss: 12.3851\n",
      "Epoch 2633/6000, Training Loss: 12.3805, Validation Loss: 12.3824\n",
      "Epoch 2634/6000, Training Loss: 12.3778, Validation Loss: 12.3798\n",
      "Epoch 2635/6000, Training Loss: 12.3751, Validation Loss: 12.3771\n",
      "Epoch 2636/6000, Training Loss: 12.3725, Validation Loss: 12.3744\n",
      "Epoch 2637/6000, Training Loss: 12.3698, Validation Loss: 12.3717\n",
      "Epoch 2638/6000, Training Loss: 12.3671, Validation Loss: 12.3691\n",
      "Epoch 2639/6000, Training Loss: 12.3645, Validation Loss: 12.3664\n",
      "Epoch 2640/6000, Training Loss: 12.3618, Validation Loss: 12.3637\n",
      "Epoch 2641/6000, Training Loss: 12.3592, Validation Loss: 12.3611\n",
      "Epoch 2642/6000, Training Loss: 12.3565, Validation Loss: 12.3584\n",
      "Epoch 2643/6000, Training Loss: 12.3539, Validation Loss: 12.3558\n",
      "Epoch 2644/6000, Training Loss: 12.3513, Validation Loss: 12.3532\n",
      "Epoch 2645/6000, Training Loss: 12.3486, Validation Loss: 12.3506\n",
      "Epoch 2646/6000, Training Loss: 12.3460, Validation Loss: 12.3480\n",
      "Epoch 2647/6000, Training Loss: 12.3435, Validation Loss: 12.3454\n",
      "Epoch 2648/6000, Training Loss: 12.3409, Validation Loss: 12.3428\n",
      "Epoch 2649/6000, Training Loss: 12.3383, Validation Loss: 12.3403\n",
      "Epoch 2650/6000, Training Loss: 12.3358, Validation Loss: 12.3377\n",
      "Epoch 2651/6000, Training Loss: 12.3332, Validation Loss: 12.3352\n",
      "Epoch 2652/6000, Training Loss: 12.3307, Validation Loss: 12.3326\n",
      "Epoch 2653/6000, Training Loss: 12.3281, Validation Loss: 12.3301\n",
      "Epoch 2654/6000, Training Loss: 12.3256, Validation Loss: 12.3276\n",
      "Epoch 2655/6000, Training Loss: 12.3231, Validation Loss: 12.3251\n",
      "Epoch 2656/6000, Training Loss: 12.3206, Validation Loss: 12.3226\n",
      "Epoch 2657/6000, Training Loss: 12.3181, Validation Loss: 12.3201\n",
      "Epoch 2658/6000, Training Loss: 12.3156, Validation Loss: 12.3176\n",
      "Epoch 2659/6000, Training Loss: 12.3131, Validation Loss: 12.3152\n",
      "Epoch 2660/6000, Training Loss: 12.3106, Validation Loss: 12.3127\n",
      "Epoch 2661/6000, Training Loss: 12.3081, Validation Loss: 12.3102\n",
      "Epoch 2662/6000, Training Loss: 12.3056, Validation Loss: 12.3077\n",
      "Epoch 2663/6000, Training Loss: 12.3031, Validation Loss: 12.3053\n",
      "Epoch 2664/6000, Training Loss: 12.3007, Validation Loss: 12.3028\n",
      "Epoch 2665/6000, Training Loss: 12.2982, Validation Loss: 12.3004\n",
      "Epoch 2666/6000, Training Loss: 12.2957, Validation Loss: 12.2979\n",
      "Epoch 2667/6000, Training Loss: 12.2933, Validation Loss: 12.2955\n",
      "Epoch 2668/6000, Training Loss: 12.2908, Validation Loss: 12.2930\n",
      "Epoch 2669/6000, Training Loss: 12.2883, Validation Loss: 12.2906\n",
      "Epoch 2670/6000, Training Loss: 12.2859, Validation Loss: 12.2881\n",
      "Epoch 2671/6000, Training Loss: 12.2834, Validation Loss: 12.2857\n",
      "Epoch 2672/6000, Training Loss: 12.2810, Validation Loss: 12.2833\n",
      "Epoch 2673/6000, Training Loss: 12.2786, Validation Loss: 12.2808\n",
      "Epoch 2674/6000, Training Loss: 12.2761, Validation Loss: 12.2784\n",
      "Epoch 2675/6000, Training Loss: 12.2737, Validation Loss: 12.2760\n",
      "Epoch 2676/6000, Training Loss: 12.2713, Validation Loss: 12.2736\n",
      "Epoch 2677/6000, Training Loss: 12.2689, Validation Loss: 12.2712\n",
      "Epoch 2678/6000, Training Loss: 12.2664, Validation Loss: 12.2688\n",
      "Epoch 2679/6000, Training Loss: 12.2640, Validation Loss: 12.2664\n",
      "Epoch 2680/6000, Training Loss: 12.2616, Validation Loss: 12.2640\n",
      "Epoch 2681/6000, Training Loss: 12.2592, Validation Loss: 12.2616\n",
      "Epoch 2682/6000, Training Loss: 12.2568, Validation Loss: 12.2592\n",
      "Epoch 2683/6000, Training Loss: 12.2544, Validation Loss: 12.2568\n",
      "Epoch 2684/6000, Training Loss: 12.2520, Validation Loss: 12.2544\n",
      "Epoch 2685/6000, Training Loss: 12.2496, Validation Loss: 12.2520\n",
      "Epoch 2686/6000, Training Loss: 12.2472, Validation Loss: 12.2496\n",
      "Epoch 2687/6000, Training Loss: 12.2448, Validation Loss: 12.2473\n",
      "Epoch 2688/6000, Training Loss: 12.2424, Validation Loss: 12.2449\n",
      "Epoch 2689/6000, Training Loss: 12.2401, Validation Loss: 12.2425\n",
      "Epoch 2690/6000, Training Loss: 12.2377, Validation Loss: 12.2401\n",
      "Epoch 2691/6000, Training Loss: 12.2353, Validation Loss: 12.2378\n",
      "Epoch 2692/6000, Training Loss: 12.2330, Validation Loss: 12.2354\n",
      "Epoch 2693/6000, Training Loss: 12.2306, Validation Loss: 12.2330\n",
      "Epoch 2694/6000, Training Loss: 12.2282, Validation Loss: 12.2307\n",
      "Epoch 2695/6000, Training Loss: 12.2259, Validation Loss: 12.2283\n",
      "Epoch 2696/6000, Training Loss: 12.2235, Validation Loss: 12.2260\n",
      "Epoch 2697/6000, Training Loss: 12.2212, Validation Loss: 12.2236\n",
      "Epoch 2698/6000, Training Loss: 12.2188, Validation Loss: 12.2213\n",
      "Epoch 2699/6000, Training Loss: 12.2165, Validation Loss: 12.2189\n",
      "Epoch 2700/6000, Training Loss: 12.2142, Validation Loss: 12.2166\n",
      "Epoch 2701/6000, Training Loss: 12.2118, Validation Loss: 12.2142\n",
      "Epoch 2702/6000, Training Loss: 12.2095, Validation Loss: 12.2119\n",
      "Epoch 2703/6000, Training Loss: 12.2072, Validation Loss: 12.2096\n",
      "Epoch 2704/6000, Training Loss: 12.2049, Validation Loss: 12.2073\n",
      "Epoch 2705/6000, Training Loss: 12.2026, Validation Loss: 12.2050\n",
      "Epoch 2706/6000, Training Loss: 12.2003, Validation Loss: 12.2028\n",
      "Epoch 2707/6000, Training Loss: 12.1981, Validation Loss: 12.2005\n",
      "Epoch 2708/6000, Training Loss: 12.1958, Validation Loss: 12.1982\n",
      "Epoch 2709/6000, Training Loss: 12.1935, Validation Loss: 12.1960\n",
      "Epoch 2710/6000, Training Loss: 12.1913, Validation Loss: 12.1937\n",
      "Epoch 2711/6000, Training Loss: 12.1891, Validation Loss: 12.1915\n",
      "Epoch 2712/6000, Training Loss: 12.1868, Validation Loss: 12.1893\n",
      "Epoch 2713/6000, Training Loss: 12.1846, Validation Loss: 12.1871\n",
      "Epoch 2714/6000, Training Loss: 12.1824, Validation Loss: 12.1849\n",
      "Epoch 2715/6000, Training Loss: 12.1802, Validation Loss: 12.1827\n",
      "Epoch 2716/6000, Training Loss: 12.1780, Validation Loss: 12.1805\n",
      "Epoch 2717/6000, Training Loss: 12.1758, Validation Loss: 12.1783\n",
      "Epoch 2718/6000, Training Loss: 12.1736, Validation Loss: 12.1761\n",
      "Epoch 2719/6000, Training Loss: 12.1714, Validation Loss: 12.1739\n",
      "Epoch 2720/6000, Training Loss: 12.1692, Validation Loss: 12.1718\n",
      "Epoch 2721/6000, Training Loss: 12.1670, Validation Loss: 12.1696\n",
      "Epoch 2722/6000, Training Loss: 12.1648, Validation Loss: 12.1674\n",
      "Epoch 2723/6000, Training Loss: 12.1627, Validation Loss: 12.1653\n",
      "Epoch 2724/6000, Training Loss: 12.1605, Validation Loss: 12.1631\n",
      "Epoch 2725/6000, Training Loss: 12.1583, Validation Loss: 12.1609\n",
      "Epoch 2726/6000, Training Loss: 12.1562, Validation Loss: 12.1588\n",
      "Epoch 2727/6000, Training Loss: 12.1540, Validation Loss: 12.1566\n",
      "Epoch 2728/6000, Training Loss: 12.1519, Validation Loss: 12.1545\n",
      "Epoch 2729/6000, Training Loss: 12.1497, Validation Loss: 12.1523\n",
      "Epoch 2730/6000, Training Loss: 12.1476, Validation Loss: 12.1502\n",
      "Epoch 2731/6000, Training Loss: 12.1454, Validation Loss: 12.1481\n",
      "Epoch 2732/6000, Training Loss: 12.1433, Validation Loss: 12.1459\n",
      "Epoch 2733/6000, Training Loss: 12.1411, Validation Loss: 12.1438\n",
      "Epoch 2734/6000, Training Loss: 12.1390, Validation Loss: 12.1417\n",
      "Epoch 2735/6000, Training Loss: 12.1369, Validation Loss: 12.1396\n",
      "Epoch 2736/6000, Training Loss: 12.1347, Validation Loss: 12.1375\n",
      "Epoch 2737/6000, Training Loss: 12.1326, Validation Loss: 12.1353\n",
      "Epoch 2738/6000, Training Loss: 12.1305, Validation Loss: 12.1332\n",
      "Epoch 2739/6000, Training Loss: 12.1284, Validation Loss: 12.1311\n",
      "Epoch 2740/6000, Training Loss: 12.1263, Validation Loss: 12.1290\n",
      "Epoch 2741/6000, Training Loss: 12.1242, Validation Loss: 12.1269\n",
      "Epoch 2742/6000, Training Loss: 12.1220, Validation Loss: 12.1248\n",
      "Epoch 2743/6000, Training Loss: 12.1199, Validation Loss: 12.1227\n",
      "Epoch 2744/6000, Training Loss: 12.1178, Validation Loss: 12.1206\n",
      "Epoch 2745/6000, Training Loss: 12.1157, Validation Loss: 12.1186\n",
      "Epoch 2746/6000, Training Loss: 12.1137, Validation Loss: 12.1165\n",
      "Epoch 2747/6000, Training Loss: 12.1116, Validation Loss: 12.1144\n",
      "Epoch 2748/6000, Training Loss: 12.1095, Validation Loss: 12.1123\n",
      "Epoch 2749/6000, Training Loss: 12.1074, Validation Loss: 12.1102\n",
      "Epoch 2750/6000, Training Loss: 12.1053, Validation Loss: 12.1082\n",
      "Epoch 2751/6000, Training Loss: 12.1032, Validation Loss: 12.1061\n",
      "Epoch 2752/6000, Training Loss: 12.1012, Validation Loss: 12.1040\n",
      "Epoch 2753/6000, Training Loss: 12.0991, Validation Loss: 12.1020\n",
      "Epoch 2754/6000, Training Loss: 12.0970, Validation Loss: 12.0999\n",
      "Epoch 2755/6000, Training Loss: 12.0950, Validation Loss: 12.0978\n",
      "Epoch 2756/6000, Training Loss: 12.0929, Validation Loss: 12.0958\n",
      "Epoch 2757/6000, Training Loss: 12.0909, Validation Loss: 12.0937\n",
      "Epoch 2758/6000, Training Loss: 12.0888, Validation Loss: 12.0917\n",
      "Epoch 2759/6000, Training Loss: 12.0867, Validation Loss: 12.0896\n",
      "Epoch 2760/6000, Training Loss: 12.0847, Validation Loss: 12.0876\n",
      "Epoch 2761/6000, Training Loss: 12.0827, Validation Loss: 12.0855\n",
      "Epoch 2762/6000, Training Loss: 12.0806, Validation Loss: 12.0835\n",
      "Epoch 2763/6000, Training Loss: 12.0786, Validation Loss: 12.0815\n",
      "Epoch 2764/6000, Training Loss: 12.0766, Validation Loss: 12.0795\n",
      "Epoch 2765/6000, Training Loss: 12.0745, Validation Loss: 12.0774\n",
      "Epoch 2766/6000, Training Loss: 12.0725, Validation Loss: 12.0754\n",
      "Epoch 2767/6000, Training Loss: 12.0705, Validation Loss: 12.0734\n",
      "Epoch 2768/6000, Training Loss: 12.0685, Validation Loss: 12.0714\n",
      "Epoch 2769/6000, Training Loss: 12.0666, Validation Loss: 12.0695\n",
      "Epoch 2770/6000, Training Loss: 12.0646, Validation Loss: 12.0675\n",
      "Epoch 2771/6000, Training Loss: 12.0626, Validation Loss: 12.0655\n",
      "Epoch 2772/6000, Training Loss: 12.0607, Validation Loss: 12.0636\n",
      "Epoch 2773/6000, Training Loss: 12.0587, Validation Loss: 12.0616\n",
      "Epoch 2774/6000, Training Loss: 12.0568, Validation Loss: 12.0597\n",
      "Epoch 2775/6000, Training Loss: 12.0548, Validation Loss: 12.0577\n",
      "Epoch 2776/6000, Training Loss: 12.0529, Validation Loss: 12.0558\n",
      "Epoch 2777/6000, Training Loss: 12.0510, Validation Loss: 12.0539\n",
      "Epoch 2778/6000, Training Loss: 12.0490, Validation Loss: 12.0520\n",
      "Epoch 2779/6000, Training Loss: 12.0471, Validation Loss: 12.0501\n",
      "Epoch 2780/6000, Training Loss: 12.0452, Validation Loss: 12.0482\n",
      "Epoch 2781/6000, Training Loss: 12.0433, Validation Loss: 12.0463\n",
      "Epoch 2782/6000, Training Loss: 12.0414, Validation Loss: 12.0444\n",
      "Epoch 2783/6000, Training Loss: 12.0395, Validation Loss: 12.0425\n",
      "Epoch 2784/6000, Training Loss: 12.0376, Validation Loss: 12.0406\n",
      "Epoch 2785/6000, Training Loss: 12.0357, Validation Loss: 12.0387\n",
      "Epoch 2786/6000, Training Loss: 12.0338, Validation Loss: 12.0369\n",
      "Epoch 2787/6000, Training Loss: 12.0319, Validation Loss: 12.0350\n",
      "Epoch 2788/6000, Training Loss: 12.0300, Validation Loss: 12.0331\n",
      "Epoch 2789/6000, Training Loss: 12.0282, Validation Loss: 12.0312\n",
      "Epoch 2790/6000, Training Loss: 12.0263, Validation Loss: 12.0294\n",
      "Epoch 2791/6000, Training Loss: 12.0244, Validation Loss: 12.0275\n",
      "Epoch 2792/6000, Training Loss: 12.0225, Validation Loss: 12.0257\n",
      "Epoch 2793/6000, Training Loss: 12.0207, Validation Loss: 12.0238\n",
      "Epoch 2794/6000, Training Loss: 12.0188, Validation Loss: 12.0220\n",
      "Epoch 2795/6000, Training Loss: 12.0169, Validation Loss: 12.0201\n",
      "Epoch 2796/6000, Training Loss: 12.0151, Validation Loss: 12.0183\n",
      "Epoch 2797/6000, Training Loss: 12.0132, Validation Loss: 12.0164\n",
      "Epoch 2798/6000, Training Loss: 12.0114, Validation Loss: 12.0146\n",
      "Epoch 2799/6000, Training Loss: 12.0095, Validation Loss: 12.0128\n",
      "Epoch 2800/6000, Training Loss: 12.0077, Validation Loss: 12.0109\n",
      "Epoch 2801/6000, Training Loss: 12.0058, Validation Loss: 12.0091\n",
      "Epoch 2802/6000, Training Loss: 12.0040, Validation Loss: 12.0073\n",
      "Epoch 2803/6000, Training Loss: 12.0022, Validation Loss: 12.0055\n",
      "Epoch 2804/6000, Training Loss: 12.0003, Validation Loss: 12.0036\n",
      "Epoch 2805/6000, Training Loss: 11.9985, Validation Loss: 12.0018\n",
      "Epoch 2806/6000, Training Loss: 11.9967, Validation Loss: 12.0000\n",
      "Epoch 2807/6000, Training Loss: 11.9948, Validation Loss: 11.9982\n",
      "Epoch 2808/6000, Training Loss: 11.9930, Validation Loss: 11.9964\n",
      "Epoch 2809/6000, Training Loss: 11.9912, Validation Loss: 11.9946\n",
      "Epoch 2810/6000, Training Loss: 11.9894, Validation Loss: 11.9928\n",
      "Epoch 2811/6000, Training Loss: 11.9876, Validation Loss: 11.9910\n",
      "Epoch 2812/6000, Training Loss: 11.9858, Validation Loss: 11.9892\n",
      "Epoch 2813/6000, Training Loss: 11.9840, Validation Loss: 11.9874\n",
      "Epoch 2814/6000, Training Loss: 11.9822, Validation Loss: 11.9856\n",
      "Epoch 2815/6000, Training Loss: 11.9804, Validation Loss: 11.9838\n",
      "Epoch 2816/6000, Training Loss: 11.9786, Validation Loss: 11.9820\n",
      "Epoch 2817/6000, Training Loss: 11.9768, Validation Loss: 11.9802\n",
      "Epoch 2818/6000, Training Loss: 11.9750, Validation Loss: 11.9784\n",
      "Epoch 2819/6000, Training Loss: 11.9732, Validation Loss: 11.9767\n",
      "Epoch 2820/6000, Training Loss: 11.9714, Validation Loss: 11.9749\n",
      "Epoch 2821/6000, Training Loss: 11.9696, Validation Loss: 11.9731\n",
      "Epoch 2822/6000, Training Loss: 11.9678, Validation Loss: 11.9713\n",
      "Epoch 2823/6000, Training Loss: 11.9661, Validation Loss: 11.9695\n",
      "Epoch 2824/6000, Training Loss: 11.9643, Validation Loss: 11.9678\n",
      "Epoch 2825/6000, Training Loss: 11.9625, Validation Loss: 11.9660\n",
      "Epoch 2826/6000, Training Loss: 11.9608, Validation Loss: 11.9642\n",
      "Epoch 2827/6000, Training Loss: 11.9590, Validation Loss: 11.9625\n",
      "Epoch 2828/6000, Training Loss: 11.9572, Validation Loss: 11.9607\n",
      "Epoch 2829/6000, Training Loss: 11.9555, Validation Loss: 11.9590\n",
      "Epoch 2830/6000, Training Loss: 11.9537, Validation Loss: 11.9572\n",
      "Epoch 2831/6000, Training Loss: 11.9520, Validation Loss: 11.9555\n",
      "Epoch 2832/6000, Training Loss: 11.9503, Validation Loss: 11.9538\n",
      "Epoch 2833/6000, Training Loss: 11.9485, Validation Loss: 11.9520\n",
      "Epoch 2834/6000, Training Loss: 11.9468, Validation Loss: 11.9503\n",
      "Epoch 2835/6000, Training Loss: 11.9451, Validation Loss: 11.9486\n",
      "Epoch 2836/6000, Training Loss: 11.9434, Validation Loss: 11.9469\n",
      "Epoch 2837/6000, Training Loss: 11.9417, Validation Loss: 11.9453\n",
      "Epoch 2838/6000, Training Loss: 11.9400, Validation Loss: 11.9436\n",
      "Epoch 2839/6000, Training Loss: 11.9383, Validation Loss: 11.9419\n",
      "Epoch 2840/6000, Training Loss: 11.9366, Validation Loss: 11.9402\n",
      "Epoch 2841/6000, Training Loss: 11.9350, Validation Loss: 11.9386\n",
      "Epoch 2842/6000, Training Loss: 11.9333, Validation Loss: 11.9369\n",
      "Epoch 2843/6000, Training Loss: 11.9317, Validation Loss: 11.9353\n",
      "Epoch 2844/6000, Training Loss: 11.9300, Validation Loss: 11.9337\n",
      "Epoch 2845/6000, Training Loss: 11.9283, Validation Loss: 11.9320\n",
      "Epoch 2846/6000, Training Loss: 11.9267, Validation Loss: 11.9304\n",
      "Epoch 2847/6000, Training Loss: 11.9251, Validation Loss: 11.9288\n",
      "Epoch 2848/6000, Training Loss: 11.9234, Validation Loss: 11.9272\n",
      "Epoch 2849/6000, Training Loss: 11.9218, Validation Loss: 11.9255\n",
      "Epoch 2850/6000, Training Loss: 11.9202, Validation Loss: 11.9239\n",
      "Epoch 2851/6000, Training Loss: 11.9185, Validation Loss: 11.9223\n",
      "Epoch 2852/6000, Training Loss: 11.9169, Validation Loss: 11.9207\n",
      "Epoch 2853/6000, Training Loss: 11.9153, Validation Loss: 11.9191\n",
      "Epoch 2854/6000, Training Loss: 11.9137, Validation Loss: 11.9175\n",
      "Epoch 2855/6000, Training Loss: 11.9121, Validation Loss: 11.9159\n",
      "Epoch 2856/6000, Training Loss: 11.9104, Validation Loss: 11.9143\n",
      "Epoch 2857/6000, Training Loss: 11.9088, Validation Loss: 11.9128\n",
      "Epoch 2858/6000, Training Loss: 11.9072, Validation Loss: 11.9112\n",
      "Epoch 2859/6000, Training Loss: 11.9056, Validation Loss: 11.9096\n",
      "Epoch 2860/6000, Training Loss: 11.9040, Validation Loss: 11.9080\n",
      "Epoch 2861/6000, Training Loss: 11.9024, Validation Loss: 11.9064\n",
      "Epoch 2862/6000, Training Loss: 11.9008, Validation Loss: 11.9049\n",
      "Epoch 2863/6000, Training Loss: 11.8992, Validation Loss: 11.9033\n",
      "Epoch 2864/6000, Training Loss: 11.8976, Validation Loss: 11.9017\n",
      "Epoch 2865/6000, Training Loss: 11.8961, Validation Loss: 11.9002\n",
      "Epoch 2866/6000, Training Loss: 11.8945, Validation Loss: 11.8986\n",
      "Epoch 2867/6000, Training Loss: 11.8929, Validation Loss: 11.8970\n",
      "Epoch 2868/6000, Training Loss: 11.8913, Validation Loss: 11.8955\n",
      "Epoch 2869/6000, Training Loss: 11.8897, Validation Loss: 11.8939\n",
      "Epoch 2870/6000, Training Loss: 11.8882, Validation Loss: 11.8924\n",
      "Epoch 2871/6000, Training Loss: 11.8866, Validation Loss: 11.8908\n",
      "Epoch 2872/6000, Training Loss: 11.8850, Validation Loss: 11.8893\n",
      "Epoch 2873/6000, Training Loss: 11.8834, Validation Loss: 11.8877\n",
      "Epoch 2874/6000, Training Loss: 11.8819, Validation Loss: 11.8862\n",
      "Epoch 2875/6000, Training Loss: 11.8803, Validation Loss: 11.8846\n",
      "Epoch 2876/6000, Training Loss: 11.8788, Validation Loss: 11.8831\n",
      "Epoch 2877/6000, Training Loss: 11.8772, Validation Loss: 11.8816\n",
      "Epoch 2878/6000, Training Loss: 11.8756, Validation Loss: 11.8800\n",
      "Epoch 2879/6000, Training Loss: 11.8741, Validation Loss: 11.8785\n",
      "Epoch 2880/6000, Training Loss: 11.8726, Validation Loss: 11.8770\n",
      "Epoch 2881/6000, Training Loss: 11.8710, Validation Loss: 11.8754\n",
      "Epoch 2882/6000, Training Loss: 11.8695, Validation Loss: 11.8739\n",
      "Epoch 2883/6000, Training Loss: 11.8679, Validation Loss: 11.8724\n",
      "Epoch 2884/6000, Training Loss: 11.8664, Validation Loss: 11.8709\n",
      "Epoch 2885/6000, Training Loss: 11.8649, Validation Loss: 11.8693\n",
      "Epoch 2886/6000, Training Loss: 11.8633, Validation Loss: 11.8678\n",
      "Epoch 2887/6000, Training Loss: 11.8618, Validation Loss: 11.8663\n",
      "Epoch 2888/6000, Training Loss: 11.8603, Validation Loss: 11.8648\n",
      "Epoch 2889/6000, Training Loss: 11.8587, Validation Loss: 11.8633\n",
      "Epoch 2890/6000, Training Loss: 11.8572, Validation Loss: 11.8618\n",
      "Epoch 2891/6000, Training Loss: 11.8557, Validation Loss: 11.8602\n",
      "Epoch 2892/6000, Training Loss: 11.8542, Validation Loss: 11.8587\n",
      "Epoch 2893/6000, Training Loss: 11.8527, Validation Loss: 11.8572\n",
      "Epoch 2894/6000, Training Loss: 11.8512, Validation Loss: 11.8557\n",
      "Epoch 2895/6000, Training Loss: 11.8496, Validation Loss: 11.8542\n",
      "Epoch 2896/6000, Training Loss: 11.8481, Validation Loss: 11.8527\n",
      "Epoch 2897/6000, Training Loss: 11.8466, Validation Loss: 11.8512\n",
      "Epoch 2898/6000, Training Loss: 11.8451, Validation Loss: 11.8497\n",
      "Epoch 2899/6000, Training Loss: 11.8436, Validation Loss: 11.8482\n",
      "Epoch 2900/6000, Training Loss: 11.8421, Validation Loss: 11.8468\n",
      "Epoch 2901/6000, Training Loss: 11.8407, Validation Loss: 11.8453\n",
      "Epoch 2902/6000, Training Loss: 11.8392, Validation Loss: 11.8438\n",
      "Epoch 2903/6000, Training Loss: 11.8377, Validation Loss: 11.8424\n",
      "Epoch 2904/6000, Training Loss: 11.8362, Validation Loss: 11.8409\n",
      "Epoch 2905/6000, Training Loss: 11.8348, Validation Loss: 11.8395\n",
      "Epoch 2906/6000, Training Loss: 11.8333, Validation Loss: 11.8380\n",
      "Epoch 2907/6000, Training Loss: 11.8319, Validation Loss: 11.8366\n",
      "Epoch 2908/6000, Training Loss: 11.8304, Validation Loss: 11.8352\n",
      "Epoch 2909/6000, Training Loss: 11.8290, Validation Loss: 11.8338\n",
      "Epoch 2910/6000, Training Loss: 11.8276, Validation Loss: 11.8324\n",
      "Epoch 2911/6000, Training Loss: 11.8262, Validation Loss: 11.8310\n",
      "Epoch 2912/6000, Training Loss: 11.8247, Validation Loss: 11.8296\n",
      "Epoch 2913/6000, Training Loss: 11.8233, Validation Loss: 11.8282\n",
      "Epoch 2914/6000, Training Loss: 11.8219, Validation Loss: 11.8268\n",
      "Epoch 2915/6000, Training Loss: 11.8205, Validation Loss: 11.8254\n",
      "Epoch 2916/6000, Training Loss: 11.8191, Validation Loss: 11.8240\n",
      "Epoch 2917/6000, Training Loss: 11.8177, Validation Loss: 11.8227\n",
      "Epoch 2918/6000, Training Loss: 11.8163, Validation Loss: 11.8213\n",
      "Epoch 2919/6000, Training Loss: 11.8150, Validation Loss: 11.8200\n",
      "Epoch 2920/6000, Training Loss: 11.8136, Validation Loss: 11.8186\n",
      "Epoch 2921/6000, Training Loss: 11.8122, Validation Loss: 11.8172\n",
      "Epoch 2922/6000, Training Loss: 11.8108, Validation Loss: 11.8159\n",
      "Epoch 2923/6000, Training Loss: 11.8094, Validation Loss: 11.8145\n",
      "Epoch 2924/6000, Training Loss: 11.8081, Validation Loss: 11.8132\n",
      "Epoch 2925/6000, Training Loss: 11.8067, Validation Loss: 11.8119\n",
      "Epoch 2926/6000, Training Loss: 11.8053, Validation Loss: 11.8105\n",
      "Epoch 2927/6000, Training Loss: 11.8040, Validation Loss: 11.8092\n",
      "Epoch 2928/6000, Training Loss: 11.8026, Validation Loss: 11.8078\n",
      "Epoch 2929/6000, Training Loss: 11.8013, Validation Loss: 11.8065\n",
      "Epoch 2930/6000, Training Loss: 11.7999, Validation Loss: 11.8052\n",
      "Epoch 2931/6000, Training Loss: 11.7985, Validation Loss: 11.8039\n",
      "Epoch 2932/6000, Training Loss: 11.7972, Validation Loss: 11.8025\n",
      "Epoch 2933/6000, Training Loss: 11.7958, Validation Loss: 11.8012\n",
      "Epoch 2934/6000, Training Loss: 11.7945, Validation Loss: 11.7999\n",
      "Epoch 2935/6000, Training Loss: 11.7932, Validation Loss: 11.7986\n",
      "Epoch 2936/6000, Training Loss: 11.7918, Validation Loss: 11.7972\n",
      "Epoch 2937/6000, Training Loss: 11.7905, Validation Loss: 11.7959\n",
      "Epoch 2938/6000, Training Loss: 11.7891, Validation Loss: 11.7946\n",
      "Epoch 2939/6000, Training Loss: 11.7878, Validation Loss: 11.7933\n",
      "Epoch 2940/6000, Training Loss: 11.7865, Validation Loss: 11.7920\n",
      "Epoch 2941/6000, Training Loss: 11.7851, Validation Loss: 11.7907\n",
      "Epoch 2942/6000, Training Loss: 11.7838, Validation Loss: 11.7894\n",
      "Epoch 2943/6000, Training Loss: 11.7825, Validation Loss: 11.7881\n",
      "Epoch 2944/6000, Training Loss: 11.7811, Validation Loss: 11.7868\n",
      "Epoch 2945/6000, Training Loss: 11.7798, Validation Loss: 11.7855\n",
      "Epoch 2946/6000, Training Loss: 11.7785, Validation Loss: 11.7842\n",
      "Epoch 2947/6000, Training Loss: 11.7772, Validation Loss: 11.7829\n",
      "Epoch 2948/6000, Training Loss: 11.7759, Validation Loss: 11.7816\n",
      "Epoch 2949/6000, Training Loss: 11.7746, Validation Loss: 11.7803\n",
      "Epoch 2950/6000, Training Loss: 11.7732, Validation Loss: 11.7790\n",
      "Epoch 2951/6000, Training Loss: 11.7719, Validation Loss: 11.7777\n",
      "Epoch 2952/6000, Training Loss: 11.7706, Validation Loss: 11.7764\n",
      "Epoch 2953/6000, Training Loss: 11.7693, Validation Loss: 11.7752\n",
      "Epoch 2954/6000, Training Loss: 11.7680, Validation Loss: 11.7739\n",
      "Epoch 2955/6000, Training Loss: 11.7667, Validation Loss: 11.7726\n",
      "Epoch 2956/6000, Training Loss: 11.7654, Validation Loss: 11.7713\n",
      "Epoch 2957/6000, Training Loss: 11.7641, Validation Loss: 11.7701\n",
      "Epoch 2958/6000, Training Loss: 11.7628, Validation Loss: 11.7688\n",
      "Epoch 2959/6000, Training Loss: 11.7616, Validation Loss: 11.7675\n",
      "Epoch 2960/6000, Training Loss: 11.7603, Validation Loss: 11.7662\n",
      "Epoch 2961/6000, Training Loss: 11.7590, Validation Loss: 11.7650\n",
      "Epoch 2962/6000, Training Loss: 11.7577, Validation Loss: 11.7637\n",
      "Epoch 2963/6000, Training Loss: 11.7564, Validation Loss: 11.7624\n",
      "Epoch 2964/6000, Training Loss: 11.7551, Validation Loss: 11.7612\n",
      "Epoch 2965/6000, Training Loss: 11.7538, Validation Loss: 11.7599\n",
      "Epoch 2966/6000, Training Loss: 11.7526, Validation Loss: 11.7586\n",
      "Epoch 2967/6000, Training Loss: 11.7513, Validation Loss: 11.7574\n",
      "Epoch 2968/6000, Training Loss: 11.7500, Validation Loss: 11.7561\n",
      "Epoch 2969/6000, Training Loss: 11.7487, Validation Loss: 11.7549\n",
      "Epoch 2970/6000, Training Loss: 11.7475, Validation Loss: 11.7536\n",
      "Epoch 2971/6000, Training Loss: 11.7462, Validation Loss: 11.7524\n",
      "Epoch 2972/6000, Training Loss: 11.7450, Validation Loss: 11.7511\n",
      "Epoch 2973/6000, Training Loss: 11.7437, Validation Loss: 11.7499\n",
      "Epoch 2974/6000, Training Loss: 11.7424, Validation Loss: 11.7487\n",
      "Epoch 2975/6000, Training Loss: 11.7412, Validation Loss: 11.7474\n",
      "Epoch 2976/6000, Training Loss: 11.7400, Validation Loss: 11.7462\n",
      "Epoch 2977/6000, Training Loss: 11.7387, Validation Loss: 11.7450\n",
      "Epoch 2978/6000, Training Loss: 11.7375, Validation Loss: 11.7438\n",
      "Epoch 2979/6000, Training Loss: 11.7363, Validation Loss: 11.7426\n",
      "Epoch 2980/6000, Training Loss: 11.7350, Validation Loss: 11.7414\n",
      "Epoch 2981/6000, Training Loss: 11.7338, Validation Loss: 11.7402\n",
      "Epoch 2982/6000, Training Loss: 11.7326, Validation Loss: 11.7390\n",
      "Epoch 2983/6000, Training Loss: 11.7314, Validation Loss: 11.7378\n",
      "Epoch 2984/6000, Training Loss: 11.7302, Validation Loss: 11.7366\n",
      "Epoch 2985/6000, Training Loss: 11.7290, Validation Loss: 11.7355\n",
      "Epoch 2986/6000, Training Loss: 11.7278, Validation Loss: 11.7343\n",
      "Epoch 2987/6000, Training Loss: 11.7267, Validation Loss: 11.7331\n",
      "Epoch 2988/6000, Training Loss: 11.7255, Validation Loss: 11.7320\n",
      "Epoch 2989/6000, Training Loss: 11.7243, Validation Loss: 11.7308\n",
      "Epoch 2990/6000, Training Loss: 11.7231, Validation Loss: 11.7297\n",
      "Epoch 2991/6000, Training Loss: 11.7220, Validation Loss: 11.7286\n",
      "Epoch 2992/6000, Training Loss: 11.7208, Validation Loss: 11.7274\n",
      "Epoch 2993/6000, Training Loss: 11.7197, Validation Loss: 11.7263\n",
      "Epoch 2994/6000, Training Loss: 11.7185, Validation Loss: 11.7252\n",
      "Epoch 2995/6000, Training Loss: 11.7174, Validation Loss: 11.7241\n",
      "Epoch 2996/6000, Training Loss: 11.7162, Validation Loss: 11.7229\n",
      "Epoch 2997/6000, Training Loss: 11.7151, Validation Loss: 11.7218\n",
      "Epoch 2998/6000, Training Loss: 11.7140, Validation Loss: 11.7207\n",
      "Epoch 2999/6000, Training Loss: 11.7128, Validation Loss: 11.7196\n",
      "Epoch 3000/6000, Training Loss: 11.7117, Validation Loss: 11.7185\n",
      "Epoch 3001/6000, Training Loss: 11.7106, Validation Loss: 11.7174\n",
      "Epoch 3002/6000, Training Loss: 11.7094, Validation Loss: 11.7163\n",
      "Epoch 3003/6000, Training Loss: 11.7083, Validation Loss: 11.7152\n",
      "Epoch 3004/6000, Training Loss: 11.7072, Validation Loss: 11.7141\n",
      "Epoch 3005/6000, Training Loss: 11.7061, Validation Loss: 11.7130\n",
      "Epoch 3006/6000, Training Loss: 11.7050, Validation Loss: 11.7119\n",
      "Epoch 3007/6000, Training Loss: 11.7039, Validation Loss: 11.7108\n",
      "Epoch 3008/6000, Training Loss: 11.7027, Validation Loss: 11.7097\n",
      "Epoch 3009/6000, Training Loss: 11.7016, Validation Loss: 11.7086\n",
      "Epoch 3010/6000, Training Loss: 11.7005, Validation Loss: 11.7075\n",
      "Epoch 3011/6000, Training Loss: 11.6994, Validation Loss: 11.7065\n",
      "Epoch 3012/6000, Training Loss: 11.6983, Validation Loss: 11.7054\n",
      "Epoch 3013/6000, Training Loss: 11.6972, Validation Loss: 11.7043\n",
      "Epoch 3014/6000, Training Loss: 11.6961, Validation Loss: 11.7032\n",
      "Epoch 3015/6000, Training Loss: 11.6950, Validation Loss: 11.7021\n",
      "Epoch 3016/6000, Training Loss: 11.6939, Validation Loss: 11.7011\n",
      "Epoch 3017/6000, Training Loss: 11.6928, Validation Loss: 11.7000\n",
      "Epoch 3018/6000, Training Loss: 11.6917, Validation Loss: 11.6989\n",
      "Epoch 3019/6000, Training Loss: 11.6906, Validation Loss: 11.6978\n",
      "Epoch 3020/6000, Training Loss: 11.6895, Validation Loss: 11.6968\n",
      "Epoch 3021/6000, Training Loss: 11.6884, Validation Loss: 11.6957\n",
      "Epoch 3022/6000, Training Loss: 11.6873, Validation Loss: 11.6946\n",
      "Epoch 3023/6000, Training Loss: 11.6862, Validation Loss: 11.6936\n",
      "Epoch 3024/6000, Training Loss: 11.6852, Validation Loss: 11.6925\n",
      "Epoch 3025/6000, Training Loss: 11.6841, Validation Loss: 11.6915\n",
      "Epoch 3026/6000, Training Loss: 11.6830, Validation Loss: 11.6904\n",
      "Epoch 3027/6000, Training Loss: 11.6819, Validation Loss: 11.6893\n",
      "Epoch 3028/6000, Training Loss: 11.6808, Validation Loss: 11.6883\n",
      "Epoch 3029/6000, Training Loss: 11.6798, Validation Loss: 11.6872\n",
      "Epoch 3030/6000, Training Loss: 11.6787, Validation Loss: 11.6862\n",
      "Epoch 3031/6000, Training Loss: 11.6776, Validation Loss: 11.6852\n",
      "Epoch 3032/6000, Training Loss: 11.6766, Validation Loss: 11.6841\n",
      "Epoch 3033/6000, Training Loss: 11.6755, Validation Loss: 11.6831\n",
      "Epoch 3034/6000, Training Loss: 11.6744, Validation Loss: 11.6820\n",
      "Epoch 3035/6000, Training Loss: 11.6734, Validation Loss: 11.6810\n",
      "Epoch 3036/6000, Training Loss: 11.6723, Validation Loss: 11.6799\n",
      "Epoch 3037/6000, Training Loss: 11.6713, Validation Loss: 11.6789\n",
      "Epoch 3038/6000, Training Loss: 11.6702, Validation Loss: 11.6779\n",
      "Epoch 3039/6000, Training Loss: 11.6691, Validation Loss: 11.6768\n",
      "Epoch 3040/6000, Training Loss: 11.6681, Validation Loss: 11.6758\n",
      "Epoch 3041/6000, Training Loss: 11.6670, Validation Loss: 11.6748\n",
      "Epoch 3042/6000, Training Loss: 11.6660, Validation Loss: 11.6737\n",
      "Epoch 3043/6000, Training Loss: 11.6649, Validation Loss: 11.6727\n",
      "Epoch 3044/6000, Training Loss: 11.6639, Validation Loss: 11.6717\n",
      "Epoch 3045/6000, Training Loss: 11.6629, Validation Loss: 11.6707\n",
      "Epoch 3046/6000, Training Loss: 11.6618, Validation Loss: 11.6696\n",
      "Epoch 3047/6000, Training Loss: 11.6608, Validation Loss: 11.6686\n",
      "Epoch 3048/6000, Training Loss: 11.6597, Validation Loss: 11.6676\n",
      "Epoch 3049/6000, Training Loss: 11.6587, Validation Loss: 11.6666\n",
      "Epoch 3050/6000, Training Loss: 11.6577, Validation Loss: 11.6655\n",
      "Epoch 3051/6000, Training Loss: 11.6566, Validation Loss: 11.6645\n",
      "Epoch 3052/6000, Training Loss: 11.6556, Validation Loss: 11.6635\n",
      "Epoch 3053/6000, Training Loss: 11.6546, Validation Loss: 11.6625\n",
      "Epoch 3054/6000, Training Loss: 11.6535, Validation Loss: 11.6615\n",
      "Epoch 3055/6000, Training Loss: 11.6525, Validation Loss: 11.6605\n",
      "Epoch 3056/6000, Training Loss: 11.6515, Validation Loss: 11.6595\n",
      "Epoch 3057/6000, Training Loss: 11.6505, Validation Loss: 11.6585\n",
      "Epoch 3058/6000, Training Loss: 11.6495, Validation Loss: 11.6575\n",
      "Epoch 3059/6000, Training Loss: 11.6485, Validation Loss: 11.6565\n",
      "Epoch 3060/6000, Training Loss: 11.6475, Validation Loss: 11.6555\n",
      "Epoch 3061/6000, Training Loss: 11.6465, Validation Loss: 11.6545\n",
      "Epoch 3062/6000, Training Loss: 11.6455, Validation Loss: 11.6536\n",
      "Epoch 3063/6000, Training Loss: 11.6445, Validation Loss: 11.6526\n",
      "Epoch 3064/6000, Training Loss: 11.6435, Validation Loss: 11.6516\n",
      "Epoch 3065/6000, Training Loss: 11.6426, Validation Loss: 11.6507\n",
      "Epoch 3066/6000, Training Loss: 11.6416, Validation Loss: 11.6497\n",
      "Epoch 3067/6000, Training Loss: 11.6406, Validation Loss: 11.6488\n",
      "Epoch 3068/6000, Training Loss: 11.6397, Validation Loss: 11.6479\n",
      "Epoch 3069/6000, Training Loss: 11.6387, Validation Loss: 11.6469\n",
      "Epoch 3070/6000, Training Loss: 11.6377, Validation Loss: 11.6460\n",
      "Epoch 3071/6000, Training Loss: 11.6368, Validation Loss: 11.6451\n",
      "Epoch 3072/6000, Training Loss: 11.6359, Validation Loss: 11.6442\n",
      "Epoch 3073/6000, Training Loss: 11.6349, Validation Loss: 11.6432\n",
      "Epoch 3074/6000, Training Loss: 11.6340, Validation Loss: 11.6423\n",
      "Epoch 3075/6000, Training Loss: 11.6330, Validation Loss: 11.6414\n",
      "Epoch 3076/6000, Training Loss: 11.6321, Validation Loss: 11.6405\n",
      "Epoch 3077/6000, Training Loss: 11.6312, Validation Loss: 11.6396\n",
      "Epoch 3078/6000, Training Loss: 11.6303, Validation Loss: 11.6387\n",
      "Epoch 3079/6000, Training Loss: 11.6293, Validation Loss: 11.6378\n",
      "Epoch 3080/6000, Training Loss: 11.6284, Validation Loss: 11.6369\n",
      "Epoch 3081/6000, Training Loss: 11.6275, Validation Loss: 11.6360\n",
      "Epoch 3082/6000, Training Loss: 11.6266, Validation Loss: 11.6351\n",
      "Epoch 3083/6000, Training Loss: 11.6257, Validation Loss: 11.6343\n",
      "Epoch 3084/6000, Training Loss: 11.6248, Validation Loss: 11.6334\n",
      "Epoch 3085/6000, Training Loss: 11.6239, Validation Loss: 11.6325\n",
      "Epoch 3086/6000, Training Loss: 11.6230, Validation Loss: 11.6316\n",
      "Epoch 3087/6000, Training Loss: 11.6221, Validation Loss: 11.6307\n",
      "Epoch 3088/6000, Training Loss: 11.6212, Validation Loss: 11.6298\n",
      "Epoch 3089/6000, Training Loss: 11.6203, Validation Loss: 11.6290\n",
      "Epoch 3090/6000, Training Loss: 11.6194, Validation Loss: 11.6281\n",
      "Epoch 3091/6000, Training Loss: 11.6185, Validation Loss: 11.6272\n",
      "Epoch 3092/6000, Training Loss: 11.6176, Validation Loss: 11.6264\n",
      "Epoch 3093/6000, Training Loss: 11.6167, Validation Loss: 11.6255\n",
      "Epoch 3094/6000, Training Loss: 11.6158, Validation Loss: 11.6246\n",
      "Epoch 3095/6000, Training Loss: 11.6149, Validation Loss: 11.6237\n",
      "Epoch 3096/6000, Training Loss: 11.6140, Validation Loss: 11.6229\n",
      "Epoch 3097/6000, Training Loss: 11.6131, Validation Loss: 11.6220\n",
      "Epoch 3098/6000, Training Loss: 11.6122, Validation Loss: 11.6212\n",
      "Epoch 3099/6000, Training Loss: 11.6113, Validation Loss: 11.6203\n",
      "Epoch 3100/6000, Training Loss: 11.6104, Validation Loss: 11.6194\n",
      "Epoch 3101/6000, Training Loss: 11.6096, Validation Loss: 11.6186\n",
      "Epoch 3102/6000, Training Loss: 11.6087, Validation Loss: 11.6177\n",
      "Epoch 3103/6000, Training Loss: 11.6078, Validation Loss: 11.6169\n",
      "Epoch 3104/6000, Training Loss: 11.6069, Validation Loss: 11.6160\n",
      "Epoch 3105/6000, Training Loss: 11.6060, Validation Loss: 11.6152\n",
      "Epoch 3106/6000, Training Loss: 11.6052, Validation Loss: 11.6143\n",
      "Epoch 3107/6000, Training Loss: 11.6043, Validation Loss: 11.6135\n",
      "Epoch 3108/6000, Training Loss: 11.6034, Validation Loss: 11.6126\n",
      "Epoch 3109/6000, Training Loss: 11.6026, Validation Loss: 11.6118\n",
      "Epoch 3110/6000, Training Loss: 11.6017, Validation Loss: 11.6109\n",
      "Epoch 3111/6000, Training Loss: 11.6008, Validation Loss: 11.6101\n",
      "Epoch 3112/6000, Training Loss: 11.6000, Validation Loss: 11.6092\n",
      "Epoch 3113/6000, Training Loss: 11.5991, Validation Loss: 11.6084\n",
      "Epoch 3114/6000, Training Loss: 11.5982, Validation Loss: 11.6076\n",
      "Epoch 3115/6000, Training Loss: 11.5974, Validation Loss: 11.6067\n",
      "Epoch 3116/6000, Training Loss: 11.5965, Validation Loss: 11.6059\n",
      "Epoch 3117/6000, Training Loss: 11.5957, Validation Loss: 11.6050\n",
      "Epoch 3118/6000, Training Loss: 11.5948, Validation Loss: 11.6042\n",
      "Epoch 3119/6000, Training Loss: 11.5939, Validation Loss: 11.6034\n",
      "Epoch 3120/6000, Training Loss: 11.5931, Validation Loss: 11.6026\n",
      "Epoch 3121/6000, Training Loss: 11.5922, Validation Loss: 11.6017\n",
      "Epoch 3122/6000, Training Loss: 11.5914, Validation Loss: 11.6009\n",
      "Epoch 3123/6000, Training Loss: 11.5905, Validation Loss: 11.6001\n",
      "Epoch 3124/6000, Training Loss: 11.5897, Validation Loss: 11.5993\n",
      "Epoch 3125/6000, Training Loss: 11.5889, Validation Loss: 11.5984\n",
      "Epoch 3126/6000, Training Loss: 11.5880, Validation Loss: 11.5976\n",
      "Epoch 3127/6000, Training Loss: 11.5872, Validation Loss: 11.5968\n",
      "Epoch 3128/6000, Training Loss: 11.5863, Validation Loss: 11.5960\n",
      "Epoch 3129/6000, Training Loss: 11.5855, Validation Loss: 11.5952\n",
      "Epoch 3130/6000, Training Loss: 11.5846, Validation Loss: 11.5943\n",
      "Epoch 3131/6000, Training Loss: 11.5838, Validation Loss: 11.5935\n",
      "Epoch 3132/6000, Training Loss: 11.5830, Validation Loss: 11.5927\n",
      "Epoch 3133/6000, Training Loss: 11.5821, Validation Loss: 11.5919\n",
      "Epoch 3134/6000, Training Loss: 11.5813, Validation Loss: 11.5911\n",
      "Epoch 3135/6000, Training Loss: 11.5805, Validation Loss: 11.5903\n",
      "Epoch 3136/6000, Training Loss: 11.5796, Validation Loss: 11.5895\n",
      "Epoch 3137/6000, Training Loss: 11.5788, Validation Loss: 11.5886\n",
      "Epoch 3138/6000, Training Loss: 11.5780, Validation Loss: 11.5878\n",
      "Epoch 3139/6000, Training Loss: 11.5772, Validation Loss: 11.5870\n",
      "Epoch 3140/6000, Training Loss: 11.5763, Validation Loss: 11.5862\n",
      "Epoch 3141/6000, Training Loss: 11.5755, Validation Loss: 11.5854\n",
      "Epoch 3142/6000, Training Loss: 11.5747, Validation Loss: 11.5846\n",
      "Epoch 3143/6000, Training Loss: 11.5739, Validation Loss: 11.5838\n",
      "Epoch 3144/6000, Training Loss: 11.5731, Validation Loss: 11.5830\n",
      "Epoch 3145/6000, Training Loss: 11.5723, Validation Loss: 11.5823\n",
      "Epoch 3146/6000, Training Loss: 11.5715, Validation Loss: 11.5815\n",
      "Epoch 3147/6000, Training Loss: 11.5707, Validation Loss: 11.5807\n",
      "Epoch 3148/6000, Training Loss: 11.5699, Validation Loss: 11.5799\n",
      "Epoch 3149/6000, Training Loss: 11.5691, Validation Loss: 11.5791\n",
      "Epoch 3150/6000, Training Loss: 11.5683, Validation Loss: 11.5784\n",
      "Epoch 3151/6000, Training Loss: 11.5675, Validation Loss: 11.5776\n",
      "Epoch 3152/6000, Training Loss: 11.5667, Validation Loss: 11.5768\n",
      "Epoch 3153/6000, Training Loss: 11.5659, Validation Loss: 11.5761\n",
      "Epoch 3154/6000, Training Loss: 11.5651, Validation Loss: 11.5753\n",
      "Epoch 3155/6000, Training Loss: 11.5644, Validation Loss: 11.5746\n",
      "Epoch 3156/6000, Training Loss: 11.5636, Validation Loss: 11.5738\n",
      "Epoch 3157/6000, Training Loss: 11.5628, Validation Loss: 11.5731\n",
      "Epoch 3158/6000, Training Loss: 11.5621, Validation Loss: 11.5723\n",
      "Epoch 3159/6000, Training Loss: 11.5613, Validation Loss: 11.5716\n",
      "Epoch 3160/6000, Training Loss: 11.5606, Validation Loss: 11.5709\n",
      "Epoch 3161/6000, Training Loss: 11.5598, Validation Loss: 11.5702\n",
      "Epoch 3162/6000, Training Loss: 11.5591, Validation Loss: 11.5694\n",
      "Epoch 3163/6000, Training Loss: 11.5583, Validation Loss: 11.5687\n",
      "Epoch 3164/6000, Training Loss: 11.5576, Validation Loss: 11.5680\n",
      "Epoch 3165/6000, Training Loss: 11.5569, Validation Loss: 11.5673\n",
      "Epoch 3166/6000, Training Loss: 11.5561, Validation Loss: 11.5666\n",
      "Epoch 3167/6000, Training Loss: 11.5554, Validation Loss: 11.5659\n",
      "Epoch 3168/6000, Training Loss: 11.5547, Validation Loss: 11.5652\n",
      "Epoch 3169/6000, Training Loss: 11.5540, Validation Loss: 11.5645\n",
      "Epoch 3170/6000, Training Loss: 11.5532, Validation Loss: 11.5638\n",
      "Epoch 3171/6000, Training Loss: 11.5525, Validation Loss: 11.5631\n",
      "Epoch 3172/6000, Training Loss: 11.5518, Validation Loss: 11.5624\n",
      "Epoch 3173/6000, Training Loss: 11.5511, Validation Loss: 11.5617\n",
      "Epoch 3174/6000, Training Loss: 11.5504, Validation Loss: 11.5610\n",
      "Epoch 3175/6000, Training Loss: 11.5497, Validation Loss: 11.5603\n",
      "Epoch 3176/6000, Training Loss: 11.5490, Validation Loss: 11.5596\n",
      "Epoch 3177/6000, Training Loss: 11.5483, Validation Loss: 11.5589\n",
      "Epoch 3178/6000, Training Loss: 11.5476, Validation Loss: 11.5582\n",
      "Epoch 3179/6000, Training Loss: 11.5468, Validation Loss: 11.5575\n",
      "Epoch 3180/6000, Training Loss: 11.5461, Validation Loss: 11.5569\n",
      "Epoch 3181/6000, Training Loss: 11.5454, Validation Loss: 11.5562\n",
      "Epoch 3182/6000, Training Loss: 11.5447, Validation Loss: 11.5555\n",
      "Epoch 3183/6000, Training Loss: 11.5440, Validation Loss: 11.5548\n",
      "Epoch 3184/6000, Training Loss: 11.5433, Validation Loss: 11.5541\n",
      "Epoch 3185/6000, Training Loss: 11.5426, Validation Loss: 11.5534\n",
      "Epoch 3186/6000, Training Loss: 11.5419, Validation Loss: 11.5528\n",
      "Epoch 3187/6000, Training Loss: 11.5413, Validation Loss: 11.5521\n",
      "Epoch 3188/6000, Training Loss: 11.5406, Validation Loss: 11.5514\n",
      "Epoch 3189/6000, Training Loss: 11.5399, Validation Loss: 11.5507\n",
      "Epoch 3190/6000, Training Loss: 11.5392, Validation Loss: 11.5501\n",
      "Epoch 3191/6000, Training Loss: 11.5385, Validation Loss: 11.5494\n",
      "Epoch 3192/6000, Training Loss: 11.5378, Validation Loss: 11.5487\n",
      "Epoch 3193/6000, Training Loss: 11.5371, Validation Loss: 11.5481\n",
      "Epoch 3194/6000, Training Loss: 11.5364, Validation Loss: 11.5474\n",
      "Epoch 3195/6000, Training Loss: 11.5357, Validation Loss: 11.5467\n",
      "Epoch 3196/6000, Training Loss: 11.5351, Validation Loss: 11.5461\n",
      "Epoch 3197/6000, Training Loss: 11.5344, Validation Loss: 11.5454\n",
      "Epoch 3198/6000, Training Loss: 11.5337, Validation Loss: 11.5448\n",
      "Epoch 3199/6000, Training Loss: 11.5330, Validation Loss: 11.5441\n",
      "Epoch 3200/6000, Training Loss: 11.5323, Validation Loss: 11.5434\n",
      "Epoch 3201/6000, Training Loss: 11.5317, Validation Loss: 11.5428\n",
      "Epoch 3202/6000, Training Loss: 11.5310, Validation Loss: 11.5421\n",
      "Epoch 3203/6000, Training Loss: 11.5303, Validation Loss: 11.5415\n",
      "Epoch 3204/6000, Training Loss: 11.5296, Validation Loss: 11.5408\n",
      "Epoch 3205/6000, Training Loss: 11.5290, Validation Loss: 11.5402\n",
      "Epoch 3206/6000, Training Loss: 11.5283, Validation Loss: 11.5395\n",
      "Epoch 3207/6000, Training Loss: 11.5276, Validation Loss: 11.5389\n",
      "Epoch 3208/6000, Training Loss: 11.5270, Validation Loss: 11.5382\n",
      "Epoch 3209/6000, Training Loss: 11.5263, Validation Loss: 11.5376\n",
      "Epoch 3210/6000, Training Loss: 11.5256, Validation Loss: 11.5369\n",
      "Epoch 3211/6000, Training Loss: 11.5250, Validation Loss: 11.5363\n",
      "Epoch 3212/6000, Training Loss: 11.5243, Validation Loss: 11.5356\n",
      "Epoch 3213/6000, Training Loss: 11.5237, Validation Loss: 11.5350\n",
      "Epoch 3214/6000, Training Loss: 11.5230, Validation Loss: 11.5343\n",
      "Epoch 3215/6000, Training Loss: 11.5224, Validation Loss: 11.5337\n",
      "Epoch 3216/6000, Training Loss: 11.5217, Validation Loss: 11.5331\n",
      "Epoch 3217/6000, Training Loss: 11.5210, Validation Loss: 11.5324\n",
      "Epoch 3218/6000, Training Loss: 11.5204, Validation Loss: 11.5318\n",
      "Epoch 3219/6000, Training Loss: 11.5197, Validation Loss: 11.5312\n",
      "Epoch 3220/6000, Training Loss: 11.5191, Validation Loss: 11.5305\n",
      "Epoch 3221/6000, Training Loss: 11.5184, Validation Loss: 11.5299\n",
      "Epoch 3222/6000, Training Loss: 11.5178, Validation Loss: 11.5293\n",
      "Epoch 3223/6000, Training Loss: 11.5172, Validation Loss: 11.5286\n",
      "Epoch 3224/6000, Training Loss: 11.5165, Validation Loss: 11.5280\n",
      "Epoch 3225/6000, Training Loss: 11.5159, Validation Loss: 11.5274\n",
      "Epoch 3226/6000, Training Loss: 11.5152, Validation Loss: 11.5267\n",
      "Epoch 3227/6000, Training Loss: 11.5146, Validation Loss: 11.5261\n",
      "Epoch 3228/6000, Training Loss: 11.5140, Validation Loss: 11.5255\n",
      "Epoch 3229/6000, Training Loss: 11.5133, Validation Loss: 11.5248\n",
      "Epoch 3230/6000, Training Loss: 11.5127, Validation Loss: 11.5242\n",
      "Epoch 3231/6000, Training Loss: 11.5120, Validation Loss: 11.5236\n",
      "Epoch 3232/6000, Training Loss: 11.5114, Validation Loss: 11.5230\n",
      "Epoch 3233/6000, Training Loss: 11.5108, Validation Loss: 11.5223\n",
      "Epoch 3234/6000, Training Loss: 11.5101, Validation Loss: 11.5217\n",
      "Epoch 3235/6000, Training Loss: 11.5095, Validation Loss: 11.5211\n",
      "Epoch 3236/6000, Training Loss: 11.5089, Validation Loss: 11.5205\n",
      "Epoch 3237/6000, Training Loss: 11.5082, Validation Loss: 11.5199\n",
      "Epoch 3238/6000, Training Loss: 11.5076, Validation Loss: 11.5192\n",
      "Epoch 3239/6000, Training Loss: 11.5070, Validation Loss: 11.5186\n",
      "Epoch 3240/6000, Training Loss: 11.5064, Validation Loss: 11.5180\n",
      "Epoch 3241/6000, Training Loss: 11.5057, Validation Loss: 11.5174\n",
      "Epoch 3242/6000, Training Loss: 11.5051, Validation Loss: 11.5168\n",
      "Epoch 3243/6000, Training Loss: 11.5045, Validation Loss: 11.5162\n",
      "Epoch 3244/6000, Training Loss: 11.5039, Validation Loss: 11.5156\n",
      "Epoch 3245/6000, Training Loss: 11.5033, Validation Loss: 11.5150\n",
      "Epoch 3246/6000, Training Loss: 11.5027, Validation Loss: 11.5144\n",
      "Epoch 3247/6000, Training Loss: 11.5021, Validation Loss: 11.5138\n",
      "Epoch 3248/6000, Training Loss: 11.5015, Validation Loss: 11.5132\n",
      "Epoch 3249/6000, Training Loss: 11.5009, Validation Loss: 11.5126\n",
      "Epoch 3250/6000, Training Loss: 11.5003, Validation Loss: 11.5120\n",
      "Epoch 3251/6000, Training Loss: 11.4997, Validation Loss: 11.5115\n",
      "Epoch 3252/6000, Training Loss: 11.4991, Validation Loss: 11.5109\n",
      "Epoch 3253/6000, Training Loss: 11.4985, Validation Loss: 11.5103\n",
      "Epoch 3254/6000, Training Loss: 11.4979, Validation Loss: 11.5097\n",
      "Epoch 3255/6000, Training Loss: 11.4973, Validation Loss: 11.5092\n",
      "Epoch 3256/6000, Training Loss: 11.4967, Validation Loss: 11.5086\n",
      "Epoch 3257/6000, Training Loss: 11.4962, Validation Loss: 11.5081\n",
      "Epoch 3258/6000, Training Loss: 11.4956, Validation Loss: 11.5075\n",
      "Epoch 3259/6000, Training Loss: 11.4950, Validation Loss: 11.5070\n",
      "Epoch 3260/6000, Training Loss: 11.4945, Validation Loss: 11.5064\n",
      "Epoch 3261/6000, Training Loss: 11.4939, Validation Loss: 11.5059\n",
      "Epoch 3262/6000, Training Loss: 11.4933, Validation Loss: 11.5053\n",
      "Epoch 3263/6000, Training Loss: 11.4928, Validation Loss: 11.5048\n",
      "Epoch 3264/6000, Training Loss: 11.4922, Validation Loss: 11.5043\n",
      "Epoch 3265/6000, Training Loss: 11.4917, Validation Loss: 11.5037\n",
      "Epoch 3266/6000, Training Loss: 11.4911, Validation Loss: 11.5032\n",
      "Epoch 3267/6000, Training Loss: 11.4906, Validation Loss: 11.5027\n",
      "Epoch 3268/6000, Training Loss: 11.4901, Validation Loss: 11.5021\n",
      "Epoch 3269/6000, Training Loss: 11.4895, Validation Loss: 11.5016\n",
      "Epoch 3270/6000, Training Loss: 11.4890, Validation Loss: 11.5011\n",
      "Epoch 3271/6000, Training Loss: 11.4884, Validation Loss: 11.5006\n",
      "Epoch 3272/6000, Training Loss: 11.4879, Validation Loss: 11.5001\n",
      "Epoch 3273/6000, Training Loss: 11.4874, Validation Loss: 11.4995\n",
      "Epoch 3274/6000, Training Loss: 11.4868, Validation Loss: 11.4990\n",
      "Epoch 3275/6000, Training Loss: 11.4863, Validation Loss: 11.4985\n",
      "Epoch 3276/6000, Training Loss: 11.4858, Validation Loss: 11.4980\n",
      "Epoch 3277/6000, Training Loss: 11.4853, Validation Loss: 11.4975\n",
      "Epoch 3278/6000, Training Loss: 11.4847, Validation Loss: 11.4970\n",
      "Epoch 3279/6000, Training Loss: 11.4842, Validation Loss: 11.4965\n",
      "Epoch 3280/6000, Training Loss: 11.4837, Validation Loss: 11.4960\n",
      "Epoch 3281/6000, Training Loss: 11.4832, Validation Loss: 11.4954\n",
      "Epoch 3282/6000, Training Loss: 11.4827, Validation Loss: 11.4949\n",
      "Epoch 3283/6000, Training Loss: 11.4821, Validation Loss: 11.4944\n",
      "Epoch 3284/6000, Training Loss: 11.4816, Validation Loss: 11.4939\n",
      "Epoch 3285/6000, Training Loss: 11.4811, Validation Loss: 11.4934\n",
      "Epoch 3286/6000, Training Loss: 11.4806, Validation Loss: 11.4929\n",
      "Epoch 3287/6000, Training Loss: 11.4801, Validation Loss: 11.4924\n",
      "Epoch 3288/6000, Training Loss: 11.4796, Validation Loss: 11.4919\n",
      "Epoch 3289/6000, Training Loss: 11.4790, Validation Loss: 11.4914\n",
      "Epoch 3290/6000, Training Loss: 11.4785, Validation Loss: 11.4909\n",
      "Epoch 3291/6000, Training Loss: 11.4780, Validation Loss: 11.4904\n",
      "Epoch 3292/6000, Training Loss: 11.4775, Validation Loss: 11.4899\n",
      "Epoch 3293/6000, Training Loss: 11.4770, Validation Loss: 11.4894\n",
      "Epoch 3294/6000, Training Loss: 11.4765, Validation Loss: 11.4889\n",
      "Epoch 3295/6000, Training Loss: 11.4760, Validation Loss: 11.4885\n",
      "Epoch 3296/6000, Training Loss: 11.4755, Validation Loss: 11.4880\n",
      "Epoch 3297/6000, Training Loss: 11.4750, Validation Loss: 11.4875\n",
      "Epoch 3298/6000, Training Loss: 11.4745, Validation Loss: 11.4870\n",
      "Epoch 3299/6000, Training Loss: 11.4740, Validation Loss: 11.4865\n",
      "Epoch 3300/6000, Training Loss: 11.4735, Validation Loss: 11.4860\n",
      "Epoch 3301/6000, Training Loss: 11.4730, Validation Loss: 11.4855\n",
      "Epoch 3302/6000, Training Loss: 11.4725, Validation Loss: 11.4850\n",
      "Epoch 3303/6000, Training Loss: 11.4720, Validation Loss: 11.4846\n",
      "Epoch 3304/6000, Training Loss: 11.4715, Validation Loss: 11.4841\n",
      "Epoch 3305/6000, Training Loss: 11.4710, Validation Loss: 11.4836\n",
      "Epoch 3306/6000, Training Loss: 11.4705, Validation Loss: 11.4831\n",
      "Epoch 3307/6000, Training Loss: 11.4700, Validation Loss: 11.4826\n",
      "Epoch 3308/6000, Training Loss: 11.4695, Validation Loss: 11.4822\n",
      "Epoch 3309/6000, Training Loss: 11.4690, Validation Loss: 11.4817\n",
      "Epoch 3310/6000, Training Loss: 11.4685, Validation Loss: 11.4812\n",
      "Epoch 3311/6000, Training Loss: 11.4680, Validation Loss: 11.4807\n",
      "Epoch 3312/6000, Training Loss: 11.4675, Validation Loss: 11.4803\n",
      "Epoch 3313/6000, Training Loss: 11.4670, Validation Loss: 11.4798\n",
      "Epoch 3314/6000, Training Loss: 11.4666, Validation Loss: 11.4793\n",
      "Epoch 3315/6000, Training Loss: 11.4661, Validation Loss: 11.4788\n",
      "Epoch 3316/6000, Training Loss: 11.4656, Validation Loss: 11.4784\n",
      "Epoch 3317/6000, Training Loss: 11.4651, Validation Loss: 11.4779\n",
      "Epoch 3318/6000, Training Loss: 11.4646, Validation Loss: 11.4774\n",
      "Epoch 3319/6000, Training Loss: 11.4641, Validation Loss: 11.4770\n",
      "Epoch 3320/6000, Training Loss: 11.4637, Validation Loss: 11.4765\n",
      "Epoch 3321/6000, Training Loss: 11.4632, Validation Loss: 11.4760\n",
      "Epoch 3322/6000, Training Loss: 11.4627, Validation Loss: 11.4756\n",
      "Epoch 3323/6000, Training Loss: 11.4622, Validation Loss: 11.4751\n",
      "Epoch 3324/6000, Training Loss: 11.4618, Validation Loss: 11.4747\n",
      "Epoch 3325/6000, Training Loss: 11.4613, Validation Loss: 11.4742\n",
      "Epoch 3326/6000, Training Loss: 11.4608, Validation Loss: 11.4737\n",
      "Epoch 3327/6000, Training Loss: 11.4604, Validation Loss: 11.4733\n",
      "Epoch 3328/6000, Training Loss: 11.4599, Validation Loss: 11.4728\n",
      "Epoch 3329/6000, Training Loss: 11.4594, Validation Loss: 11.4724\n",
      "Epoch 3330/6000, Training Loss: 11.4590, Validation Loss: 11.4719\n",
      "Epoch 3331/6000, Training Loss: 11.4585, Validation Loss: 11.4714\n",
      "Epoch 3332/6000, Training Loss: 11.4580, Validation Loss: 11.4710\n",
      "Epoch 3333/6000, Training Loss: 11.4576, Validation Loss: 11.4705\n",
      "Epoch 3334/6000, Training Loss: 11.4571, Validation Loss: 11.4701\n",
      "Epoch 3335/6000, Training Loss: 11.4566, Validation Loss: 11.4696\n",
      "Epoch 3336/6000, Training Loss: 11.4562, Validation Loss: 11.4692\n",
      "Epoch 3337/6000, Training Loss: 11.4557, Validation Loss: 11.4687\n",
      "Epoch 3338/6000, Training Loss: 11.4552, Validation Loss: 11.4683\n",
      "Epoch 3339/6000, Training Loss: 11.4548, Validation Loss: 11.4678\n",
      "Epoch 3340/6000, Training Loss: 11.4543, Validation Loss: 11.4674\n",
      "Epoch 3341/6000, Training Loss: 11.4539, Validation Loss: 11.4669\n",
      "Epoch 3342/6000, Training Loss: 11.4534, Validation Loss: 11.4665\n",
      "Epoch 3343/6000, Training Loss: 11.4530, Validation Loss: 11.4660\n",
      "Epoch 3344/6000, Training Loss: 11.4525, Validation Loss: 11.4656\n",
      "Epoch 3345/6000, Training Loss: 11.4520, Validation Loss: 11.4651\n",
      "Epoch 3346/6000, Training Loss: 11.4516, Validation Loss: 11.4647\n",
      "Epoch 3347/6000, Training Loss: 11.4511, Validation Loss: 11.4642\n",
      "Epoch 3348/6000, Training Loss: 11.4507, Validation Loss: 11.4638\n",
      "Epoch 3349/6000, Training Loss: 11.4502, Validation Loss: 11.4634\n",
      "Epoch 3350/6000, Training Loss: 11.4498, Validation Loss: 11.4629\n",
      "Epoch 3351/6000, Training Loss: 11.4493, Validation Loss: 11.4625\n",
      "Epoch 3352/6000, Training Loss: 11.4489, Validation Loss: 11.4620\n",
      "Epoch 3353/6000, Training Loss: 11.4484, Validation Loss: 11.4616\n",
      "Epoch 3354/6000, Training Loss: 11.4480, Validation Loss: 11.4612\n",
      "Epoch 3355/6000, Training Loss: 11.4476, Validation Loss: 11.4607\n",
      "Epoch 3356/6000, Training Loss: 11.4471, Validation Loss: 11.4603\n",
      "Epoch 3357/6000, Training Loss: 11.4467, Validation Loss: 11.4599\n",
      "Epoch 3358/6000, Training Loss: 11.4462, Validation Loss: 11.4594\n",
      "Epoch 3359/6000, Training Loss: 11.4458, Validation Loss: 11.4590\n",
      "Epoch 3360/6000, Training Loss: 11.4454, Validation Loss: 11.4586\n",
      "Epoch 3361/6000, Training Loss: 11.4450, Validation Loss: 11.4582\n",
      "Epoch 3362/6000, Training Loss: 11.4445, Validation Loss: 11.4578\n",
      "Epoch 3363/6000, Training Loss: 11.4441, Validation Loss: 11.4574\n",
      "Epoch 3364/6000, Training Loss: 11.4437, Validation Loss: 11.4569\n",
      "Epoch 3365/6000, Training Loss: 11.4433, Validation Loss: 11.4565\n",
      "Epoch 3366/6000, Training Loss: 11.4428, Validation Loss: 11.4561\n",
      "Epoch 3367/6000, Training Loss: 11.4424, Validation Loss: 11.4557\n",
      "Epoch 3368/6000, Training Loss: 11.4420, Validation Loss: 11.4553\n",
      "Epoch 3369/6000, Training Loss: 11.4416, Validation Loss: 11.4549\n",
      "Epoch 3370/6000, Training Loss: 11.4412, Validation Loss: 11.4545\n",
      "Epoch 3371/6000, Training Loss: 11.4408, Validation Loss: 11.4542\n",
      "Epoch 3372/6000, Training Loss: 11.4404, Validation Loss: 11.4538\n",
      "Epoch 3373/6000, Training Loss: 11.4400, Validation Loss: 11.4534\n",
      "Epoch 3374/6000, Training Loss: 11.4396, Validation Loss: 11.4530\n",
      "Epoch 3375/6000, Training Loss: 11.4392, Validation Loss: 11.4526\n",
      "Epoch 3376/6000, Training Loss: 11.4388, Validation Loss: 11.4522\n",
      "Epoch 3377/6000, Training Loss: 11.4384, Validation Loss: 11.4519\n",
      "Epoch 3378/6000, Training Loss: 11.4380, Validation Loss: 11.4515\n",
      "Epoch 3379/6000, Training Loss: 11.4377, Validation Loss: 11.4511\n",
      "Epoch 3380/6000, Training Loss: 11.4373, Validation Loss: 11.4508\n",
      "Epoch 3381/6000, Training Loss: 11.4369, Validation Loss: 11.4504\n",
      "Epoch 3382/6000, Training Loss: 11.4365, Validation Loss: 11.4500\n",
      "Epoch 3383/6000, Training Loss: 11.4361, Validation Loss: 11.4497\n",
      "Epoch 3384/6000, Training Loss: 11.4358, Validation Loss: 11.4493\n",
      "Epoch 3385/6000, Training Loss: 11.4354, Validation Loss: 11.4489\n",
      "Epoch 3386/6000, Training Loss: 11.4350, Validation Loss: 11.4486\n",
      "Epoch 3387/6000, Training Loss: 11.4346, Validation Loss: 11.4482\n",
      "Epoch 3388/6000, Training Loss: 11.4343, Validation Loss: 11.4479\n",
      "Epoch 3389/6000, Training Loss: 11.4339, Validation Loss: 11.4475\n",
      "Epoch 3390/6000, Training Loss: 11.4335, Validation Loss: 11.4471\n",
      "Epoch 3391/6000, Training Loss: 11.4332, Validation Loss: 11.4468\n",
      "Epoch 3392/6000, Training Loss: 11.4328, Validation Loss: 11.4464\n",
      "Epoch 3393/6000, Training Loss: 11.4324, Validation Loss: 11.4461\n",
      "Epoch 3394/6000, Training Loss: 11.4321, Validation Loss: 11.4457\n",
      "Epoch 3395/6000, Training Loss: 11.4317, Validation Loss: 11.4454\n",
      "Epoch 3396/6000, Training Loss: 11.4314, Validation Loss: 11.4450\n",
      "Epoch 3397/6000, Training Loss: 11.4310, Validation Loss: 11.4447\n",
      "Epoch 3398/6000, Training Loss: 11.4306, Validation Loss: 11.4443\n",
      "Epoch 3399/6000, Training Loss: 11.4303, Validation Loss: 11.4440\n",
      "Epoch 3400/6000, Training Loss: 11.4299, Validation Loss: 11.4437\n",
      "Epoch 3401/6000, Training Loss: 11.4296, Validation Loss: 11.4433\n",
      "Epoch 3402/6000, Training Loss: 11.4292, Validation Loss: 11.4430\n",
      "Epoch 3403/6000, Training Loss: 11.4288, Validation Loss: 11.4426\n",
      "Epoch 3404/6000, Training Loss: 11.4285, Validation Loss: 11.4423\n",
      "Epoch 3405/6000, Training Loss: 11.4281, Validation Loss: 11.4419\n",
      "Epoch 3406/6000, Training Loss: 11.4278, Validation Loss: 11.4416\n",
      "Epoch 3407/6000, Training Loss: 11.4274, Validation Loss: 11.4413\n",
      "Epoch 3408/6000, Training Loss: 11.4271, Validation Loss: 11.4409\n",
      "Epoch 3409/6000, Training Loss: 11.4267, Validation Loss: 11.4406\n",
      "Epoch 3410/6000, Training Loss: 11.4264, Validation Loss: 11.4402\n",
      "Epoch 3411/6000, Training Loss: 11.4260, Validation Loss: 11.4399\n",
      "Epoch 3412/6000, Training Loss: 11.4257, Validation Loss: 11.4396\n",
      "Epoch 3413/6000, Training Loss: 11.4253, Validation Loss: 11.4392\n",
      "Epoch 3414/6000, Training Loss: 11.4250, Validation Loss: 11.4389\n",
      "Epoch 3415/6000, Training Loss: 11.4246, Validation Loss: 11.4385\n",
      "Epoch 3416/6000, Training Loss: 11.4243, Validation Loss: 11.4382\n",
      "Epoch 3417/6000, Training Loss: 11.4239, Validation Loss: 11.4379\n",
      "Epoch 3418/6000, Training Loss: 11.4236, Validation Loss: 11.4375\n",
      "Epoch 3419/6000, Training Loss: 11.4232, Validation Loss: 11.4372\n",
      "Epoch 3420/6000, Training Loss: 11.4229, Validation Loss: 11.4369\n",
      "Epoch 3421/6000, Training Loss: 11.4226, Validation Loss: 11.4365\n",
      "Epoch 3422/6000, Training Loss: 11.4222, Validation Loss: 11.4362\n",
      "Epoch 3423/6000, Training Loss: 11.4219, Validation Loss: 11.4359\n",
      "Epoch 3424/6000, Training Loss: 11.4215, Validation Loss: 11.4355\n",
      "Epoch 3425/6000, Training Loss: 11.4212, Validation Loss: 11.4352\n",
      "Epoch 3426/6000, Training Loss: 11.4208, Validation Loss: 11.4349\n",
      "Epoch 3427/6000, Training Loss: 11.4205, Validation Loss: 11.4346\n",
      "Epoch 3428/6000, Training Loss: 11.4202, Validation Loss: 11.4342\n",
      "Epoch 3429/6000, Training Loss: 11.4198, Validation Loss: 11.4339\n",
      "Epoch 3430/6000, Training Loss: 11.4195, Validation Loss: 11.4336\n",
      "Epoch 3431/6000, Training Loss: 11.4192, Validation Loss: 11.4333\n",
      "Epoch 3432/6000, Training Loss: 11.4188, Validation Loss: 11.4329\n",
      "Epoch 3433/6000, Training Loss: 11.4185, Validation Loss: 11.4326\n",
      "Epoch 3434/6000, Training Loss: 11.4182, Validation Loss: 11.4323\n",
      "Epoch 3435/6000, Training Loss: 11.4178, Validation Loss: 11.4320\n",
      "Epoch 3436/6000, Training Loss: 11.4175, Validation Loss: 11.4317\n",
      "Epoch 3437/6000, Training Loss: 11.4172, Validation Loss: 11.4313\n",
      "Epoch 3438/6000, Training Loss: 11.4169, Validation Loss: 11.4310\n",
      "Epoch 3439/6000, Training Loss: 11.4165, Validation Loss: 11.4307\n",
      "Epoch 3440/6000, Training Loss: 11.4162, Validation Loss: 11.4304\n",
      "Epoch 3441/6000, Training Loss: 11.4159, Validation Loss: 11.4301\n",
      "Epoch 3442/6000, Training Loss: 11.4156, Validation Loss: 11.4298\n",
      "Epoch 3443/6000, Training Loss: 11.4152, Validation Loss: 11.4294\n",
      "Epoch 3444/6000, Training Loss: 11.4149, Validation Loss: 11.4291\n",
      "Epoch 3445/6000, Training Loss: 11.4146, Validation Loss: 11.4288\n",
      "Epoch 3446/6000, Training Loss: 11.4143, Validation Loss: 11.4285\n",
      "Epoch 3447/6000, Training Loss: 11.4139, Validation Loss: 11.4282\n",
      "Epoch 3448/6000, Training Loss: 11.4136, Validation Loss: 11.4279\n",
      "Epoch 3449/6000, Training Loss: 11.4133, Validation Loss: 11.4276\n",
      "Epoch 3450/6000, Training Loss: 11.4130, Validation Loss: 11.4273\n",
      "Epoch 3451/6000, Training Loss: 11.4127, Validation Loss: 11.4269\n",
      "Epoch 3452/6000, Training Loss: 11.4123, Validation Loss: 11.4266\n",
      "Epoch 3453/6000, Training Loss: 11.4120, Validation Loss: 11.4263\n",
      "Epoch 3454/6000, Training Loss: 11.4117, Validation Loss: 11.4260\n",
      "Epoch 3455/6000, Training Loss: 11.4114, Validation Loss: 11.4257\n",
      "Epoch 3456/6000, Training Loss: 11.4111, Validation Loss: 11.4254\n",
      "Epoch 3457/6000, Training Loss: 11.4108, Validation Loss: 11.4251\n",
      "Epoch 3458/6000, Training Loss: 11.4105, Validation Loss: 11.4248\n",
      "Epoch 3459/6000, Training Loss: 11.4101, Validation Loss: 11.4245\n",
      "Epoch 3460/6000, Training Loss: 11.4098, Validation Loss: 11.4242\n",
      "Epoch 3461/6000, Training Loss: 11.4095, Validation Loss: 11.4239\n",
      "Epoch 3462/6000, Training Loss: 11.4092, Validation Loss: 11.4236\n",
      "Epoch 3463/6000, Training Loss: 11.4089, Validation Loss: 11.4233\n",
      "Epoch 3464/6000, Training Loss: 11.4086, Validation Loss: 11.4230\n",
      "Epoch 3465/6000, Training Loss: 11.4083, Validation Loss: 11.4227\n",
      "Epoch 3466/6000, Training Loss: 11.4080, Validation Loss: 11.4224\n",
      "Epoch 3467/6000, Training Loss: 11.4077, Validation Loss: 11.4221\n",
      "Epoch 3468/6000, Training Loss: 11.4073, Validation Loss: 11.4218\n",
      "Epoch 3469/6000, Training Loss: 11.4070, Validation Loss: 11.4215\n",
      "Epoch 3470/6000, Training Loss: 11.4067, Validation Loss: 11.4212\n",
      "Epoch 3471/6000, Training Loss: 11.4064, Validation Loss: 11.4209\n",
      "Epoch 3472/6000, Training Loss: 11.4061, Validation Loss: 11.4206\n",
      "Epoch 3473/6000, Training Loss: 11.4058, Validation Loss: 11.4203\n",
      "Epoch 3474/6000, Training Loss: 11.4055, Validation Loss: 11.4200\n",
      "Epoch 3475/6000, Training Loss: 11.4052, Validation Loss: 11.4197\n",
      "Epoch 3476/6000, Training Loss: 11.4049, Validation Loss: 11.4194\n",
      "Epoch 3477/6000, Training Loss: 11.4046, Validation Loss: 11.4191\n",
      "Epoch 3478/6000, Training Loss: 11.4043, Validation Loss: 11.4188\n",
      "Epoch 3479/6000, Training Loss: 11.4040, Validation Loss: 11.4185\n",
      "Epoch 3480/6000, Training Loss: 11.4037, Validation Loss: 11.4182\n",
      "Epoch 3481/6000, Training Loss: 11.4034, Validation Loss: 11.4179\n",
      "Epoch 3482/6000, Training Loss: 11.4031, Validation Loss: 11.4177\n",
      "Epoch 3483/6000, Training Loss: 11.4028, Validation Loss: 11.4174\n",
      "Epoch 3484/6000, Training Loss: 11.4025, Validation Loss: 11.4171\n",
      "Epoch 3485/6000, Training Loss: 11.4022, Validation Loss: 11.4168\n",
      "Epoch 3486/6000, Training Loss: 11.4019, Validation Loss: 11.4165\n",
      "Epoch 3487/6000, Training Loss: 11.4016, Validation Loss: 11.4162\n",
      "Epoch 3488/6000, Training Loss: 11.4013, Validation Loss: 11.4159\n",
      "Epoch 3489/6000, Training Loss: 11.4010, Validation Loss: 11.4157\n",
      "Epoch 3490/6000, Training Loss: 11.4008, Validation Loss: 11.4154\n",
      "Epoch 3491/6000, Training Loss: 11.4005, Validation Loss: 11.4151\n",
      "Epoch 3492/6000, Training Loss: 11.4002, Validation Loss: 11.4148\n",
      "Epoch 3493/6000, Training Loss: 11.3999, Validation Loss: 11.4146\n",
      "Epoch 3494/6000, Training Loss: 11.3996, Validation Loss: 11.4143\n",
      "Epoch 3495/6000, Training Loss: 11.3993, Validation Loss: 11.4140\n",
      "Epoch 3496/6000, Training Loss: 11.3991, Validation Loss: 11.4138\n",
      "Epoch 3497/6000, Training Loss: 11.3988, Validation Loss: 11.4135\n",
      "Epoch 3498/6000, Training Loss: 11.3985, Validation Loss: 11.4132\n",
      "Epoch 3499/6000, Training Loss: 11.3982, Validation Loss: 11.4130\n",
      "Epoch 3500/6000, Training Loss: 11.3980, Validation Loss: 11.4127\n",
      "Epoch 3501/6000, Training Loss: 11.3977, Validation Loss: 11.4124\n",
      "Epoch 3502/6000, Training Loss: 11.3974, Validation Loss: 11.4122\n",
      "Epoch 3503/6000, Training Loss: 11.3972, Validation Loss: 11.4119\n",
      "Epoch 3504/6000, Training Loss: 11.3969, Validation Loss: 11.4117\n",
      "Epoch 3505/6000, Training Loss: 11.3966, Validation Loss: 11.4114\n",
      "Epoch 3506/6000, Training Loss: 11.3964, Validation Loss: 11.4112\n",
      "Epoch 3507/6000, Training Loss: 11.3961, Validation Loss: 11.4109\n",
      "Epoch 3508/6000, Training Loss: 11.3959, Validation Loss: 11.4107\n",
      "Epoch 3509/6000, Training Loss: 11.3956, Validation Loss: 11.4104\n",
      "Epoch 3510/6000, Training Loss: 11.3954, Validation Loss: 11.4102\n",
      "Epoch 3511/6000, Training Loss: 11.3951, Validation Loss: 11.4100\n",
      "Epoch 3512/6000, Training Loss: 11.3949, Validation Loss: 11.4097\n",
      "Epoch 3513/6000, Training Loss: 11.3946, Validation Loss: 11.4095\n",
      "Epoch 3514/6000, Training Loss: 11.3944, Validation Loss: 11.4092\n",
      "Epoch 3515/6000, Training Loss: 11.3941, Validation Loss: 11.4090\n",
      "Epoch 3516/6000, Training Loss: 11.3939, Validation Loss: 11.4088\n",
      "Epoch 3517/6000, Training Loss: 11.3936, Validation Loss: 11.4085\n",
      "Epoch 3518/6000, Training Loss: 11.3934, Validation Loss: 11.4083\n",
      "Epoch 3519/6000, Training Loss: 11.3932, Validation Loss: 11.4081\n",
      "Epoch 3520/6000, Training Loss: 11.3929, Validation Loss: 11.4079\n",
      "Epoch 3521/6000, Training Loss: 11.3927, Validation Loss: 11.4076\n",
      "Epoch 3522/6000, Training Loss: 11.3924, Validation Loss: 11.4074\n",
      "Epoch 3523/6000, Training Loss: 11.3922, Validation Loss: 11.4072\n",
      "Epoch 3524/6000, Training Loss: 11.3920, Validation Loss: 11.4070\n",
      "Epoch 3525/6000, Training Loss: 11.3917, Validation Loss: 11.4067\n",
      "Epoch 3526/6000, Training Loss: 11.3915, Validation Loss: 11.4065\n",
      "Epoch 3527/6000, Training Loss: 11.3913, Validation Loss: 11.4063\n",
      "Epoch 3528/6000, Training Loss: 11.3911, Validation Loss: 11.4061\n",
      "Epoch 3529/6000, Training Loss: 11.3908, Validation Loss: 11.4059\n",
      "Epoch 3530/6000, Training Loss: 11.3906, Validation Loss: 11.4056\n",
      "Epoch 3531/6000, Training Loss: 11.3904, Validation Loss: 11.4054\n",
      "Epoch 3532/6000, Training Loss: 11.3902, Validation Loss: 11.4052\n",
      "Epoch 3533/6000, Training Loss: 11.3899, Validation Loss: 11.4050\n",
      "Epoch 3534/6000, Training Loss: 11.3897, Validation Loss: 11.4048\n",
      "Epoch 3535/6000, Training Loss: 11.3895, Validation Loss: 11.4046\n",
      "Epoch 3536/6000, Training Loss: 11.3893, Validation Loss: 11.4044\n",
      "Epoch 3537/6000, Training Loss: 11.3890, Validation Loss: 11.4041\n",
      "Epoch 3538/6000, Training Loss: 11.3888, Validation Loss: 11.4039\n",
      "Epoch 3539/6000, Training Loss: 11.3886, Validation Loss: 11.4037\n",
      "Epoch 3540/6000, Training Loss: 11.3884, Validation Loss: 11.4035\n",
      "Epoch 3541/6000, Training Loss: 11.3882, Validation Loss: 11.4033\n",
      "Epoch 3542/6000, Training Loss: 11.3879, Validation Loss: 11.4031\n",
      "Epoch 3543/6000, Training Loss: 11.3877, Validation Loss: 11.4029\n",
      "Epoch 3544/6000, Training Loss: 11.3875, Validation Loss: 11.4027\n",
      "Epoch 3545/6000, Training Loss: 11.3873, Validation Loss: 11.4024\n",
      "Epoch 3546/6000, Training Loss: 11.3871, Validation Loss: 11.4022\n",
      "Epoch 3547/6000, Training Loss: 11.3868, Validation Loss: 11.4020\n",
      "Epoch 3548/6000, Training Loss: 11.3866, Validation Loss: 11.4018\n",
      "Epoch 3549/6000, Training Loss: 11.3864, Validation Loss: 11.4016\n",
      "Epoch 3550/6000, Training Loss: 11.3862, Validation Loss: 11.4014\n",
      "Epoch 3551/6000, Training Loss: 11.3860, Validation Loss: 11.4012\n",
      "Epoch 3552/6000, Training Loss: 11.3858, Validation Loss: 11.4010\n",
      "Epoch 3553/6000, Training Loss: 11.3855, Validation Loss: 11.4008\n",
      "Epoch 3554/6000, Training Loss: 11.3853, Validation Loss: 11.4006\n",
      "Epoch 3555/6000, Training Loss: 11.3851, Validation Loss: 11.4004\n",
      "Epoch 3556/6000, Training Loss: 11.3849, Validation Loss: 11.4002\n",
      "Epoch 3557/6000, Training Loss: 11.3847, Validation Loss: 11.4000\n",
      "Epoch 3558/6000, Training Loss: 11.3845, Validation Loss: 11.3997\n",
      "Epoch 3559/6000, Training Loss: 11.3843, Validation Loss: 11.3995\n",
      "Epoch 3560/6000, Training Loss: 11.3841, Validation Loss: 11.3993\n",
      "Epoch 3561/6000, Training Loss: 11.3838, Validation Loss: 11.3991\n",
      "Epoch 3562/6000, Training Loss: 11.3836, Validation Loss: 11.3989\n",
      "Epoch 3563/6000, Training Loss: 11.3834, Validation Loss: 11.3987\n",
      "Epoch 3564/6000, Training Loss: 11.3832, Validation Loss: 11.3985\n",
      "Epoch 3565/6000, Training Loss: 11.3830, Validation Loss: 11.3983\n",
      "Epoch 3566/6000, Training Loss: 11.3828, Validation Loss: 11.3981\n",
      "Epoch 3567/6000, Training Loss: 11.3826, Validation Loss: 11.3979\n",
      "Epoch 3568/6000, Training Loss: 11.3824, Validation Loss: 11.3977\n",
      "Epoch 3569/6000, Training Loss: 11.3822, Validation Loss: 11.3975\n",
      "Epoch 3570/6000, Training Loss: 11.3820, Validation Loss: 11.3973\n",
      "Epoch 3571/6000, Training Loss: 11.3817, Validation Loss: 11.3971\n",
      "Epoch 3572/6000, Training Loss: 11.3815, Validation Loss: 11.3969\n",
      "Epoch 3573/6000, Training Loss: 11.3813, Validation Loss: 11.3967\n",
      "Epoch 3574/6000, Training Loss: 11.3811, Validation Loss: 11.3965\n",
      "Epoch 3575/6000, Training Loss: 11.3809, Validation Loss: 11.3963\n",
      "Epoch 3576/6000, Training Loss: 11.3807, Validation Loss: 11.3961\n",
      "Epoch 3577/6000, Training Loss: 11.3805, Validation Loss: 11.3959\n",
      "Epoch 3578/6000, Training Loss: 11.3803, Validation Loss: 11.3958\n",
      "Epoch 3579/6000, Training Loss: 11.3801, Validation Loss: 11.3956\n",
      "Epoch 3580/6000, Training Loss: 11.3799, Validation Loss: 11.3954\n",
      "Epoch 3581/6000, Training Loss: 11.3797, Validation Loss: 11.3952\n",
      "Epoch 3582/6000, Training Loss: 11.3795, Validation Loss: 11.3950\n",
      "Epoch 3583/6000, Training Loss: 11.3793, Validation Loss: 11.3948\n",
      "Epoch 3584/6000, Training Loss: 11.3791, Validation Loss: 11.3946\n",
      "Epoch 3585/6000, Training Loss: 11.3789, Validation Loss: 11.3944\n",
      "Epoch 3586/6000, Training Loss: 11.3787, Validation Loss: 11.3942\n",
      "Epoch 3587/6000, Training Loss: 11.3785, Validation Loss: 11.3940\n",
      "Epoch 3588/6000, Training Loss: 11.3783, Validation Loss: 11.3938\n",
      "Epoch 3589/6000, Training Loss: 11.3781, Validation Loss: 11.3936\n",
      "Epoch 3590/6000, Training Loss: 11.3779, Validation Loss: 11.3935\n",
      "Epoch 3591/6000, Training Loss: 11.3777, Validation Loss: 11.3933\n",
      "Epoch 3592/6000, Training Loss: 11.3775, Validation Loss: 11.3931\n",
      "Epoch 3593/6000, Training Loss: 11.3773, Validation Loss: 11.3929\n",
      "Epoch 3594/6000, Training Loss: 11.3771, Validation Loss: 11.3927\n",
      "Epoch 3595/6000, Training Loss: 11.3769, Validation Loss: 11.3925\n",
      "Epoch 3596/6000, Training Loss: 11.3768, Validation Loss: 11.3923\n",
      "Epoch 3597/6000, Training Loss: 11.3766, Validation Loss: 11.3921\n",
      "Epoch 3598/6000, Training Loss: 11.3764, Validation Loss: 11.3920\n",
      "Epoch 3599/6000, Training Loss: 11.3762, Validation Loss: 11.3918\n",
      "Epoch 3600/6000, Training Loss: 11.3760, Validation Loss: 11.3916\n",
      "Epoch 3601/6000, Training Loss: 11.3758, Validation Loss: 11.3914\n",
      "Epoch 3602/6000, Training Loss: 11.3756, Validation Loss: 11.3912\n",
      "Epoch 3603/6000, Training Loss: 11.3754, Validation Loss: 11.3910\n",
      "Epoch 3604/6000, Training Loss: 11.3752, Validation Loss: 11.3908\n",
      "Epoch 3605/6000, Training Loss: 11.3750, Validation Loss: 11.3907\n",
      "Epoch 3606/6000, Training Loss: 11.3748, Validation Loss: 11.3905\n",
      "Epoch 3607/6000, Training Loss: 11.3747, Validation Loss: 11.3903\n",
      "Epoch 3608/6000, Training Loss: 11.3745, Validation Loss: 11.3901\n",
      "Epoch 3609/6000, Training Loss: 11.3743, Validation Loss: 11.3899\n",
      "Epoch 3610/6000, Training Loss: 11.3741, Validation Loss: 11.3898\n",
      "Epoch 3611/6000, Training Loss: 11.3739, Validation Loss: 11.3896\n",
      "Epoch 3612/6000, Training Loss: 11.3737, Validation Loss: 11.3894\n",
      "Epoch 3613/6000, Training Loss: 11.3735, Validation Loss: 11.3892\n",
      "Epoch 3614/6000, Training Loss: 11.3734, Validation Loss: 11.3890\n",
      "Epoch 3615/6000, Training Loss: 11.3732, Validation Loss: 11.3889\n",
      "Epoch 3616/6000, Training Loss: 11.3730, Validation Loss: 11.3887\n",
      "Epoch 3617/6000, Training Loss: 11.3728, Validation Loss: 11.3885\n",
      "Epoch 3618/6000, Training Loss: 11.3726, Validation Loss: 11.3883\n",
      "Epoch 3619/6000, Training Loss: 11.3724, Validation Loss: 11.3881\n",
      "Epoch 3620/6000, Training Loss: 11.3722, Validation Loss: 11.3880\n",
      "Epoch 3621/6000, Training Loss: 11.3721, Validation Loss: 11.3878\n",
      "Epoch 3622/6000, Training Loss: 11.3719, Validation Loss: 11.3876\n",
      "Epoch 3623/6000, Training Loss: 11.3717, Validation Loss: 11.3874\n",
      "Epoch 3624/6000, Training Loss: 11.3715, Validation Loss: 11.3873\n",
      "Epoch 3625/6000, Training Loss: 11.3713, Validation Loss: 11.3871\n",
      "Epoch 3626/6000, Training Loss: 11.3712, Validation Loss: 11.3869\n",
      "Epoch 3627/6000, Training Loss: 11.3710, Validation Loss: 11.3867\n",
      "Epoch 3628/6000, Training Loss: 11.3708, Validation Loss: 11.3866\n",
      "Epoch 3629/6000, Training Loss: 11.3706, Validation Loss: 11.3864\n",
      "Epoch 3630/6000, Training Loss: 11.3704, Validation Loss: 11.3862\n",
      "Epoch 3631/6000, Training Loss: 11.3702, Validation Loss: 11.3860\n",
      "Epoch 3632/6000, Training Loss: 11.3701, Validation Loss: 11.3859\n",
      "Epoch 3633/6000, Training Loss: 11.3699, Validation Loss: 11.3857\n",
      "Epoch 3634/6000, Training Loss: 11.3697, Validation Loss: 11.3855\n",
      "Epoch 3635/6000, Training Loss: 11.3695, Validation Loss: 11.3853\n",
      "Epoch 3636/6000, Training Loss: 11.3693, Validation Loss: 11.3852\n",
      "Epoch 3637/6000, Training Loss: 11.3692, Validation Loss: 11.3850\n",
      "Epoch 3638/6000, Training Loss: 11.3690, Validation Loss: 11.3848\n",
      "Epoch 3639/6000, Training Loss: 11.3688, Validation Loss: 11.3847\n",
      "Epoch 3640/6000, Training Loss: 11.3686, Validation Loss: 11.3845\n",
      "Epoch 3641/6000, Training Loss: 11.3685, Validation Loss: 11.3843\n",
      "Epoch 3642/6000, Training Loss: 11.3683, Validation Loss: 11.3842\n",
      "Epoch 3643/6000, Training Loss: 11.3681, Validation Loss: 11.3840\n",
      "Epoch 3644/6000, Training Loss: 11.3679, Validation Loss: 11.3838\n",
      "Epoch 3645/6000, Training Loss: 11.3678, Validation Loss: 11.3837\n",
      "Epoch 3646/6000, Training Loss: 11.3676, Validation Loss: 11.3835\n",
      "Epoch 3647/6000, Training Loss: 11.3674, Validation Loss: 11.3833\n",
      "Epoch 3648/6000, Training Loss: 11.3672, Validation Loss: 11.3832\n",
      "Epoch 3649/6000, Training Loss: 11.3671, Validation Loss: 11.3830\n",
      "Epoch 3650/6000, Training Loss: 11.3669, Validation Loss: 11.3829\n",
      "Epoch 3651/6000, Training Loss: 11.3667, Validation Loss: 11.3827\n",
      "Epoch 3652/6000, Training Loss: 11.3666, Validation Loss: 11.3825\n",
      "Epoch 3653/6000, Training Loss: 11.3664, Validation Loss: 11.3824\n",
      "Epoch 3654/6000, Training Loss: 11.3662, Validation Loss: 11.3822\n",
      "Epoch 3655/6000, Training Loss: 11.3661, Validation Loss: 11.3821\n",
      "Epoch 3656/6000, Training Loss: 11.3659, Validation Loss: 11.3819\n",
      "Epoch 3657/6000, Training Loss: 11.3658, Validation Loss: 11.3818\n",
      "Epoch 3658/6000, Training Loss: 11.3656, Validation Loss: 11.3816\n",
      "Epoch 3659/6000, Training Loss: 11.3654, Validation Loss: 11.3815\n",
      "Epoch 3660/6000, Training Loss: 11.3653, Validation Loss: 11.3813\n",
      "Epoch 3661/6000, Training Loss: 11.3651, Validation Loss: 11.3812\n",
      "Epoch 3662/6000, Training Loss: 11.3650, Validation Loss: 11.3810\n",
      "Epoch 3663/6000, Training Loss: 11.3648, Validation Loss: 11.3809\n",
      "Epoch 3664/6000, Training Loss: 11.3647, Validation Loss: 11.3807\n",
      "Epoch 3665/6000, Training Loss: 11.3645, Validation Loss: 11.3806\n",
      "Epoch 3666/6000, Training Loss: 11.3644, Validation Loss: 11.3805\n",
      "Epoch 3667/6000, Training Loss: 11.3642, Validation Loss: 11.3803\n",
      "Epoch 3668/6000, Training Loss: 11.3641, Validation Loss: 11.3802\n",
      "Epoch 3669/6000, Training Loss: 11.3639, Validation Loss: 11.3800\n",
      "Epoch 3670/6000, Training Loss: 11.3638, Validation Loss: 11.3799\n",
      "Epoch 3671/6000, Training Loss: 11.3636, Validation Loss: 11.3798\n",
      "Epoch 3672/6000, Training Loss: 11.3635, Validation Loss: 11.3796\n",
      "Epoch 3673/6000, Training Loss: 11.3633, Validation Loss: 11.3795\n",
      "Epoch 3674/6000, Training Loss: 11.3632, Validation Loss: 11.3794\n",
      "Epoch 3675/6000, Training Loss: 11.3630, Validation Loss: 11.3792\n",
      "Epoch 3676/6000, Training Loss: 11.3629, Validation Loss: 11.3791\n",
      "Epoch 3677/6000, Training Loss: 11.3628, Validation Loss: 11.3790\n",
      "Epoch 3678/6000, Training Loss: 11.3626, Validation Loss: 11.3789\n",
      "Epoch 3679/6000, Training Loss: 11.3625, Validation Loss: 11.3787\n",
      "Epoch 3680/6000, Training Loss: 11.3624, Validation Loss: 11.3786\n",
      "Epoch 3681/6000, Training Loss: 11.3622, Validation Loss: 11.3785\n",
      "Epoch 3682/6000, Training Loss: 11.3621, Validation Loss: 11.3784\n",
      "Epoch 3683/6000, Training Loss: 11.3620, Validation Loss: 11.3782\n",
      "Epoch 3684/6000, Training Loss: 11.3618, Validation Loss: 11.3781\n",
      "Epoch 3685/6000, Training Loss: 11.3617, Validation Loss: 11.3780\n",
      "Epoch 3686/6000, Training Loss: 11.3616, Validation Loss: 11.3779\n",
      "Epoch 3687/6000, Training Loss: 11.3614, Validation Loss: 11.3778\n",
      "Epoch 3688/6000, Training Loss: 11.3613, Validation Loss: 11.3776\n",
      "Epoch 3689/6000, Training Loss: 11.3612, Validation Loss: 11.3775\n",
      "Epoch 3690/6000, Training Loss: 11.3611, Validation Loss: 11.3774\n",
      "Epoch 3691/6000, Training Loss: 11.3609, Validation Loss: 11.3773\n",
      "Epoch 3692/6000, Training Loss: 11.3608, Validation Loss: 11.3772\n",
      "Epoch 3693/6000, Training Loss: 11.3607, Validation Loss: 11.3771\n",
      "Epoch 3694/6000, Training Loss: 11.3606, Validation Loss: 11.3770\n",
      "Epoch 3695/6000, Training Loss: 11.3605, Validation Loss: 11.3768\n",
      "Epoch 3696/6000, Training Loss: 11.3603, Validation Loss: 11.3767\n",
      "Epoch 3697/6000, Training Loss: 11.3602, Validation Loss: 11.3766\n",
      "Epoch 3698/6000, Training Loss: 11.3601, Validation Loss: 11.3765\n",
      "Epoch 3699/6000, Training Loss: 11.3600, Validation Loss: 11.3764\n",
      "Epoch 3700/6000, Training Loss: 11.3599, Validation Loss: 11.3763\n",
      "Epoch 3701/6000, Training Loss: 11.3597, Validation Loss: 11.3762\n",
      "Epoch 3702/6000, Training Loss: 11.3596, Validation Loss: 11.3761\n",
      "Epoch 3703/6000, Training Loss: 11.3595, Validation Loss: 11.3760\n",
      "Epoch 3704/6000, Training Loss: 11.3594, Validation Loss: 11.3759\n",
      "Epoch 3705/6000, Training Loss: 11.3593, Validation Loss: 11.3758\n",
      "Epoch 3706/6000, Training Loss: 11.3591, Validation Loss: 11.3756\n",
      "Epoch 3707/6000, Training Loss: 11.3590, Validation Loss: 11.3755\n",
      "Epoch 3708/6000, Training Loss: 11.3589, Validation Loss: 11.3754\n",
      "Epoch 3709/6000, Training Loss: 11.3588, Validation Loss: 11.3753\n",
      "Epoch 3710/6000, Training Loss: 11.3587, Validation Loss: 11.3752\n",
      "Epoch 3711/6000, Training Loss: 11.3586, Validation Loss: 11.3751\n",
      "Epoch 3712/6000, Training Loss: 11.3585, Validation Loss: 11.3750\n",
      "Epoch 3713/6000, Training Loss: 11.3583, Validation Loss: 11.3749\n",
      "Epoch 3714/6000, Training Loss: 11.3582, Validation Loss: 11.3748\n",
      "Epoch 3715/6000, Training Loss: 11.3581, Validation Loss: 11.3747\n",
      "Epoch 3716/6000, Training Loss: 11.3580, Validation Loss: 11.3746\n",
      "Epoch 3717/6000, Training Loss: 11.3579, Validation Loss: 11.3745\n",
      "Epoch 3718/6000, Training Loss: 11.3578, Validation Loss: 11.3744\n",
      "Epoch 3719/6000, Training Loss: 11.3577, Validation Loss: 11.3743\n",
      "Epoch 3720/6000, Training Loss: 11.3575, Validation Loss: 11.3742\n",
      "Epoch 3721/6000, Training Loss: 11.3574, Validation Loss: 11.3741\n",
      "Epoch 3722/6000, Training Loss: 11.3573, Validation Loss: 11.3740\n",
      "Epoch 3723/6000, Training Loss: 11.3572, Validation Loss: 11.3739\n",
      "Epoch 3724/6000, Training Loss: 11.3571, Validation Loss: 11.3738\n",
      "Epoch 3725/6000, Training Loss: 11.3570, Validation Loss: 11.3737\n",
      "Epoch 3726/6000, Training Loss: 11.3569, Validation Loss: 11.3736\n",
      "Epoch 3727/6000, Training Loss: 11.3568, Validation Loss: 11.3735\n",
      "Epoch 3728/6000, Training Loss: 11.3566, Validation Loss: 11.3734\n",
      "Epoch 3729/6000, Training Loss: 11.3565, Validation Loss: 11.3733\n",
      "Epoch 3730/6000, Training Loss: 11.3564, Validation Loss: 11.3732\n",
      "Epoch 3731/6000, Training Loss: 11.3563, Validation Loss: 11.3731\n",
      "Epoch 3732/6000, Training Loss: 11.3562, Validation Loss: 11.3730\n",
      "Epoch 3733/6000, Training Loss: 11.3561, Validation Loss: 11.3729\n",
      "Epoch 3734/6000, Training Loss: 11.3560, Validation Loss: 11.3728\n",
      "Epoch 3735/6000, Training Loss: 11.3559, Validation Loss: 11.3727\n",
      "Epoch 3736/6000, Training Loss: 11.3558, Validation Loss: 11.3726\n",
      "Epoch 3737/6000, Training Loss: 11.3556, Validation Loss: 11.3725\n",
      "Epoch 3738/6000, Training Loss: 11.3555, Validation Loss: 11.3724\n",
      "Epoch 3739/6000, Training Loss: 11.3554, Validation Loss: 11.3723\n",
      "Epoch 3740/6000, Training Loss: 11.3553, Validation Loss: 11.3722\n",
      "Epoch 3741/6000, Training Loss: 11.3552, Validation Loss: 11.3721\n",
      "Epoch 3742/6000, Training Loss: 11.3551, Validation Loss: 11.3720\n",
      "Epoch 3743/6000, Training Loss: 11.3550, Validation Loss: 11.3719\n",
      "Epoch 3744/6000, Training Loss: 11.3549, Validation Loss: 11.3718\n",
      "Epoch 3745/6000, Training Loss: 11.3548, Validation Loss: 11.3717\n",
      "Epoch 3746/6000, Training Loss: 11.3547, Validation Loss: 11.3716\n",
      "Epoch 3747/6000, Training Loss: 11.3546, Validation Loss: 11.3715\n",
      "Epoch 3748/6000, Training Loss: 11.3545, Validation Loss: 11.3714\n",
      "Epoch 3749/6000, Training Loss: 11.3544, Validation Loss: 11.3713\n",
      "Epoch 3750/6000, Training Loss: 11.3542, Validation Loss: 11.3712\n",
      "Epoch 3751/6000, Training Loss: 11.3541, Validation Loss: 11.3711\n",
      "Epoch 3752/6000, Training Loss: 11.3540, Validation Loss: 11.3710\n",
      "Epoch 3753/6000, Training Loss: 11.3539, Validation Loss: 11.3709\n",
      "Epoch 3754/6000, Training Loss: 11.3538, Validation Loss: 11.3708\n",
      "Epoch 3755/6000, Training Loss: 11.3537, Validation Loss: 11.3707\n",
      "Epoch 3756/6000, Training Loss: 11.3536, Validation Loss: 11.3706\n",
      "Epoch 3757/6000, Training Loss: 11.3535, Validation Loss: 11.3705\n",
      "Epoch 3758/6000, Training Loss: 11.3534, Validation Loss: 11.3704\n",
      "Epoch 3759/6000, Training Loss: 11.3533, Validation Loss: 11.3703\n",
      "Epoch 3760/6000, Training Loss: 11.3532, Validation Loss: 11.3702\n",
      "Epoch 3761/6000, Training Loss: 11.3531, Validation Loss: 11.3702\n",
      "Epoch 3762/6000, Training Loss: 11.3530, Validation Loss: 11.3701\n",
      "Epoch 3763/6000, Training Loss: 11.3529, Validation Loss: 11.3700\n",
      "Epoch 3764/6000, Training Loss: 11.3528, Validation Loss: 11.3699\n",
      "Epoch 3765/6000, Training Loss: 11.3527, Validation Loss: 11.3698\n",
      "Epoch 3766/6000, Training Loss: 11.3526, Validation Loss: 11.3697\n",
      "Epoch 3767/6000, Training Loss: 11.3525, Validation Loss: 11.3696\n",
      "Epoch 3768/6000, Training Loss: 11.3524, Validation Loss: 11.3695\n",
      "Epoch 3769/6000, Training Loss: 11.3523, Validation Loss: 11.3694\n",
      "Epoch 3770/6000, Training Loss: 11.3522, Validation Loss: 11.3693\n",
      "Epoch 3771/6000, Training Loss: 11.3521, Validation Loss: 11.3692\n",
      "Epoch 3772/6000, Training Loss: 11.3520, Validation Loss: 11.3692\n",
      "Epoch 3773/6000, Training Loss: 11.3519, Validation Loss: 11.3691\n",
      "Epoch 3774/6000, Training Loss: 11.3518, Validation Loss: 11.3690\n",
      "Epoch 3775/6000, Training Loss: 11.3517, Validation Loss: 11.3689\n",
      "Epoch 3776/6000, Training Loss: 11.3516, Validation Loss: 11.3688\n",
      "Epoch 3777/6000, Training Loss: 11.3515, Validation Loss: 11.3687\n",
      "Epoch 3778/6000, Training Loss: 11.3514, Validation Loss: 11.3686\n",
      "Epoch 3779/6000, Training Loss: 11.3513, Validation Loss: 11.3685\n",
      "Epoch 3780/6000, Training Loss: 11.3512, Validation Loss: 11.3685\n",
      "Epoch 3781/6000, Training Loss: 11.3511, Validation Loss: 11.3684\n",
      "Epoch 3782/6000, Training Loss: 11.3510, Validation Loss: 11.3683\n",
      "Epoch 3783/6000, Training Loss: 11.3509, Validation Loss: 11.3682\n",
      "Epoch 3784/6000, Training Loss: 11.3508, Validation Loss: 11.3681\n",
      "Epoch 3785/6000, Training Loss: 11.3507, Validation Loss: 11.3680\n",
      "Epoch 3786/6000, Training Loss: 11.3506, Validation Loss: 11.3679\n",
      "Epoch 3787/6000, Training Loss: 11.3505, Validation Loss: 11.3679\n",
      "Epoch 3788/6000, Training Loss: 11.3504, Validation Loss: 11.3678\n",
      "Epoch 3789/6000, Training Loss: 11.3503, Validation Loss: 11.3677\n",
      "Epoch 3790/6000, Training Loss: 11.3502, Validation Loss: 11.3676\n",
      "Epoch 3791/6000, Training Loss: 11.3501, Validation Loss: 11.3675\n",
      "Epoch 3792/6000, Training Loss: 11.3500, Validation Loss: 11.3674\n",
      "Epoch 3793/6000, Training Loss: 11.3500, Validation Loss: 11.3673\n",
      "Epoch 3794/6000, Training Loss: 11.3499, Validation Loss: 11.3673\n",
      "Epoch 3795/6000, Training Loss: 11.3498, Validation Loss: 11.3672\n",
      "Epoch 3796/6000, Training Loss: 11.3497, Validation Loss: 11.3671\n",
      "Epoch 3797/6000, Training Loss: 11.3496, Validation Loss: 11.3670\n",
      "Epoch 3798/6000, Training Loss: 11.3495, Validation Loss: 11.3669\n",
      "Epoch 3799/6000, Training Loss: 11.3494, Validation Loss: 11.3668\n",
      "Epoch 3800/6000, Training Loss: 11.3493, Validation Loss: 11.3667\n",
      "Epoch 3801/6000, Training Loss: 11.3492, Validation Loss: 11.3666\n",
      "Epoch 3802/6000, Training Loss: 11.3491, Validation Loss: 11.3666\n",
      "Epoch 3803/6000, Training Loss: 11.3490, Validation Loss: 11.3665\n",
      "Epoch 3804/6000, Training Loss: 11.3490, Validation Loss: 11.3664\n",
      "Epoch 3805/6000, Training Loss: 11.3489, Validation Loss: 11.3663\n",
      "Epoch 3806/6000, Training Loss: 11.3488, Validation Loss: 11.3662\n",
      "Epoch 3807/6000, Training Loss: 11.3487, Validation Loss: 11.3661\n",
      "Epoch 3808/6000, Training Loss: 11.3486, Validation Loss: 11.3661\n",
      "Epoch 3809/6000, Training Loss: 11.3485, Validation Loss: 11.3660\n",
      "Epoch 3810/6000, Training Loss: 11.3484, Validation Loss: 11.3659\n",
      "Epoch 3811/6000, Training Loss: 11.3483, Validation Loss: 11.3658\n",
      "Epoch 3812/6000, Training Loss: 11.3482, Validation Loss: 11.3657\n",
      "Epoch 3813/6000, Training Loss: 11.3482, Validation Loss: 11.3656\n",
      "Epoch 3814/6000, Training Loss: 11.3481, Validation Loss: 11.3656\n",
      "Epoch 3815/6000, Training Loss: 11.3480, Validation Loss: 11.3655\n",
      "Epoch 3816/6000, Training Loss: 11.3479, Validation Loss: 11.3654\n",
      "Epoch 3817/6000, Training Loss: 11.3478, Validation Loss: 11.3653\n",
      "Epoch 3818/6000, Training Loss: 11.3477, Validation Loss: 11.3652\n",
      "Epoch 3819/6000, Training Loss: 11.3476, Validation Loss: 11.3651\n",
      "Epoch 3820/6000, Training Loss: 11.3476, Validation Loss: 11.3651\n",
      "Epoch 3821/6000, Training Loss: 11.3475, Validation Loss: 11.3650\n",
      "Epoch 3822/6000, Training Loss: 11.3474, Validation Loss: 11.3649\n",
      "Epoch 3823/6000, Training Loss: 11.3473, Validation Loss: 11.3648\n",
      "Epoch 3824/6000, Training Loss: 11.3472, Validation Loss: 11.3647\n",
      "Epoch 3825/6000, Training Loss: 11.3471, Validation Loss: 11.3647\n",
      "Epoch 3826/6000, Training Loss: 11.3470, Validation Loss: 11.3646\n",
      "Epoch 3827/6000, Training Loss: 11.3470, Validation Loss: 11.3645\n",
      "Epoch 3828/6000, Training Loss: 11.3469, Validation Loss: 11.3644\n",
      "Epoch 3829/6000, Training Loss: 11.3468, Validation Loss: 11.3644\n",
      "Epoch 3830/6000, Training Loss: 11.3467, Validation Loss: 11.3643\n",
      "Epoch 3831/6000, Training Loss: 11.3466, Validation Loss: 11.3642\n",
      "Epoch 3832/6000, Training Loss: 11.3465, Validation Loss: 11.3641\n",
      "Epoch 3833/6000, Training Loss: 11.3465, Validation Loss: 11.3640\n",
      "Epoch 3834/6000, Training Loss: 11.3464, Validation Loss: 11.3640\n",
      "Epoch 3835/6000, Training Loss: 11.3463, Validation Loss: 11.3639\n",
      "Epoch 3836/6000, Training Loss: 11.3462, Validation Loss: 11.3638\n",
      "Epoch 3837/6000, Training Loss: 11.3461, Validation Loss: 11.3637\n",
      "Epoch 3838/6000, Training Loss: 11.3460, Validation Loss: 11.3637\n",
      "Epoch 3839/6000, Training Loss: 11.3460, Validation Loss: 11.3636\n",
      "Epoch 3840/6000, Training Loss: 11.3459, Validation Loss: 11.3635\n",
      "Epoch 3841/6000, Training Loss: 11.3458, Validation Loss: 11.3634\n",
      "Epoch 3842/6000, Training Loss: 11.3457, Validation Loss: 11.3634\n",
      "Epoch 3843/6000, Training Loss: 11.3456, Validation Loss: 11.3633\n",
      "Epoch 3844/6000, Training Loss: 11.3456, Validation Loss: 11.3632\n",
      "Epoch 3845/6000, Training Loss: 11.3455, Validation Loss: 11.3631\n",
      "Epoch 3846/6000, Training Loss: 11.3454, Validation Loss: 11.3630\n",
      "Epoch 3847/6000, Training Loss: 11.3453, Validation Loss: 11.3630\n",
      "Epoch 3848/6000, Training Loss: 11.3452, Validation Loss: 11.3629\n",
      "Epoch 3849/6000, Training Loss: 11.3452, Validation Loss: 11.3628\n",
      "Epoch 3850/6000, Training Loss: 11.3451, Validation Loss: 11.3627\n",
      "Epoch 3851/6000, Training Loss: 11.3450, Validation Loss: 11.3627\n",
      "Epoch 3852/6000, Training Loss: 11.3449, Validation Loss: 11.3626\n",
      "Epoch 3853/6000, Training Loss: 11.3448, Validation Loss: 11.3625\n",
      "Epoch 3854/6000, Training Loss: 11.3448, Validation Loss: 11.3624\n",
      "Epoch 3855/6000, Training Loss: 11.3447, Validation Loss: 11.3624\n",
      "Epoch 3856/6000, Training Loss: 11.3446, Validation Loss: 11.3623\n",
      "Epoch 3857/6000, Training Loss: 11.3445, Validation Loss: 11.3622\n",
      "Epoch 3858/6000, Training Loss: 11.3444, Validation Loss: 11.3622\n",
      "Epoch 3859/6000, Training Loss: 11.3444, Validation Loss: 11.3621\n",
      "Epoch 3860/6000, Training Loss: 11.3443, Validation Loss: 11.3620\n",
      "Epoch 3861/6000, Training Loss: 11.3442, Validation Loss: 11.3619\n",
      "Epoch 3862/6000, Training Loss: 11.3441, Validation Loss: 11.3619\n",
      "Epoch 3863/6000, Training Loss: 11.3441, Validation Loss: 11.3618\n",
      "Epoch 3864/6000, Training Loss: 11.3440, Validation Loss: 11.3617\n",
      "Epoch 3865/6000, Training Loss: 11.3439, Validation Loss: 11.3616\n",
      "Epoch 3866/6000, Training Loss: 11.3438, Validation Loss: 11.3616\n",
      "Epoch 3867/6000, Training Loss: 11.3438, Validation Loss: 11.3615\n",
      "Epoch 3868/6000, Training Loss: 11.3437, Validation Loss: 11.3614\n",
      "Epoch 3869/6000, Training Loss: 11.3436, Validation Loss: 11.3614\n",
      "Epoch 3870/6000, Training Loss: 11.3435, Validation Loss: 11.3613\n",
      "Epoch 3871/6000, Training Loss: 11.3435, Validation Loss: 11.3612\n",
      "Epoch 3872/6000, Training Loss: 11.3434, Validation Loss: 11.3612\n",
      "Epoch 3873/6000, Training Loss: 11.3433, Validation Loss: 11.3611\n",
      "Epoch 3874/6000, Training Loss: 11.3432, Validation Loss: 11.3610\n",
      "Epoch 3875/6000, Training Loss: 11.3432, Validation Loss: 11.3610\n",
      "Epoch 3876/6000, Training Loss: 11.3431, Validation Loss: 11.3609\n",
      "Epoch 3877/6000, Training Loss: 11.3430, Validation Loss: 11.3608\n",
      "Epoch 3878/6000, Training Loss: 11.3430, Validation Loss: 11.3608\n",
      "Epoch 3879/6000, Training Loss: 11.3429, Validation Loss: 11.3607\n",
      "Epoch 3880/6000, Training Loss: 11.3428, Validation Loss: 11.3607\n",
      "Epoch 3881/6000, Training Loss: 11.3427, Validation Loss: 11.3606\n",
      "Epoch 3882/6000, Training Loss: 11.3427, Validation Loss: 11.3605\n",
      "Epoch 3883/6000, Training Loss: 11.3426, Validation Loss: 11.3605\n",
      "Epoch 3884/6000, Training Loss: 11.3425, Validation Loss: 11.3604\n",
      "Epoch 3885/6000, Training Loss: 11.3425, Validation Loss: 11.3603\n",
      "Epoch 3886/6000, Training Loss: 11.3424, Validation Loss: 11.3603\n",
      "Epoch 3887/6000, Training Loss: 11.3423, Validation Loss: 11.3602\n",
      "Epoch 3888/6000, Training Loss: 11.3423, Validation Loss: 11.3602\n",
      "Epoch 3889/6000, Training Loss: 11.3422, Validation Loss: 11.3601\n",
      "Epoch 3890/6000, Training Loss: 11.3422, Validation Loss: 11.3600\n",
      "Epoch 3891/6000, Training Loss: 11.3421, Validation Loss: 11.3600\n",
      "Epoch 3892/6000, Training Loss: 11.3420, Validation Loss: 11.3599\n",
      "Epoch 3893/6000, Training Loss: 11.3420, Validation Loss: 11.3599\n",
      "Epoch 3894/6000, Training Loss: 11.3419, Validation Loss: 11.3598\n",
      "Epoch 3895/6000, Training Loss: 11.3418, Validation Loss: 11.3598\n",
      "Epoch 3896/6000, Training Loss: 11.3418, Validation Loss: 11.3597\n",
      "Epoch 3897/6000, Training Loss: 11.3417, Validation Loss: 11.3597\n",
      "Epoch 3898/6000, Training Loss: 11.3417, Validation Loss: 11.3596\n",
      "Epoch 3899/6000, Training Loss: 11.3416, Validation Loss: 11.3596\n",
      "Epoch 3900/6000, Training Loss: 11.3415, Validation Loss: 11.3595\n",
      "Epoch 3901/6000, Training Loss: 11.3415, Validation Loss: 11.3595\n",
      "Epoch 3902/6000, Training Loss: 11.3414, Validation Loss: 11.3594\n",
      "Epoch 3903/6000, Training Loss: 11.3414, Validation Loss: 11.3593\n",
      "Epoch 3904/6000, Training Loss: 11.3413, Validation Loss: 11.3593\n",
      "Epoch 3905/6000, Training Loss: 11.3413, Validation Loss: 11.3593\n",
      "Epoch 3906/6000, Training Loss: 11.3412, Validation Loss: 11.3592\n",
      "Epoch 3907/6000, Training Loss: 11.3412, Validation Loss: 11.3592\n",
      "Epoch 3908/6000, Training Loss: 11.3411, Validation Loss: 11.3591\n",
      "Epoch 3909/6000, Training Loss: 11.3411, Validation Loss: 11.3591\n",
      "Epoch 3910/6000, Training Loss: 11.3410, Validation Loss: 11.3590\n",
      "Epoch 3911/6000, Training Loss: 11.3409, Validation Loss: 11.3590\n",
      "Epoch 3912/6000, Training Loss: 11.3409, Validation Loss: 11.3589\n",
      "Epoch 3913/6000, Training Loss: 11.3408, Validation Loss: 11.3589\n",
      "Epoch 3914/6000, Training Loss: 11.3408, Validation Loss: 11.3588\n",
      "Epoch 3915/6000, Training Loss: 11.3407, Validation Loss: 11.3588\n",
      "Epoch 3916/6000, Training Loss: 11.3407, Validation Loss: 11.3587\n",
      "Epoch 3917/6000, Training Loss: 11.3406, Validation Loss: 11.3587\n",
      "Epoch 3918/6000, Training Loss: 11.3406, Validation Loss: 11.3586\n",
      "Epoch 3919/6000, Training Loss: 11.3405, Validation Loss: 11.3586\n",
      "Epoch 3920/6000, Training Loss: 11.3405, Validation Loss: 11.3586\n",
      "Epoch 3921/6000, Training Loss: 11.3404, Validation Loss: 11.3585\n",
      "Epoch 3922/6000, Training Loss: 11.3404, Validation Loss: 11.3585\n",
      "Epoch 3923/6000, Training Loss: 11.3404, Validation Loss: 11.3584\n",
      "Epoch 3924/6000, Training Loss: 11.3403, Validation Loss: 11.3584\n",
      "Epoch 3925/6000, Training Loss: 11.3403, Validation Loss: 11.3583\n",
      "Epoch 3926/6000, Training Loss: 11.3402, Validation Loss: 11.3583\n",
      "Epoch 3927/6000, Training Loss: 11.3402, Validation Loss: 11.3583\n",
      "Epoch 3928/6000, Training Loss: 11.3401, Validation Loss: 11.3582\n",
      "Epoch 3929/6000, Training Loss: 11.3401, Validation Loss: 11.3582\n",
      "Epoch 3930/6000, Training Loss: 11.3400, Validation Loss: 11.3581\n",
      "Epoch 3931/6000, Training Loss: 11.3400, Validation Loss: 11.3581\n",
      "Epoch 3932/6000, Training Loss: 11.3399, Validation Loss: 11.3581\n",
      "Epoch 3933/6000, Training Loss: 11.3399, Validation Loss: 11.3580\n",
      "Epoch 3934/6000, Training Loss: 11.3399, Validation Loss: 11.3580\n",
      "Epoch 3935/6000, Training Loss: 11.3398, Validation Loss: 11.3579\n",
      "Epoch 3936/6000, Training Loss: 11.3398, Validation Loss: 11.3579\n",
      "Epoch 3937/6000, Training Loss: 11.3397, Validation Loss: 11.3579\n",
      "Epoch 3938/6000, Training Loss: 11.3397, Validation Loss: 11.3578\n",
      "Epoch 3939/6000, Training Loss: 11.3396, Validation Loss: 11.3578\n",
      "Epoch 3940/6000, Training Loss: 11.3396, Validation Loss: 11.3578\n",
      "Epoch 3941/6000, Training Loss: 11.3396, Validation Loss: 11.3577\n",
      "Epoch 3942/6000, Training Loss: 11.3395, Validation Loss: 11.3577\n",
      "Epoch 3943/6000, Training Loss: 11.3395, Validation Loss: 11.3576\n",
      "Epoch 3944/6000, Training Loss: 11.3394, Validation Loss: 11.3576\n",
      "Epoch 3945/6000, Training Loss: 11.3394, Validation Loss: 11.3576\n",
      "Epoch 3946/6000, Training Loss: 11.3394, Validation Loss: 11.3575\n",
      "Epoch 3947/6000, Training Loss: 11.3393, Validation Loss: 11.3575\n",
      "Epoch 3948/6000, Training Loss: 11.3393, Validation Loss: 11.3575\n",
      "Epoch 3949/6000, Training Loss: 11.3392, Validation Loss: 11.3574\n",
      "Epoch 3950/6000, Training Loss: 11.3392, Validation Loss: 11.3574\n",
      "Epoch 3951/6000, Training Loss: 11.3391, Validation Loss: 11.3574\n",
      "Epoch 3952/6000, Training Loss: 11.3391, Validation Loss: 11.3573\n",
      "Epoch 3953/6000, Training Loss: 11.3391, Validation Loss: 11.3573\n",
      "Epoch 3954/6000, Training Loss: 11.3390, Validation Loss: 11.3572\n",
      "Epoch 3955/6000, Training Loss: 11.3390, Validation Loss: 11.3572\n",
      "Epoch 3956/6000, Training Loss: 11.3390, Validation Loss: 11.3572\n",
      "Epoch 3957/6000, Training Loss: 11.3389, Validation Loss: 11.3571\n",
      "Epoch 3958/6000, Training Loss: 11.3389, Validation Loss: 11.3571\n",
      "Epoch 3959/6000, Training Loss: 11.3388, Validation Loss: 11.3571\n",
      "Epoch 3960/6000, Training Loss: 11.3388, Validation Loss: 11.3570\n",
      "Epoch 3961/6000, Training Loss: 11.3388, Validation Loss: 11.3570\n",
      "Epoch 3962/6000, Training Loss: 11.3387, Validation Loss: 11.3570\n",
      "Epoch 3963/6000, Training Loss: 11.3387, Validation Loss: 11.3569\n",
      "Epoch 3964/6000, Training Loss: 11.3386, Validation Loss: 11.3569\n",
      "Epoch 3965/6000, Training Loss: 11.3386, Validation Loss: 11.3569\n",
      "Epoch 3966/6000, Training Loss: 11.3386, Validation Loss: 11.3568\n",
      "Epoch 3967/6000, Training Loss: 11.3385, Validation Loss: 11.3568\n",
      "Epoch 3968/6000, Training Loss: 11.3385, Validation Loss: 11.3568\n",
      "Epoch 3969/6000, Training Loss: 11.3384, Validation Loss: 11.3567\n",
      "Epoch 3970/6000, Training Loss: 11.3384, Validation Loss: 11.3567\n",
      "Epoch 3971/6000, Training Loss: 11.3384, Validation Loss: 11.3567\n",
      "Epoch 3972/6000, Training Loss: 11.3383, Validation Loss: 11.3566\n",
      "Epoch 3973/6000, Training Loss: 11.3383, Validation Loss: 11.3566\n",
      "Epoch 3974/6000, Training Loss: 11.3383, Validation Loss: 11.3566\n",
      "Epoch 3975/6000, Training Loss: 11.3382, Validation Loss: 11.3565\n",
      "Epoch 3976/6000, Training Loss: 11.3382, Validation Loss: 11.3565\n",
      "Epoch 3977/6000, Training Loss: 11.3381, Validation Loss: 11.3565\n",
      "Epoch 3978/6000, Training Loss: 11.3381, Validation Loss: 11.3564\n",
      "Epoch 3979/6000, Training Loss: 11.3381, Validation Loss: 11.3564\n",
      "Epoch 3980/6000, Training Loss: 11.3380, Validation Loss: 11.3564\n",
      "Epoch 3981/6000, Training Loss: 11.3380, Validation Loss: 11.3563\n",
      "Epoch 3982/6000, Training Loss: 11.3380, Validation Loss: 11.3563\n",
      "Epoch 3983/6000, Training Loss: 11.3379, Validation Loss: 11.3563\n",
      "Epoch 3984/6000, Training Loss: 11.3379, Validation Loss: 11.3562\n",
      "Epoch 3985/6000, Training Loss: 11.3378, Validation Loss: 11.3562\n",
      "Epoch 3986/6000, Training Loss: 11.3378, Validation Loss: 11.3562\n",
      "Epoch 3987/6000, Training Loss: 11.3378, Validation Loss: 11.3561\n",
      "Epoch 3988/6000, Training Loss: 11.3377, Validation Loss: 11.3561\n",
      "Epoch 3989/6000, Training Loss: 11.3377, Validation Loss: 11.3561\n",
      "Epoch 3990/6000, Training Loss: 11.3377, Validation Loss: 11.3560\n",
      "Epoch 3991/6000, Training Loss: 11.3376, Validation Loss: 11.3560\n",
      "Epoch 3992/6000, Training Loss: 11.3376, Validation Loss: 11.3560\n",
      "Epoch 3993/6000, Training Loss: 11.3375, Validation Loss: 11.3559\n",
      "Epoch 3994/6000, Training Loss: 11.3375, Validation Loss: 11.3559\n",
      "Epoch 3995/6000, Training Loss: 11.3375, Validation Loss: 11.3559\n",
      "Epoch 3996/6000, Training Loss: 11.3374, Validation Loss: 11.3558\n",
      "Epoch 3997/6000, Training Loss: 11.3374, Validation Loss: 11.3558\n",
      "Epoch 3998/6000, Training Loss: 11.3374, Validation Loss: 11.3558\n",
      "Epoch 3999/6000, Training Loss: 11.3373, Validation Loss: 11.3557\n",
      "Epoch 4000/6000, Training Loss: 11.3373, Validation Loss: 11.3557\n",
      "Epoch 4001/6000, Training Loss: 11.3373, Validation Loss: 11.3557\n",
      "Epoch 4002/6000, Training Loss: 11.3372, Validation Loss: 11.3556\n",
      "Epoch 4003/6000, Training Loss: 11.3372, Validation Loss: 11.3556\n",
      "Epoch 4004/6000, Training Loss: 11.3371, Validation Loss: 11.3556\n",
      "Epoch 4005/6000, Training Loss: 11.3371, Validation Loss: 11.3556\n",
      "Epoch 4006/6000, Training Loss: 11.3371, Validation Loss: 11.3555\n",
      "Epoch 4007/6000, Training Loss: 11.3370, Validation Loss: 11.3555\n",
      "Epoch 4008/6000, Training Loss: 11.3370, Validation Loss: 11.3555\n",
      "Epoch 4009/6000, Training Loss: 11.3370, Validation Loss: 11.3554\n",
      "Epoch 4010/6000, Training Loss: 11.3369, Validation Loss: 11.3554\n",
      "Epoch 4011/6000, Training Loss: 11.3369, Validation Loss: 11.3554\n",
      "Epoch 4012/6000, Training Loss: 11.3369, Validation Loss: 11.3553\n",
      "Epoch 4013/6000, Training Loss: 11.3368, Validation Loss: 11.3553\n",
      "Epoch 4014/6000, Training Loss: 11.3368, Validation Loss: 11.3553\n",
      "Epoch 4015/6000, Training Loss: 11.3368, Validation Loss: 11.3552\n",
      "Epoch 4016/6000, Training Loss: 11.3367, Validation Loss: 11.3552\n",
      "Epoch 4017/6000, Training Loss: 11.3367, Validation Loss: 11.3552\n",
      "Epoch 4018/6000, Training Loss: 11.3367, Validation Loss: 11.3552\n",
      "Epoch 4019/6000, Training Loss: 11.3366, Validation Loss: 11.3551\n",
      "Epoch 4020/6000, Training Loss: 11.3366, Validation Loss: 11.3551\n",
      "Epoch 4021/6000, Training Loss: 11.3366, Validation Loss: 11.3551\n",
      "Epoch 4022/6000, Training Loss: 11.3365, Validation Loss: 11.3550\n",
      "Epoch 4023/6000, Training Loss: 11.3365, Validation Loss: 11.3550\n",
      "Epoch 4024/6000, Training Loss: 11.3365, Validation Loss: 11.3550\n",
      "Epoch 4025/6000, Training Loss: 11.3364, Validation Loss: 11.3550\n",
      "Epoch 4026/6000, Training Loss: 11.3364, Validation Loss: 11.3549\n",
      "Epoch 4027/6000, Training Loss: 11.3364, Validation Loss: 11.3549\n",
      "Epoch 4028/6000, Training Loss: 11.3363, Validation Loss: 11.3549\n",
      "Epoch 4029/6000, Training Loss: 11.3363, Validation Loss: 11.3548\n",
      "Epoch 4030/6000, Training Loss: 11.3363, Validation Loss: 11.3548\n",
      "Epoch 4031/6000, Training Loss: 11.3362, Validation Loss: 11.3548\n",
      "Epoch 4032/6000, Training Loss: 11.3362, Validation Loss: 11.3547\n",
      "Epoch 4033/6000, Training Loss: 11.3362, Validation Loss: 11.3547\n",
      "Epoch 4034/6000, Training Loss: 11.3361, Validation Loss: 11.3547\n",
      "Epoch 4035/6000, Training Loss: 11.3361, Validation Loss: 11.3547\n",
      "Epoch 4036/6000, Training Loss: 11.3361, Validation Loss: 11.3546\n",
      "Epoch 4037/6000, Training Loss: 11.3360, Validation Loss: 11.3546\n",
      "Epoch 4038/6000, Training Loss: 11.3360, Validation Loss: 11.3546\n",
      "Epoch 4039/6000, Training Loss: 11.3360, Validation Loss: 11.3546\n",
      "Epoch 4040/6000, Training Loss: 11.3359, Validation Loss: 11.3545\n",
      "Epoch 4041/6000, Training Loss: 11.3359, Validation Loss: 11.3545\n",
      "Epoch 4042/6000, Training Loss: 11.3359, Validation Loss: 11.3545\n",
      "Epoch 4043/6000, Training Loss: 11.3358, Validation Loss: 11.3544\n",
      "Epoch 4044/6000, Training Loss: 11.3358, Validation Loss: 11.3544\n",
      "Epoch 4045/6000, Training Loss: 11.3358, Validation Loss: 11.3544\n",
      "Epoch 4046/6000, Training Loss: 11.3358, Validation Loss: 11.3544\n",
      "Epoch 4047/6000, Training Loss: 11.3357, Validation Loss: 11.3543\n",
      "Epoch 4048/6000, Training Loss: 11.3357, Validation Loss: 11.3543\n",
      "Epoch 4049/6000, Training Loss: 11.3357, Validation Loss: 11.3543\n",
      "Epoch 4050/6000, Training Loss: 11.3356, Validation Loss: 11.3542\n",
      "Epoch 4051/6000, Training Loss: 11.3356, Validation Loss: 11.3542\n",
      "Epoch 4052/6000, Training Loss: 11.3356, Validation Loss: 11.3542\n",
      "Epoch 4053/6000, Training Loss: 11.3355, Validation Loss: 11.3542\n",
      "Epoch 4054/6000, Training Loss: 11.3355, Validation Loss: 11.3541\n",
      "Epoch 4055/6000, Training Loss: 11.3355, Validation Loss: 11.3541\n",
      "Epoch 4056/6000, Training Loss: 11.3355, Validation Loss: 11.3541\n",
      "Epoch 4057/6000, Training Loss: 11.3354, Validation Loss: 11.3541\n",
      "Epoch 4058/6000, Training Loss: 11.3354, Validation Loss: 11.3540\n",
      "Epoch 4059/6000, Training Loss: 11.3354, Validation Loss: 11.3540\n",
      "Epoch 4060/6000, Training Loss: 11.3353, Validation Loss: 11.3540\n",
      "Epoch 4061/6000, Training Loss: 11.3353, Validation Loss: 11.3540\n",
      "Epoch 4062/6000, Training Loss: 11.3353, Validation Loss: 11.3539\n",
      "Epoch 4063/6000, Training Loss: 11.3352, Validation Loss: 11.3539\n",
      "Epoch 4064/6000, Training Loss: 11.3352, Validation Loss: 11.3539\n",
      "Epoch 4065/6000, Training Loss: 11.3352, Validation Loss: 11.3539\n",
      "Epoch 4066/6000, Training Loss: 11.3352, Validation Loss: 11.3538\n",
      "Epoch 4067/6000, Training Loss: 11.3351, Validation Loss: 11.3538\n",
      "Epoch 4068/6000, Training Loss: 11.3351, Validation Loss: 11.3538\n",
      "Epoch 4069/6000, Training Loss: 11.3351, Validation Loss: 11.3538\n",
      "Epoch 4070/6000, Training Loss: 11.3350, Validation Loss: 11.3537\n",
      "Epoch 4071/6000, Training Loss: 11.3350, Validation Loss: 11.3537\n",
      "Epoch 4072/6000, Training Loss: 11.3350, Validation Loss: 11.3537\n",
      "Epoch 4073/6000, Training Loss: 11.3350, Validation Loss: 11.3537\n",
      "Epoch 4074/6000, Training Loss: 11.3349, Validation Loss: 11.3536\n",
      "Epoch 4075/6000, Training Loss: 11.3349, Validation Loss: 11.3536\n",
      "Epoch 4076/6000, Training Loss: 11.3349, Validation Loss: 11.3536\n",
      "Epoch 4077/6000, Training Loss: 11.3349, Validation Loss: 11.3536\n",
      "Epoch 4078/6000, Training Loss: 11.3348, Validation Loss: 11.3535\n",
      "Epoch 4079/6000, Training Loss: 11.3348, Validation Loss: 11.3535\n",
      "Epoch 4080/6000, Training Loss: 11.3348, Validation Loss: 11.3535\n",
      "Epoch 4081/6000, Training Loss: 11.3347, Validation Loss: 11.3535\n",
      "Epoch 4082/6000, Training Loss: 11.3347, Validation Loss: 11.3534\n",
      "Epoch 4083/6000, Training Loss: 11.3347, Validation Loss: 11.3534\n",
      "Epoch 4084/6000, Training Loss: 11.3347, Validation Loss: 11.3534\n",
      "Epoch 4085/6000, Training Loss: 11.3346, Validation Loss: 11.3534\n",
      "Epoch 4086/6000, Training Loss: 11.3346, Validation Loss: 11.3533\n",
      "Epoch 4087/6000, Training Loss: 11.3346, Validation Loss: 11.3533\n",
      "Epoch 4088/6000, Training Loss: 11.3346, Validation Loss: 11.3533\n",
      "Epoch 4089/6000, Training Loss: 11.3345, Validation Loss: 11.3533\n",
      "Epoch 4090/6000, Training Loss: 11.3345, Validation Loss: 11.3532\n",
      "Epoch 4091/6000, Training Loss: 11.3345, Validation Loss: 11.3532\n",
      "Epoch 4092/6000, Training Loss: 11.3345, Validation Loss: 11.3532\n",
      "Epoch 4093/6000, Training Loss: 11.3344, Validation Loss: 11.3532\n",
      "Epoch 4094/6000, Training Loss: 11.3344, Validation Loss: 11.3532\n",
      "Epoch 4095/6000, Training Loss: 11.3344, Validation Loss: 11.3531\n",
      "Epoch 4096/6000, Training Loss: 11.3344, Validation Loss: 11.3531\n",
      "Epoch 4097/6000, Training Loss: 11.3343, Validation Loss: 11.3531\n",
      "Epoch 4098/6000, Training Loss: 11.3343, Validation Loss: 11.3531\n",
      "Epoch 4099/6000, Training Loss: 11.3343, Validation Loss: 11.3530\n",
      "Epoch 4100/6000, Training Loss: 11.3343, Validation Loss: 11.3530\n",
      "Epoch 4101/6000, Training Loss: 11.3342, Validation Loss: 11.3530\n",
      "Epoch 4102/6000, Training Loss: 11.3342, Validation Loss: 11.3530\n",
      "Epoch 4103/6000, Training Loss: 11.3342, Validation Loss: 11.3530\n",
      "Epoch 4104/6000, Training Loss: 11.3342, Validation Loss: 11.3529\n",
      "Epoch 4105/6000, Training Loss: 11.3341, Validation Loss: 11.3529\n",
      "Epoch 4106/6000, Training Loss: 11.3341, Validation Loss: 11.3529\n",
      "Epoch 4107/6000, Training Loss: 11.3341, Validation Loss: 11.3529\n",
      "Epoch 4108/6000, Training Loss: 11.3341, Validation Loss: 11.3529\n",
      "Epoch 4109/6000, Training Loss: 11.3340, Validation Loss: 11.3528\n",
      "Epoch 4110/6000, Training Loss: 11.3340, Validation Loss: 11.3528\n",
      "Epoch 4111/6000, Training Loss: 11.3340, Validation Loss: 11.3528\n",
      "Epoch 4112/6000, Training Loss: 11.3340, Validation Loss: 11.3528\n",
      "Epoch 4113/6000, Training Loss: 11.3339, Validation Loss: 11.3527\n",
      "Epoch 4114/6000, Training Loss: 11.3339, Validation Loss: 11.3527\n",
      "Epoch 4115/6000, Training Loss: 11.3339, Validation Loss: 11.3527\n",
      "Epoch 4116/6000, Training Loss: 11.3339, Validation Loss: 11.3527\n",
      "Epoch 4117/6000, Training Loss: 11.3339, Validation Loss: 11.3527\n",
      "Epoch 4118/6000, Training Loss: 11.3338, Validation Loss: 11.3526\n",
      "Epoch 4119/6000, Training Loss: 11.3338, Validation Loss: 11.3526\n",
      "Epoch 4120/6000, Training Loss: 11.3338, Validation Loss: 11.3526\n",
      "Epoch 4121/6000, Training Loss: 11.3338, Validation Loss: 11.3526\n",
      "Epoch 4122/6000, Training Loss: 11.3337, Validation Loss: 11.3526\n",
      "Epoch 4123/6000, Training Loss: 11.3337, Validation Loss: 11.3525\n",
      "Epoch 4124/6000, Training Loss: 11.3337, Validation Loss: 11.3525\n",
      "Epoch 4125/6000, Training Loss: 11.3337, Validation Loss: 11.3525\n",
      "Epoch 4126/6000, Training Loss: 11.3336, Validation Loss: 11.3525\n",
      "Epoch 4127/6000, Training Loss: 11.3336, Validation Loss: 11.3525\n",
      "Epoch 4128/6000, Training Loss: 11.3336, Validation Loss: 11.3524\n",
      "Epoch 4129/6000, Training Loss: 11.3336, Validation Loss: 11.3524\n",
      "Epoch 4130/6000, Training Loss: 11.3336, Validation Loss: 11.3524\n",
      "Epoch 4131/6000, Training Loss: 11.3335, Validation Loss: 11.3524\n",
      "Epoch 4132/6000, Training Loss: 11.3335, Validation Loss: 11.3524\n",
      "Epoch 4133/6000, Training Loss: 11.3335, Validation Loss: 11.3523\n",
      "Epoch 4134/6000, Training Loss: 11.3335, Validation Loss: 11.3523\n",
      "Epoch 4135/6000, Training Loss: 11.3334, Validation Loss: 11.3523\n",
      "Epoch 4136/6000, Training Loss: 11.3334, Validation Loss: 11.3523\n",
      "Epoch 4137/6000, Training Loss: 11.3334, Validation Loss: 11.3523\n",
      "Epoch 4138/6000, Training Loss: 11.3334, Validation Loss: 11.3522\n",
      "Epoch 4139/6000, Training Loss: 11.3334, Validation Loss: 11.3522\n",
      "Epoch 4140/6000, Training Loss: 11.3333, Validation Loss: 11.3522\n",
      "Epoch 4141/6000, Training Loss: 11.3333, Validation Loss: 11.3522\n",
      "Epoch 4142/6000, Training Loss: 11.3333, Validation Loss: 11.3522\n",
      "Epoch 4143/6000, Training Loss: 11.3333, Validation Loss: 11.3521\n",
      "Epoch 4144/6000, Training Loss: 11.3333, Validation Loss: 11.3521\n",
      "Epoch 4145/6000, Training Loss: 11.3332, Validation Loss: 11.3521\n",
      "Epoch 4146/6000, Training Loss: 11.3332, Validation Loss: 11.3521\n",
      "Epoch 4147/6000, Training Loss: 11.3332, Validation Loss: 11.3521\n",
      "Epoch 4148/6000, Training Loss: 11.3332, Validation Loss: 11.3521\n",
      "Epoch 4149/6000, Training Loss: 11.3331, Validation Loss: 11.3520\n",
      "Epoch 4150/6000, Training Loss: 11.3331, Validation Loss: 11.3520\n",
      "Epoch 4151/6000, Training Loss: 11.3331, Validation Loss: 11.3520\n",
      "Epoch 4152/6000, Training Loss: 11.3331, Validation Loss: 11.3520\n",
      "Epoch 4153/6000, Training Loss: 11.3331, Validation Loss: 11.3520\n",
      "Epoch 4154/6000, Training Loss: 11.3330, Validation Loss: 11.3519\n",
      "Epoch 4155/6000, Training Loss: 11.3330, Validation Loss: 11.3519\n",
      "Epoch 4156/6000, Training Loss: 11.3330, Validation Loss: 11.3519\n",
      "Epoch 4157/6000, Training Loss: 11.3330, Validation Loss: 11.3519\n",
      "Epoch 4158/6000, Training Loss: 11.3330, Validation Loss: 11.3519\n",
      "Epoch 4159/6000, Training Loss: 11.3329, Validation Loss: 11.3519\n",
      "Epoch 4160/6000, Training Loss: 11.3329, Validation Loss: 11.3518\n",
      "Epoch 4161/6000, Training Loss: 11.3329, Validation Loss: 11.3518\n",
      "Epoch 4162/6000, Training Loss: 11.3329, Validation Loss: 11.3518\n",
      "Epoch 4163/6000, Training Loss: 11.3329, Validation Loss: 11.3518\n",
      "Epoch 4164/6000, Training Loss: 11.3328, Validation Loss: 11.3518\n",
      "Epoch 4165/6000, Training Loss: 11.3328, Validation Loss: 11.3517\n",
      "Epoch 4166/6000, Training Loss: 11.3328, Validation Loss: 11.3517\n",
      "Epoch 4167/6000, Training Loss: 11.3328, Validation Loss: 11.3517\n",
      "Epoch 4168/6000, Training Loss: 11.3328, Validation Loss: 11.3517\n",
      "Epoch 4169/6000, Training Loss: 11.3327, Validation Loss: 11.3517\n",
      "Epoch 4170/6000, Training Loss: 11.3327, Validation Loss: 11.3517\n",
      "Epoch 4171/6000, Training Loss: 11.3327, Validation Loss: 11.3516\n",
      "Epoch 4172/6000, Training Loss: 11.3327, Validation Loss: 11.3516\n",
      "Epoch 4173/6000, Training Loss: 11.3327, Validation Loss: 11.3516\n",
      "Epoch 4174/6000, Training Loss: 11.3326, Validation Loss: 11.3516\n",
      "Epoch 4175/6000, Training Loss: 11.3326, Validation Loss: 11.3516\n",
      "Epoch 4176/6000, Training Loss: 11.3326, Validation Loss: 11.3515\n",
      "Epoch 4177/6000, Training Loss: 11.3326, Validation Loss: 11.3515\n",
      "Epoch 4178/6000, Training Loss: 11.3326, Validation Loss: 11.3515\n",
      "Epoch 4179/6000, Training Loss: 11.3326, Validation Loss: 11.3515\n",
      "Epoch 4180/6000, Training Loss: 11.3325, Validation Loss: 11.3515\n",
      "Epoch 4181/6000, Training Loss: 11.3325, Validation Loss: 11.3515\n",
      "Epoch 4182/6000, Training Loss: 11.3325, Validation Loss: 11.3514\n",
      "Epoch 4183/6000, Training Loss: 11.3325, Validation Loss: 11.3514\n",
      "Epoch 4184/6000, Training Loss: 11.3325, Validation Loss: 11.3514\n",
      "Epoch 4185/6000, Training Loss: 11.3324, Validation Loss: 11.3514\n",
      "Epoch 4186/6000, Training Loss: 11.3324, Validation Loss: 11.3514\n",
      "Epoch 4187/6000, Training Loss: 11.3324, Validation Loss: 11.3514\n",
      "Epoch 4188/6000, Training Loss: 11.3324, Validation Loss: 11.3513\n",
      "Epoch 4189/6000, Training Loss: 11.3324, Validation Loss: 11.3513\n",
      "Epoch 4190/6000, Training Loss: 11.3323, Validation Loss: 11.3513\n",
      "Epoch 4191/6000, Training Loss: 11.3323, Validation Loss: 11.3513\n",
      "Epoch 4192/6000, Training Loss: 11.3323, Validation Loss: 11.3513\n",
      "Epoch 4193/6000, Training Loss: 11.3323, Validation Loss: 11.3513\n",
      "Epoch 4194/6000, Training Loss: 11.3323, Validation Loss: 11.3512\n",
      "Epoch 4195/6000, Training Loss: 11.3323, Validation Loss: 11.3512\n",
      "Epoch 4196/6000, Training Loss: 11.3322, Validation Loss: 11.3512\n",
      "Epoch 4197/6000, Training Loss: 11.3322, Validation Loss: 11.3512\n",
      "Epoch 4198/6000, Training Loss: 11.3322, Validation Loss: 11.3512\n",
      "Epoch 4199/6000, Training Loss: 11.3322, Validation Loss: 11.3512\n",
      "Epoch 4200/6000, Training Loss: 11.3322, Validation Loss: 11.3511\n",
      "Epoch 4201/6000, Training Loss: 11.3321, Validation Loss: 11.3511\n",
      "Epoch 4202/6000, Training Loss: 11.3321, Validation Loss: 11.3511\n",
      "Epoch 4203/6000, Training Loss: 11.3321, Validation Loss: 11.3511\n",
      "Epoch 4204/6000, Training Loss: 11.3321, Validation Loss: 11.3511\n",
      "Epoch 4205/6000, Training Loss: 11.3321, Validation Loss: 11.3511\n",
      "Epoch 4206/6000, Training Loss: 11.3320, Validation Loss: 11.3510\n",
      "Epoch 4207/6000, Training Loss: 11.3320, Validation Loss: 11.3510\n",
      "Epoch 4208/6000, Training Loss: 11.3320, Validation Loss: 11.3510\n",
      "Epoch 4209/6000, Training Loss: 11.3320, Validation Loss: 11.3510\n",
      "Epoch 4210/6000, Training Loss: 11.3320, Validation Loss: 11.3510\n",
      "Epoch 4211/6000, Training Loss: 11.3320, Validation Loss: 11.3510\n",
      "Epoch 4212/6000, Training Loss: 11.3319, Validation Loss: 11.3509\n",
      "Epoch 4213/6000, Training Loss: 11.3319, Validation Loss: 11.3509\n",
      "Epoch 4214/6000, Training Loss: 11.3319, Validation Loss: 11.3509\n",
      "Epoch 4215/6000, Training Loss: 11.3319, Validation Loss: 11.3509\n",
      "Epoch 4216/6000, Training Loss: 11.3319, Validation Loss: 11.3509\n",
      "Epoch 4217/6000, Training Loss: 11.3319, Validation Loss: 11.3509\n",
      "Epoch 4218/6000, Training Loss: 11.3318, Validation Loss: 11.3508\n",
      "Epoch 4219/6000, Training Loss: 11.3318, Validation Loss: 11.3508\n",
      "Epoch 4220/6000, Training Loss: 11.3318, Validation Loss: 11.3508\n",
      "Epoch 4221/6000, Training Loss: 11.3318, Validation Loss: 11.3508\n",
      "Epoch 4222/6000, Training Loss: 11.3318, Validation Loss: 11.3508\n",
      "Epoch 4223/6000, Training Loss: 11.3318, Validation Loss: 11.3508\n",
      "Epoch 4224/6000, Training Loss: 11.3317, Validation Loss: 11.3508\n",
      "Epoch 4225/6000, Training Loss: 11.3317, Validation Loss: 11.3507\n",
      "Epoch 4226/6000, Training Loss: 11.3317, Validation Loss: 11.3507\n",
      "Epoch 4227/6000, Training Loss: 11.3317, Validation Loss: 11.3507\n",
      "Epoch 4228/6000, Training Loss: 11.3317, Validation Loss: 11.3507\n",
      "Epoch 4229/6000, Training Loss: 11.3317, Validation Loss: 11.3507\n",
      "Epoch 4230/6000, Training Loss: 11.3316, Validation Loss: 11.3507\n",
      "Epoch 4231/6000, Training Loss: 11.3316, Validation Loss: 11.3507\n",
      "Epoch 4232/6000, Training Loss: 11.3316, Validation Loss: 11.3506\n",
      "Epoch 4233/6000, Training Loss: 11.3316, Validation Loss: 11.3506\n",
      "Epoch 4234/6000, Training Loss: 11.3316, Validation Loss: 11.3506\n",
      "Epoch 4235/6000, Training Loss: 11.3316, Validation Loss: 11.3506\n",
      "Epoch 4236/6000, Training Loss: 11.3315, Validation Loss: 11.3506\n",
      "Epoch 4237/6000, Training Loss: 11.3315, Validation Loss: 11.3506\n",
      "Epoch 4238/6000, Training Loss: 11.3315, Validation Loss: 11.3505\n",
      "Epoch 4239/6000, Training Loss: 11.3315, Validation Loss: 11.3505\n",
      "Epoch 4240/6000, Training Loss: 11.3315, Validation Loss: 11.3505\n",
      "Epoch 4241/6000, Training Loss: 11.3315, Validation Loss: 11.3505\n",
      "Epoch 4242/6000, Training Loss: 11.3315, Validation Loss: 11.3505\n",
      "Epoch 4243/6000, Training Loss: 11.3314, Validation Loss: 11.3505\n",
      "Epoch 4244/6000, Training Loss: 11.3314, Validation Loss: 11.3505\n",
      "Epoch 4245/6000, Training Loss: 11.3314, Validation Loss: 11.3505\n",
      "Epoch 4246/6000, Training Loss: 11.3314, Validation Loss: 11.3504\n",
      "Epoch 4247/6000, Training Loss: 11.3314, Validation Loss: 11.3504\n",
      "Epoch 4248/6000, Training Loss: 11.3314, Validation Loss: 11.3504\n",
      "Epoch 4249/6000, Training Loss: 11.3314, Validation Loss: 11.3504\n",
      "Epoch 4250/6000, Training Loss: 11.3313, Validation Loss: 11.3504\n",
      "Epoch 4251/6000, Training Loss: 11.3313, Validation Loss: 11.3504\n",
      "Epoch 4252/6000, Training Loss: 11.3313, Validation Loss: 11.3504\n",
      "Epoch 4253/6000, Training Loss: 11.3313, Validation Loss: 11.3504\n",
      "Epoch 4254/6000, Training Loss: 11.3313, Validation Loss: 11.3503\n",
      "Epoch 4255/6000, Training Loss: 11.3313, Validation Loss: 11.3503\n",
      "Epoch 4256/6000, Training Loss: 11.3313, Validation Loss: 11.3503\n",
      "Epoch 4257/6000, Training Loss: 11.3312, Validation Loss: 11.3503\n",
      "Epoch 4258/6000, Training Loss: 11.3312, Validation Loss: 11.3503\n",
      "Epoch 4259/6000, Training Loss: 11.3312, Validation Loss: 11.3503\n",
      "Epoch 4260/6000, Training Loss: 11.3312, Validation Loss: 11.3503\n",
      "Epoch 4261/6000, Training Loss: 11.3312, Validation Loss: 11.3503\n",
      "Epoch 4262/6000, Training Loss: 11.3312, Validation Loss: 11.3503\n",
      "Epoch 4263/6000, Training Loss: 11.3312, Validation Loss: 11.3502\n",
      "Epoch 4264/6000, Training Loss: 11.3312, Validation Loss: 11.3502\n",
      "Epoch 4265/6000, Training Loss: 11.3311, Validation Loss: 11.3502\n",
      "Epoch 4266/6000, Training Loss: 11.3311, Validation Loss: 11.3502\n",
      "Epoch 4267/6000, Training Loss: 11.3311, Validation Loss: 11.3502\n",
      "Epoch 4268/6000, Training Loss: 11.3311, Validation Loss: 11.3502\n",
      "Epoch 4269/6000, Training Loss: 11.3311, Validation Loss: 11.3502\n",
      "Epoch 4270/6000, Training Loss: 11.3311, Validation Loss: 11.3502\n",
      "Epoch 4271/6000, Training Loss: 11.3311, Validation Loss: 11.3502\n",
      "Epoch 4272/6000, Training Loss: 11.3311, Validation Loss: 11.3502\n",
      "Epoch 4273/6000, Training Loss: 11.3311, Validation Loss: 11.3501\n",
      "Epoch 4274/6000, Training Loss: 11.3310, Validation Loss: 11.3501\n",
      "Epoch 4275/6000, Training Loss: 11.3310, Validation Loss: 11.3501\n",
      "Epoch 4276/6000, Training Loss: 11.3310, Validation Loss: 11.3501\n",
      "Epoch 4277/6000, Training Loss: 11.3310, Validation Loss: 11.3501\n",
      "Epoch 4278/6000, Training Loss: 11.3310, Validation Loss: 11.3501\n",
      "Epoch 4279/6000, Training Loss: 11.3310, Validation Loss: 11.3501\n",
      "Epoch 4280/6000, Training Loss: 11.3310, Validation Loss: 11.3501\n",
      "Epoch 4281/6000, Training Loss: 11.3310, Validation Loss: 11.3501\n",
      "Epoch 4282/6000, Training Loss: 11.3310, Validation Loss: 11.3501\n",
      "Epoch 4283/6000, Training Loss: 11.3309, Validation Loss: 11.3500\n",
      "Epoch 4284/6000, Training Loss: 11.3309, Validation Loss: 11.3500\n",
      "Epoch 4285/6000, Training Loss: 11.3309, Validation Loss: 11.3500\n",
      "Epoch 4286/6000, Training Loss: 11.3309, Validation Loss: 11.3500\n",
      "Epoch 4287/6000, Training Loss: 11.3309, Validation Loss: 11.3500\n",
      "Epoch 4288/6000, Training Loss: 11.3309, Validation Loss: 11.3500\n",
      "Epoch 4289/6000, Training Loss: 11.3309, Validation Loss: 11.3500\n",
      "Epoch 4290/6000, Training Loss: 11.3309, Validation Loss: 11.3500\n",
      "Epoch 4291/6000, Training Loss: 11.3309, Validation Loss: 11.3500\n",
      "Epoch 4292/6000, Training Loss: 11.3309, Validation Loss: 11.3500\n",
      "Epoch 4293/6000, Training Loss: 11.3309, Validation Loss: 11.3500\n",
      "Epoch 4294/6000, Training Loss: 11.3308, Validation Loss: 11.3500\n",
      "Epoch 4295/6000, Training Loss: 11.3308, Validation Loss: 11.3500\n",
      "Epoch 4296/6000, Training Loss: 11.3308, Validation Loss: 11.3499\n",
      "Epoch 4297/6000, Training Loss: 11.3308, Validation Loss: 11.3499\n",
      "Epoch 4298/6000, Training Loss: 11.3308, Validation Loss: 11.3499\n",
      "Epoch 4299/6000, Training Loss: 11.3308, Validation Loss: 11.3499\n",
      "Epoch 4300/6000, Training Loss: 11.3308, Validation Loss: 11.3499\n",
      "Epoch 4301/6000, Training Loss: 11.3308, Validation Loss: 11.3499\n",
      "Epoch 4302/6000, Training Loss: 11.3308, Validation Loss: 11.3499\n",
      "Epoch 4303/6000, Training Loss: 11.3308, Validation Loss: 11.3499\n",
      "Epoch 4304/6000, Training Loss: 11.3308, Validation Loss: 11.3499\n",
      "Epoch 4305/6000, Training Loss: 11.3307, Validation Loss: 11.3499\n",
      "Epoch 4306/6000, Training Loss: 11.3307, Validation Loss: 11.3499\n",
      "Epoch 4307/6000, Training Loss: 11.3307, Validation Loss: 11.3499\n",
      "Epoch 4308/6000, Training Loss: 11.3307, Validation Loss: 11.3499\n",
      "Epoch 4309/6000, Training Loss: 11.3307, Validation Loss: 11.3499\n",
      "Epoch 4310/6000, Training Loss: 11.3307, Validation Loss: 11.3499\n",
      "Epoch 4311/6000, Training Loss: 11.3307, Validation Loss: 11.3498\n",
      "Epoch 4312/6000, Training Loss: 11.3307, Validation Loss: 11.3498\n",
      "Epoch 4313/6000, Training Loss: 11.3307, Validation Loss: 11.3498\n",
      "Epoch 4314/6000, Training Loss: 11.3307, Validation Loss: 11.3498\n",
      "Epoch 4315/6000, Training Loss: 11.3307, Validation Loss: 11.3498\n",
      "Epoch 4316/6000, Training Loss: 11.3307, Validation Loss: 11.3498\n",
      "Epoch 4317/6000, Training Loss: 11.3307, Validation Loss: 11.3498\n",
      "Epoch 4318/6000, Training Loss: 11.3307, Validation Loss: 11.3498\n",
      "Epoch 4319/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4320/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4321/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4322/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4323/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4324/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4325/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4326/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4327/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4328/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4329/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4330/6000, Training Loss: 11.3306, Validation Loss: 11.3498\n",
      "Epoch 4331/6000, Training Loss: 11.3306, Validation Loss: 11.3497\n",
      "Epoch 4332/6000, Training Loss: 11.3306, Validation Loss: 11.3497\n",
      "Epoch 4333/6000, Training Loss: 11.3306, Validation Loss: 11.3497\n",
      "Epoch 4334/6000, Training Loss: 11.3306, Validation Loss: 11.3497\n",
      "Epoch 4335/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4336/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4337/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4338/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4339/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4340/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4341/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4342/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4343/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4344/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4345/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4346/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4347/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4348/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4349/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4350/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4351/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4352/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4353/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4354/6000, Training Loss: 11.3305, Validation Loss: 11.3497\n",
      "Epoch 4355/6000, Training Loss: 11.3304, Validation Loss: 11.3497\n",
      "Epoch 4356/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4357/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4358/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4359/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4360/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4361/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4362/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4363/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4364/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4365/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4366/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4367/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4368/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4369/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4370/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4371/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4372/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4373/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4374/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4375/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4376/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4377/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4378/6000, Training Loss: 11.3304, Validation Loss: 11.3496\n",
      "Epoch 4379/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4380/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4381/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4382/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4383/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4384/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4385/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4386/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4387/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4388/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4389/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4390/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4391/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4392/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4393/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4394/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4395/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4396/6000, Training Loss: 11.3303, Validation Loss: 11.3496\n",
      "Epoch 4397/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4398/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4399/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4400/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4401/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4402/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4403/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4404/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4405/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4406/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4407/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4408/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4409/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4410/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4411/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4412/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4413/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4414/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4415/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4416/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4417/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4418/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4419/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4420/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4421/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4422/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4423/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4424/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4425/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4426/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4427/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4428/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4429/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4430/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4431/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4432/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4433/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4434/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4435/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4436/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4437/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4438/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4439/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4440/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4441/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4442/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4443/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4444/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4445/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4446/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4447/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4448/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4449/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4450/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4451/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4452/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4453/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4454/6000, Training Loss: 11.3302, Validation Loss: 11.3495\n",
      "Epoch 4455/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4456/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4457/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4458/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4459/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4460/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4461/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4462/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4463/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4464/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4465/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4466/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4467/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4468/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4469/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4470/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4471/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4472/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4473/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4474/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4475/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4476/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4477/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4478/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4479/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4480/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4481/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4482/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4483/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4484/6000, Training Loss: 11.3301, Validation Loss: 11.3495\n",
      "Epoch 4485/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4486/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4487/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4488/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4489/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4490/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4491/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4492/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4493/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4494/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4495/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4496/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4497/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4498/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4499/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4500/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4501/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4502/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4503/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4504/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4505/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4506/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4507/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4508/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4509/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4510/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4511/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4512/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4513/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4514/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4515/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4516/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4517/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4518/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4519/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4520/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4521/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4522/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4523/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4524/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4525/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4526/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4527/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4528/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4529/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4530/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4531/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4532/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4533/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4534/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4535/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4536/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4537/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4538/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4539/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4540/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4541/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4542/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4543/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4544/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4545/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4546/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4547/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4548/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4549/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4550/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4551/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4552/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4553/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4554/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4555/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4556/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4557/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4558/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4559/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4560/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4561/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4562/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4563/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4564/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4565/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4566/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4567/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4568/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4569/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4570/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4571/6000, Training Loss: 11.3300, Validation Loss: 11.3494\n",
      "Epoch 4572/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4573/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4574/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4575/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4576/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4577/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4578/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4579/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4580/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4581/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4582/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4583/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4584/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4585/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4586/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4587/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4588/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4589/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4590/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4591/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4592/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4593/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4594/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4595/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4596/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4597/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4598/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4599/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4600/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4601/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4602/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4603/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4604/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4605/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4606/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4607/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4608/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4609/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4610/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4611/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4612/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4613/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4614/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4615/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4616/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4617/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4618/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4619/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4620/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4621/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4622/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4623/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4624/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4625/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4626/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4627/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4628/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4629/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4630/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4631/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4632/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4633/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4634/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4635/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4636/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4637/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4638/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4639/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4640/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4641/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4642/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4643/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4644/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4645/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4646/6000, Training Loss: 11.3299, Validation Loss: 11.3494\n",
      "Epoch 4647/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4648/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4649/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4650/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4651/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4652/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4653/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4654/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4655/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4656/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4657/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4658/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4659/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4660/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4661/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4662/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4663/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4664/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4665/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4666/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4667/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4668/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4669/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4670/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4671/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4672/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4673/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4674/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4675/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4676/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4677/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4678/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4679/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4680/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4681/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4682/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4683/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4684/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4685/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4686/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4687/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4688/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4689/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4690/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4691/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4692/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4693/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4694/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4695/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4696/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4697/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4698/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4699/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4700/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4701/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4702/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4703/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4704/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4705/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4706/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4707/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4708/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4709/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4710/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4711/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4712/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4713/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4714/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4715/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4716/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4717/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4718/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4719/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4720/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4721/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4722/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4723/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4724/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4725/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4726/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4727/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4728/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4729/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4730/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4731/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4732/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4733/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4734/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4735/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4736/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4737/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4738/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4739/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4740/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4741/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4742/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4743/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4744/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4745/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4746/6000, Training Loss: 11.3298, Validation Loss: 11.3494\n",
      "Epoch 4747/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4748/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4749/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4750/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4751/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4752/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4753/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4754/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4755/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4756/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4757/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4758/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4759/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4760/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4761/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4762/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4763/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4764/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4765/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4766/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4767/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4768/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4769/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4770/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4771/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4772/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4773/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4774/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4775/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4776/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4777/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4778/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4779/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4780/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4781/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4782/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4783/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4784/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4785/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4786/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4787/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4788/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4789/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4790/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4791/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4792/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4793/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4794/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4795/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4796/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4797/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4798/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4799/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4800/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4801/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4802/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4803/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4804/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4805/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4806/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4807/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4808/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4809/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4810/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4811/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4812/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4813/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4814/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4815/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4816/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4817/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4818/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4819/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4820/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4821/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4822/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4823/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4824/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4825/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4826/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4827/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4828/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4829/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4830/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4831/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4832/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4833/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4834/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4835/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4836/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4837/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4838/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4839/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4840/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4841/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4842/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4843/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4844/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4845/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4846/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4847/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4848/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4849/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4850/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4851/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4852/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4853/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4854/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4855/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4856/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4857/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4858/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4859/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4860/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4861/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4862/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4863/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4864/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4865/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4866/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4867/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4868/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4869/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4870/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4871/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4872/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4873/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4874/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4875/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4876/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4877/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4878/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4879/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4880/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4881/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4882/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4883/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4884/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4885/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4886/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4887/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4888/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4889/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4890/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4891/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4892/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4893/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4894/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4895/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4896/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4897/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4898/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4899/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4900/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4901/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4902/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4903/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4904/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4905/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4906/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4907/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4908/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4909/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4910/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4911/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4912/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4913/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4914/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4915/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4916/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4917/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4918/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4919/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4920/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4921/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4922/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4923/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4924/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4925/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4926/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4927/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4928/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4929/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4930/6000, Training Loss: 11.3297, Validation Loss: 11.3494\n",
      "Epoch 4931/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4932/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4933/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4934/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4935/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4936/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4937/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4938/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4939/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4940/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4941/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4942/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4943/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4944/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4945/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4946/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4947/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4948/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4949/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4950/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4951/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4952/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4953/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4954/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4955/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4956/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4957/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4958/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4959/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4960/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4961/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4962/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4963/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4964/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4965/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4966/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4967/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4968/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4969/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4970/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4971/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4972/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4973/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4974/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4975/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4976/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4977/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4978/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4979/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4980/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4981/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4982/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4983/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4984/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4985/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4986/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4987/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4988/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4989/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4990/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4991/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4992/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4993/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4994/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4995/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4996/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4997/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4998/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4999/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5000/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5001/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5002/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5003/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5004/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5005/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5006/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5007/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5008/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5009/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5010/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5011/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5012/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5013/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5014/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5015/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5016/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5017/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5018/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5019/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5020/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5021/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5022/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5023/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5024/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5025/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5026/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5027/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5028/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5029/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5030/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5031/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5032/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5033/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5034/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5035/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5036/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5037/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5038/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5039/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5040/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5041/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5042/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5043/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5044/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5045/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5046/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5047/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5048/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5049/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5050/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5051/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5052/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5053/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5054/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5055/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5056/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5057/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5058/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5059/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5060/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5061/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5062/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5063/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5064/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5065/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5066/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5067/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5068/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5069/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5070/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5071/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5072/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5073/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5074/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5075/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5076/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5077/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5078/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5079/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5080/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5081/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5082/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5083/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5084/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5085/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5086/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5087/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5088/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5089/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5090/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5091/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5092/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5093/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5094/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5095/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5096/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5097/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5098/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5099/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5100/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5101/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5102/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5103/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5104/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5105/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5106/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5107/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5108/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5109/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5110/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5111/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5112/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5113/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5114/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5115/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5116/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5117/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5118/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5119/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5120/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5121/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5122/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5123/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5124/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5125/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5126/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5127/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5128/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5129/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5130/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5131/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5132/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5133/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5134/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5135/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5136/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5137/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5138/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5139/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5140/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5141/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5142/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5143/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5144/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5145/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5146/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5147/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5148/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5149/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5150/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5151/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5152/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5153/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5154/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5155/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5156/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5157/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5158/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5159/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5160/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5161/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5162/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5163/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5164/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5165/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5166/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5167/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5168/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5169/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5170/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5171/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5172/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5173/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5174/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5175/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5176/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5177/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5178/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5179/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5180/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5181/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5182/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5183/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5184/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5185/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5186/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5187/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5188/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5189/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5190/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5191/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5192/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5193/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5194/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5195/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5196/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5197/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5198/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5199/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5200/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5201/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5202/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5203/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5204/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5205/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5206/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5207/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5208/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5209/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5210/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5211/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5212/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5213/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5214/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5215/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5216/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5217/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5218/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5219/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5220/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5221/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5222/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5223/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5224/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5225/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5226/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5227/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5228/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5229/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5230/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5231/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5232/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5233/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5234/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5235/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5236/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5237/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5238/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5239/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5240/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5241/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5242/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5243/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5244/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5245/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5246/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5247/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5248/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5249/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5250/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5251/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5252/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5253/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5254/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5255/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5256/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5257/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5258/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5259/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5260/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5261/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5262/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5263/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5264/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5265/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5266/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5267/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5268/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5269/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5270/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5271/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5272/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5273/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5274/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5275/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5276/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5277/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5278/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5279/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5280/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5281/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5282/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5283/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5284/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5285/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5286/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5287/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5288/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5289/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5290/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5291/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5292/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5293/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5294/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5295/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5296/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5297/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5298/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5299/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5300/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5301/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5302/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5303/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5304/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5305/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5306/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5307/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5308/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5309/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5310/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5311/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5312/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5313/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5314/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5315/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5316/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5317/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5318/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5319/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5320/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5321/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5322/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5323/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5324/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5325/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5326/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5327/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5328/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5329/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5330/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5331/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5332/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5333/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5334/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5335/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5336/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5337/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5338/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5339/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5340/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5341/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5342/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5343/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5344/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5345/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5346/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5347/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5348/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5349/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5350/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5351/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5352/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5353/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5354/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5355/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5356/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5357/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5358/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5359/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5360/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5361/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5362/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5363/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5364/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5365/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5366/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5367/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5368/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5369/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5370/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5371/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5372/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5373/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5374/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5375/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5376/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5377/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5378/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5379/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5380/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5381/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5382/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5383/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5384/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5385/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5386/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5387/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5388/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5389/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5390/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5391/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5392/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5393/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5394/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5395/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5396/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5397/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5398/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5399/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5400/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5401/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5402/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5403/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5404/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5405/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5406/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5407/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5408/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5409/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5410/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5411/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5412/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5413/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5414/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5415/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5416/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5417/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5418/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5419/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5420/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5421/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5422/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5423/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5424/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5425/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5426/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5427/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5428/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5429/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5430/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5431/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5432/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5433/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5434/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5435/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5436/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5437/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5438/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5439/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5440/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5441/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5442/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5443/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5444/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5445/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5446/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5447/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5448/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5449/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5450/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5451/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5452/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5453/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5454/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5455/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5456/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5457/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5458/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5459/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5460/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5461/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5462/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5463/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5464/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5465/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5466/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5467/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5468/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5469/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5470/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5471/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5472/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5473/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5474/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5475/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5476/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5477/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5478/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5479/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5480/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5481/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5482/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5483/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5484/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5485/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5486/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5487/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5488/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5489/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5490/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5491/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5492/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5493/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5494/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5495/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5496/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5497/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5498/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5499/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5500/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5501/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5502/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5503/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5504/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5505/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5506/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5507/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5508/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5509/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5510/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5511/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5512/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5513/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5514/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5515/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5516/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5517/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5518/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5519/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5520/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5521/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5522/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5523/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5524/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5525/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5526/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5527/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5528/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5529/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5530/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5531/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5532/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5533/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5534/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5535/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5536/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5537/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5538/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5539/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5540/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5541/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5542/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5543/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5544/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5545/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5546/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5547/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5548/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5549/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5550/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5551/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5552/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5553/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5554/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5555/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5556/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5557/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5558/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5559/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5560/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5561/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5562/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5563/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5564/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5565/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5566/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5567/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5568/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5569/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5570/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5571/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5572/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5573/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5574/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5575/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5576/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5577/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5578/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5579/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5580/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5581/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5582/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5583/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5584/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5585/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5586/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5587/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5588/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5589/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5590/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5591/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5592/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5593/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5594/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5595/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5596/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5597/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5598/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5599/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5600/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5601/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5602/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5603/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5604/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5605/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5606/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5607/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5608/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5609/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5610/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5611/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5612/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5613/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5614/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5615/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5616/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5617/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5618/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5619/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5620/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5621/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5622/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5623/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5624/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5625/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5626/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5627/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5628/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5629/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5630/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5631/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5632/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5633/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5634/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5635/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5636/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5637/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5638/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5639/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5640/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5641/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5642/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5643/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5644/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5645/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5646/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5647/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5648/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5649/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5650/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5651/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5652/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5653/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5654/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5655/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5656/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5657/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5658/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5659/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5660/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5661/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5662/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5663/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5664/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5665/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5666/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5667/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5668/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5669/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5670/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5671/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5672/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5673/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5674/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5675/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5676/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5677/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5678/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5679/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5680/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5681/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5682/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5683/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5684/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5685/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5686/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5687/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5688/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5689/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5690/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5691/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5692/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5693/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5694/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5695/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5696/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5697/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5698/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5699/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5700/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5701/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5702/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5703/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5704/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5705/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5706/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5707/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5708/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5709/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5710/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5711/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5712/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5713/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5714/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5715/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5716/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5717/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5718/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5719/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5720/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5721/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5722/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5723/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5724/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5725/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5726/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5727/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5728/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5729/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5730/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5731/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5732/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5733/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5734/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5735/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5736/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5737/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5738/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5739/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5740/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5741/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5742/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5743/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5744/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5745/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5746/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5747/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5748/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5749/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5750/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5751/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5752/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5753/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5754/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5755/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5756/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5757/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5758/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5759/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5760/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5761/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5762/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5763/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5764/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5765/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5766/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5767/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5768/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5769/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5770/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5771/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5772/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5773/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5774/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5775/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5776/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5777/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5778/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5779/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5780/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5781/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5782/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5783/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5784/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5785/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5786/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5787/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5788/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5789/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5790/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5791/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5792/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5793/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5794/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5795/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5796/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5797/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5798/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5799/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5800/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5801/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5802/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5803/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5804/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5805/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5806/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5807/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5808/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5809/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5810/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5811/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5812/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5813/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5814/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5815/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5816/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5817/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5818/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5819/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5820/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5821/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5822/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5823/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5824/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5825/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5826/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5827/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5828/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5829/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5830/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5831/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5832/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5833/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5834/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5835/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5836/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5837/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5838/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5839/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5840/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5841/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5842/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5843/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5844/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5845/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5846/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5847/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5848/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5849/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5850/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5851/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5852/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5853/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5854/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5855/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5856/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5857/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5858/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5859/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5860/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5861/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5862/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5863/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5864/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5865/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5866/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5867/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5868/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5869/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5870/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5871/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5872/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5873/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5874/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5875/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5876/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5877/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5878/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5879/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5880/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5881/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5882/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5883/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5884/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5885/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5886/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5887/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5888/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5889/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5890/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5891/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5892/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5893/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5894/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5895/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5896/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5897/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5898/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5899/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5900/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5901/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5902/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5903/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5904/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5905/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5906/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5907/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5908/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5909/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5910/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5911/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5912/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5913/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5914/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5915/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5916/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5917/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5918/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5919/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5920/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5921/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5922/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5923/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5924/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5925/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5926/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5927/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5928/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5929/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5930/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5931/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5932/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5933/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5934/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5935/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5936/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5937/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5938/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5939/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5940/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5941/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5942/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5943/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5944/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5945/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5946/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5947/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5948/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5949/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5950/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5951/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5952/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5953/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5954/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5955/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5956/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5957/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5958/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5959/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5960/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5961/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5962/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5963/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5964/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5965/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5966/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5967/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5968/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5969/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5970/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5971/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5972/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5973/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5974/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5975/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5976/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5977/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5978/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5979/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5980/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5981/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5982/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5983/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5984/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5985/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5986/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5987/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5988/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5989/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5990/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5991/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5992/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5993/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5994/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5995/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5996/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5997/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5998/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5999/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 6000/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.25%\n",
      "Mean Percent Accuracy (Yield): 89.17%\n",
      "Median Percent Accuracy (Days to Harvest): 79.45%\n",
      "Median Percent Accuracy (Yield): 92.56%\n",
      "Accuracy Within 10% (Days to Harvest): 23.58%\n",
      "Accuracy Within 10% (Yield): 62.50%\n",
      "MAE (Days to Harvest): 22.45\n",
      "MAE (Yield): 0.40\n",
      "RMSE (Days to Harvest): 25.94\n",
      "RMSE (Yield): 0.50\n",
      "\n",
      "Validation Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.17%\n",
      "Mean Percent Accuracy (Yield): 89.00%\n",
      "Median Percent Accuracy (Days to Harvest): 79.42%\n",
      "Median Percent Accuracy (Yield): 92.52%\n",
      "Accuracy Within 10% (Days to Harvest): 23.47%\n",
      "Accuracy Within 10% (Yield): 62.39%\n",
      "MAE (Days to Harvest): 22.49\n",
      "MAE (Yield): 0.40\n",
      "RMSE (Days to Harvest): 25.98\n",
      "RMSE (Yield): 0.50\n",
      "\n",
      "Training completed in 14.00 seconds\n",
      "Final Training Loss: 11.3296\n",
      "Final Validation Loss: 11.3495\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y.values, test_size=0.2, random_state=88)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Move data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor, X_val_tensor = X_train_tensor.to(device), X_val_tensor.to(device)\n",
    "y_train_tensor, y_val_tensor = y_train_tensor.to(device), y_val_tensor.to(device)\n",
    "\n",
    "# Define SVM-like regression model\n",
    "class SVMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVMRegressor, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 2)  # Output two targets: Days to Harvest and Yield\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Define hinge loss for -SVR\n",
    "class EpsilonSVRLoss(nn.Module):\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        super(EpsilonSVRLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        abs_diff = torch.abs(predictions - targets)\n",
    "        loss = torch.mean(torch.clamp(abs_diff - self.epsilon, min=0))\n",
    "        return loss\n",
    "\n",
    "# Instantiate model, loss, and optimizer\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = SVMRegressor(input_dim).to(device)\n",
    "criterion = EpsilonSVRLoss(epsilon=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 6000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_predictions = model(X_train_tensor)\n",
    "    train_loss = criterion(train_predictions, y_train_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_val_tensor)\n",
    "        val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "    # Store losses\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time  # Training time in seconds\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(y_actual, y_pred, tolerance=0.1):\n",
    "    # Percent accuracy\n",
    "    percent_accuracies = 100 * (1 - torch.abs(y_actual - y_pred) / y_actual)\n",
    "    mean_percent_accuracy = torch.mean(percent_accuracies, axis=0).detach().cpu().numpy()\n",
    "    median_percent_accuracy = torch.median(percent_accuracies, axis=0).values.detach().cpu().numpy()\n",
    "\n",
    "    # Accuracy within tolerance\n",
    "    within_tolerance = torch.abs(y_actual - y_pred) <= (tolerance * y_actual)\n",
    "    percent_within_tolerance = torch.mean(within_tolerance.float(), axis=0).detach().cpu().numpy() * 100\n",
    "\n",
    "    # MAE\n",
    "    mae = torch.mean(torch.abs(y_actual - y_pred), axis=0).detach().cpu().numpy()\n",
    "\n",
    "    # RMSE\n",
    "    rmse = torch.sqrt(torch.mean((y_actual - y_pred) ** 2, axis=0)).detach().cpu().numpy()\n",
    "\n",
    "    return {\n",
    "        \"mean_percent_accuracy\": mean_percent_accuracy,\n",
    "        \"median_percent_accuracy\": median_percent_accuracy,\n",
    "        \"percent_within_tolerance\": percent_within_tolerance,\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "    }\n",
    "\n",
    "# Get predictions and metrics\n",
    "train_metrics = evaluate_model(y_train_tensor, train_predictions)\n",
    "val_metrics = evaluate_model(y_val_tensor, val_predictions)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {train_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {train_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {train_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {train_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {train_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {train_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {train_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {train_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {train_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {train_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {val_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {val_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {val_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {val_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {val_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {val_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {val_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {val_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {val_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {val_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "final_train_loss = train_losses[-1]\n",
    "final_val_loss = val_losses[-1]\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000, Training Loss: 5807.8989, Validation Loss: 5613.1685\n",
      "Epoch 2/2000, Training Loss: 5618.0962, Validation Loss: 5430.0557\n",
      "Epoch 3/2000, Training Loss: 5434.8774, Validation Loss: 5253.2969\n",
      "Epoch 4/2000, Training Loss: 5258.0156, Validation Loss: 5082.6719\n",
      "Epoch 5/2000, Training Loss: 5087.2896, Validation Loss: 4917.9673\n",
      "Epoch 6/2000, Training Loss: 4922.4858, Validation Loss: 4758.9795\n",
      "Epoch 7/2000, Training Loss: 4763.3994, Validation Loss: 4605.5088\n",
      "Epoch 8/2000, Training Loss: 4609.8315, Validation Loss: 4457.3628\n",
      "Epoch 9/2000, Training Loss: 4461.5918, Validation Loss: 4314.3589\n",
      "Epoch 10/2000, Training Loss: 4318.4951, Validation Loss: 4176.3169\n",
      "Epoch 11/2000, Training Loss: 4180.3618, Validation Loss: 4043.0659\n",
      "Epoch 12/2000, Training Loss: 4047.0212, Validation Loss: 3914.4390\n",
      "Epoch 13/2000, Training Loss: 3918.3064, Validation Loss: 3790.2754\n",
      "Epoch 14/2000, Training Loss: 3794.0562, Validation Loss: 3670.4211\n",
      "Epoch 15/2000, Training Loss: 3674.1169, Validation Loss: 3554.7261\n",
      "Epoch 16/2000, Training Loss: 3558.3389, Validation Loss: 3443.0461\n",
      "Epoch 17/2000, Training Loss: 3446.5767, Validation Loss: 3335.2417\n",
      "Epoch 18/2000, Training Loss: 3338.6924, Validation Loss: 3231.1787\n",
      "Epoch 19/2000, Training Loss: 3234.5500, Validation Loss: 3130.7271\n",
      "Epoch 20/2000, Training Loss: 3134.0210, Validation Loss: 3033.7617\n",
      "Epoch 21/2000, Training Loss: 3036.9795, Validation Loss: 2940.1614\n",
      "Epoch 22/2000, Training Loss: 2943.3042, Validation Loss: 2849.8093\n",
      "Epoch 23/2000, Training Loss: 2852.8787, Validation Loss: 2762.5928\n",
      "Epoch 24/2000, Training Loss: 2765.5903, Validation Loss: 2678.4036\n",
      "Epoch 25/2000, Training Loss: 2681.3298, Validation Loss: 2597.1360\n",
      "Epoch 26/2000, Training Loss: 2599.9922, Validation Loss: 2518.6887\n",
      "Epoch 27/2000, Training Loss: 2521.4766, Validation Loss: 2442.9641\n",
      "Epoch 28/2000, Training Loss: 2445.6851, Validation Loss: 2369.8679\n",
      "Epoch 29/2000, Training Loss: 2372.5222, Validation Loss: 2299.3081\n",
      "Epoch 30/2000, Training Loss: 2301.8979, Validation Loss: 2231.1975\n",
      "Epoch 31/2000, Training Loss: 2233.7234, Validation Loss: 2165.4507\n",
      "Epoch 32/2000, Training Loss: 2167.9143, Validation Loss: 2101.9856\n",
      "Epoch 33/2000, Training Loss: 2104.3877, Validation Loss: 2040.7233\n",
      "Epoch 34/2000, Training Loss: 2043.0649, Validation Loss: 1981.5875\n",
      "Epoch 35/2000, Training Loss: 1983.8695, Validation Loss: 1924.5038\n",
      "Epoch 36/2000, Training Loss: 1926.7278, Validation Loss: 1869.4016\n",
      "Epoch 37/2000, Training Loss: 1871.5684, Validation Loss: 1816.2119\n",
      "Epoch 38/2000, Training Loss: 1818.3221, Validation Loss: 1764.8683\n",
      "Epoch 39/2000, Training Loss: 1766.9233, Validation Loss: 1715.3068\n",
      "Epoch 40/2000, Training Loss: 1717.3075, Validation Loss: 1667.4653\n",
      "Epoch 41/2000, Training Loss: 1669.4128, Validation Loss: 1621.2845\n",
      "Epoch 42/2000, Training Loss: 1623.1797, Validation Loss: 1576.7067\n",
      "Epoch 43/2000, Training Loss: 1578.5503, Validation Loss: 1533.6763\n",
      "Epoch 44/2000, Training Loss: 1535.4692, Validation Loss: 1492.1392\n",
      "Epoch 45/2000, Training Loss: 1493.8826, Validation Loss: 1452.0441\n",
      "Epoch 46/2000, Training Loss: 1453.7386, Validation Loss: 1413.3406\n",
      "Epoch 47/2000, Training Loss: 1414.9871, Validation Loss: 1375.9806\n",
      "Epoch 48/2000, Training Loss: 1377.5800, Validation Loss: 1339.9174\n",
      "Epoch 49/2000, Training Loss: 1341.4707, Validation Loss: 1305.1061\n",
      "Epoch 50/2000, Training Loss: 1306.6139, Validation Loss: 1271.5031\n",
      "Epoch 51/2000, Training Loss: 1272.9662, Validation Loss: 1239.0663\n",
      "Epoch 52/2000, Training Loss: 1240.4857, Validation Loss: 1207.7559\n",
      "Epoch 53/2000, Training Loss: 1209.1321, Validation Loss: 1177.5321\n",
      "Epoch 54/2000, Training Loss: 1178.8660, Validation Loss: 1148.3575\n",
      "Epoch 55/2000, Training Loss: 1149.6499, Validation Loss: 1120.1957\n",
      "Epoch 56/2000, Training Loss: 1121.4471, Validation Loss: 1093.0115\n",
      "Epoch 57/2000, Training Loss: 1094.2229, Validation Loss: 1066.7709\n",
      "Epoch 58/2000, Training Loss: 1067.9429, Validation Loss: 1041.4413\n",
      "Epoch 59/2000, Training Loss: 1042.5745, Validation Loss: 1016.9911\n",
      "Epoch 60/2000, Training Loss: 1018.0864, Validation Loss: 993.3898\n",
      "Epoch 61/2000, Training Loss: 994.4474, Validation Loss: 970.6076\n",
      "Epoch 62/2000, Training Loss: 971.6286, Validation Loss: 948.6165\n",
      "Epoch 63/2000, Training Loss: 949.6014, Validation Loss: 927.3889\n",
      "Epoch 64/2000, Training Loss: 928.3384, Validation Loss: 906.8981\n",
      "Epoch 65/2000, Training Loss: 907.8128, Validation Loss: 887.1188\n",
      "Epoch 66/2000, Training Loss: 887.9991, Validation Loss: 868.0261\n",
      "Epoch 67/2000, Training Loss: 868.8729, Validation Loss: 849.5963\n",
      "Epoch 68/2000, Training Loss: 850.4101, Validation Loss: 831.8064\n",
      "Epoch 69/2000, Training Loss: 832.5878, Validation Loss: 814.6342\n",
      "Epoch 70/2000, Training Loss: 815.3836, Validation Loss: 798.0581\n",
      "Epoch 71/2000, Training Loss: 798.7762, Validation Loss: 782.0576\n",
      "Epoch 72/2000, Training Loss: 782.7450, Validation Loss: 766.6126\n",
      "Epoch 73/2000, Training Loss: 767.2698, Validation Loss: 751.7039\n",
      "Epoch 74/2000, Training Loss: 752.3314, Validation Loss: 737.3129\n",
      "Epoch 75/2000, Training Loss: 737.9114, Validation Loss: 723.4216\n",
      "Epoch 76/2000, Training Loss: 723.9913, Validation Loss: 710.0126\n",
      "Epoch 77/2000, Training Loss: 710.5541, Validation Loss: 697.0693\n",
      "Epoch 78/2000, Training Loss: 697.5831, Validation Loss: 684.5754\n",
      "Epoch 79/2000, Training Loss: 685.0621, Validation Loss: 672.5153\n",
      "Epoch 80/2000, Training Loss: 672.9754, Validation Loss: 660.8741\n",
      "Epoch 81/2000, Training Loss: 661.3079, Validation Loss: 649.6370\n",
      "Epoch 82/2000, Training Loss: 650.0451, Validation Loss: 638.7903\n",
      "Epoch 83/2000, Training Loss: 639.1730, Validation Loss: 628.3201\n",
      "Epoch 84/2000, Training Loss: 628.6779, Validation Loss: 618.2136\n",
      "Epoch 85/2000, Training Loss: 618.5470, Validation Loss: 608.4581\n",
      "Epoch 86/2000, Training Loss: 608.7675, Validation Loss: 599.0413\n",
      "Epoch 87/2000, Training Loss: 599.3271, Validation Loss: 589.9516\n",
      "Epoch 88/2000, Training Loss: 590.2144, Validation Loss: 581.1776\n",
      "Epoch 89/2000, Training Loss: 581.4175, Validation Loss: 572.7083\n",
      "Epoch 90/2000, Training Loss: 572.9259, Validation Loss: 564.5333\n",
      "Epoch 91/2000, Training Loss: 564.7288, Validation Loss: 556.6420\n",
      "Epoch 92/2000, Training Loss: 556.8160, Validation Loss: 549.0250\n",
      "Epoch 93/2000, Training Loss: 549.1777, Validation Loss: 541.6723\n",
      "Epoch 94/2000, Training Loss: 541.8043, Validation Loss: 534.5753\n",
      "Epoch 95/2000, Training Loss: 534.6866, Validation Loss: 527.7245\n",
      "Epoch 96/2000, Training Loss: 527.8159, Validation Loss: 521.1118\n",
      "Epoch 97/2000, Training Loss: 521.1835, Validation Loss: 514.7288\n",
      "Epoch 98/2000, Training Loss: 514.7811, Validation Loss: 508.5675\n",
      "Epoch 99/2000, Training Loss: 508.6007, Validation Loss: 502.6203\n",
      "Epoch 100/2000, Training Loss: 502.6347, Validation Loss: 496.8797\n",
      "Epoch 101/2000, Training Loss: 496.8757, Validation Loss: 491.3385\n",
      "Epoch 102/2000, Training Loss: 491.3164, Validation Loss: 485.9898\n",
      "Epoch 103/2000, Training Loss: 485.9499, Validation Loss: 480.8269\n",
      "Epoch 104/2000, Training Loss: 480.7696, Validation Loss: 475.8434\n",
      "Epoch 105/2000, Training Loss: 475.7689, Validation Loss: 471.0330\n",
      "Epoch 106/2000, Training Loss: 470.9417, Validation Loss: 466.3897\n",
      "Epoch 107/2000, Training Loss: 466.2819, Validation Loss: 461.9078\n",
      "Epoch 108/2000, Training Loss: 461.7837, Validation Loss: 457.5816\n",
      "Epoch 109/2000, Training Loss: 457.4415, Validation Loss: 453.4057\n",
      "Epoch 110/2000, Training Loss: 453.2499, Validation Loss: 449.3750\n",
      "Epoch 111/2000, Training Loss: 449.2038, Validation Loss: 445.4842\n",
      "Epoch 112/2000, Training Loss: 445.2979, Validation Loss: 441.7287\n",
      "Epoch 113/2000, Training Loss: 441.5275, Validation Loss: 438.1038\n",
      "Epoch 114/2000, Training Loss: 437.8879, Validation Loss: 434.6048\n",
      "Epoch 115/2000, Training Loss: 434.3745, Validation Loss: 431.2273\n",
      "Epoch 116/2000, Training Loss: 430.9830, Validation Loss: 427.9673\n",
      "Epoch 117/2000, Training Loss: 427.7091, Validation Loss: 424.8205\n",
      "Epoch 118/2000, Training Loss: 424.5487, Validation Loss: 421.7831\n",
      "Epoch 119/2000, Training Loss: 421.4979, Validation Loss: 418.8513\n",
      "Epoch 120/2000, Training Loss: 418.5530, Validation Loss: 416.0214\n",
      "Epoch 121/2000, Training Loss: 415.7102, Validation Loss: 413.2899\n",
      "Epoch 122/2000, Training Loss: 412.9659, Validation Loss: 410.6533\n",
      "Epoch 123/2000, Training Loss: 410.3169, Validation Loss: 408.1084\n",
      "Epoch 124/2000, Training Loss: 407.7597, Validation Loss: 405.6519\n",
      "Epoch 125/2000, Training Loss: 405.2912, Validation Loss: 403.2809\n",
      "Epoch 126/2000, Training Loss: 402.9083, Validation Loss: 400.9923\n",
      "Epoch 127/2000, Training Loss: 400.6081, Validation Loss: 398.7831\n",
      "Epoch 128/2000, Training Loss: 398.3876, Validation Loss: 396.6509\n",
      "Epoch 129/2000, Training Loss: 396.2441, Validation Loss: 394.5928\n",
      "Epoch 130/2000, Training Loss: 394.1750, Validation Loss: 392.6062\n",
      "Epoch 131/2000, Training Loss: 392.1776, Validation Loss: 390.6887\n",
      "Epoch 132/2000, Training Loss: 390.2495, Validation Loss: 388.8379\n",
      "Epoch 133/2000, Training Loss: 388.3882, Validation Loss: 387.0514\n",
      "Epoch 134/2000, Training Loss: 386.5914, Validation Loss: 385.3271\n",
      "Epoch 135/2000, Training Loss: 384.8571, Validation Loss: 383.6628\n",
      "Epoch 136/2000, Training Loss: 383.1828, Validation Loss: 382.0563\n",
      "Epoch 137/2000, Training Loss: 381.5666, Validation Loss: 380.5057\n",
      "Epoch 138/2000, Training Loss: 380.0064, Validation Loss: 379.0090\n",
      "Epoch 139/2000, Training Loss: 378.5004, Validation Loss: 377.5645\n",
      "Epoch 140/2000, Training Loss: 377.0465, Validation Loss: 376.1701\n",
      "Epoch 141/2000, Training Loss: 375.6432, Validation Loss: 374.8242\n",
      "Epoch 142/2000, Training Loss: 374.2884, Validation Loss: 373.5252\n",
      "Epoch 143/2000, Training Loss: 372.9807, Validation Loss: 372.2714\n",
      "Epoch 144/2000, Training Loss: 371.7182, Validation Loss: 371.0612\n",
      "Epoch 145/2000, Training Loss: 370.4995, Validation Loss: 369.8931\n",
      "Epoch 146/2000, Training Loss: 369.3232, Validation Loss: 368.7656\n",
      "Epoch 147/2000, Training Loss: 368.1875, Validation Loss: 367.6774\n",
      "Epoch 148/2000, Training Loss: 367.0913, Validation Loss: 366.6271\n",
      "Epoch 149/2000, Training Loss: 366.0331, Validation Loss: 365.6133\n",
      "Epoch 150/2000, Training Loss: 365.0116, Validation Loss: 364.6347\n",
      "Epoch 151/2000, Training Loss: 364.0254, Validation Loss: 363.6903\n",
      "Epoch 152/2000, Training Loss: 363.0735, Validation Loss: 362.7787\n",
      "Epoch 153/2000, Training Loss: 362.1546, Validation Loss: 361.8988\n",
      "Epoch 154/2000, Training Loss: 361.2676, Validation Loss: 361.0496\n",
      "Epoch 155/2000, Training Loss: 360.4113, Validation Loss: 360.2299\n",
      "Epoch 156/2000, Training Loss: 359.5846, Validation Loss: 359.4388\n",
      "Epoch 157/2000, Training Loss: 358.7867, Validation Loss: 358.6753\n",
      "Epoch 158/2000, Training Loss: 358.0164, Validation Loss: 357.9383\n",
      "Epoch 159/2000, Training Loss: 357.2728, Validation Loss: 357.2269\n",
      "Epoch 160/2000, Training Loss: 356.5550, Validation Loss: 356.5404\n",
      "Epoch 161/2000, Training Loss: 355.8621, Validation Loss: 355.8777\n",
      "Epoch 162/2000, Training Loss: 355.1932, Validation Loss: 355.2382\n",
      "Epoch 163/2000, Training Loss: 354.5475, Validation Loss: 354.6209\n",
      "Epoch 164/2000, Training Loss: 353.9242, Validation Loss: 354.0251\n",
      "Epoch 165/2000, Training Loss: 353.3225, Validation Loss: 353.4501\n",
      "Epoch 166/2000, Training Loss: 352.7416, Validation Loss: 352.8951\n",
      "Epoch 167/2000, Training Loss: 352.1809, Validation Loss: 352.3594\n",
      "Epoch 168/2000, Training Loss: 351.6397, Validation Loss: 351.8424\n",
      "Epoch 169/2000, Training Loss: 351.1172, Validation Loss: 351.3434\n",
      "Epoch 170/2000, Training Loss: 350.6127, Validation Loss: 350.8618\n",
      "Epoch 171/2000, Training Loss: 350.1258, Validation Loss: 350.3970\n",
      "Epoch 172/2000, Training Loss: 349.6558, Validation Loss: 349.9485\n",
      "Epoch 173/2000, Training Loss: 349.2021, Validation Loss: 349.5154\n",
      "Epoch 174/2000, Training Loss: 348.7640, Validation Loss: 349.0976\n",
      "Epoch 175/2000, Training Loss: 348.3412, Validation Loss: 348.6943\n",
      "Epoch 176/2000, Training Loss: 347.9330, Validation Loss: 348.3050\n",
      "Epoch 177/2000, Training Loss: 347.5390, Validation Loss: 347.9294\n",
      "Epoch 178/2000, Training Loss: 347.1586, Validation Loss: 347.5668\n",
      "Epoch 179/2000, Training Loss: 346.7914, Validation Loss: 347.2169\n",
      "Epoch 180/2000, Training Loss: 346.4370, Validation Loss: 346.8792\n",
      "Epoch 181/2000, Training Loss: 346.0948, Validation Loss: 346.5532\n",
      "Epoch 182/2000, Training Loss: 345.7645, Validation Loss: 346.2387\n",
      "Epoch 183/2000, Training Loss: 345.4456, Validation Loss: 345.9351\n",
      "Epoch 184/2000, Training Loss: 345.1378, Validation Loss: 345.6421\n",
      "Epoch 185/2000, Training Loss: 344.8406, Validation Loss: 345.3593\n",
      "Epoch 186/2000, Training Loss: 344.5538, Validation Loss: 345.0864\n",
      "Epoch 187/2000, Training Loss: 344.2769, Validation Loss: 344.8230\n",
      "Epoch 188/2000, Training Loss: 344.0095, Validation Loss: 344.5688\n",
      "Epoch 189/2000, Training Loss: 343.7515, Validation Loss: 344.3235\n",
      "Epoch 190/2000, Training Loss: 343.5023, Validation Loss: 344.0868\n",
      "Epoch 191/2000, Training Loss: 343.2619, Validation Loss: 343.8582\n",
      "Epoch 192/2000, Training Loss: 343.0297, Validation Loss: 343.6378\n",
      "Epoch 193/2000, Training Loss: 342.8056, Validation Loss: 343.4249\n",
      "Epoch 194/2000, Training Loss: 342.5893, Validation Loss: 343.2196\n",
      "Epoch 195/2000, Training Loss: 342.3805, Validation Loss: 343.0214\n",
      "Epoch 196/2000, Training Loss: 342.1788, Validation Loss: 342.8301\n",
      "Epoch 197/2000, Training Loss: 341.9842, Validation Loss: 342.6455\n",
      "Epoch 198/2000, Training Loss: 341.7963, Validation Loss: 342.4674\n",
      "Epoch 199/2000, Training Loss: 341.6150, Validation Loss: 342.2955\n",
      "Epoch 200/2000, Training Loss: 341.4398, Validation Loss: 342.1296\n",
      "Epoch 201/2000, Training Loss: 341.2708, Validation Loss: 341.9695\n",
      "Epoch 202/2000, Training Loss: 341.1077, Validation Loss: 341.8150\n",
      "Epoch 203/2000, Training Loss: 340.9502, Validation Loss: 341.6659\n",
      "Epoch 204/2000, Training Loss: 340.7981, Validation Loss: 341.5220\n",
      "Epoch 205/2000, Training Loss: 340.6513, Validation Loss: 341.3832\n",
      "Epoch 206/2000, Training Loss: 340.5096, Validation Loss: 341.2491\n",
      "Epoch 207/2000, Training Loss: 340.3728, Validation Loss: 341.1199\n",
      "Epoch 208/2000, Training Loss: 340.2407, Validation Loss: 340.9951\n",
      "Epoch 209/2000, Training Loss: 340.1133, Validation Loss: 340.8747\n",
      "Epoch 210/2000, Training Loss: 339.9902, Validation Loss: 340.7585\n",
      "Epoch 211/2000, Training Loss: 339.8713, Validation Loss: 340.6463\n",
      "Epoch 212/2000, Training Loss: 339.7567, Validation Loss: 340.5381\n",
      "Epoch 213/2000, Training Loss: 339.6459, Validation Loss: 340.4337\n",
      "Epoch 214/2000, Training Loss: 339.5391, Validation Loss: 340.3330\n",
      "Epoch 215/2000, Training Loss: 339.4359, Validation Loss: 340.2357\n",
      "Epoch 216/2000, Training Loss: 339.3363, Validation Loss: 340.1419\n",
      "Epoch 217/2000, Training Loss: 339.2401, Validation Loss: 340.0514\n",
      "Epoch 218/2000, Training Loss: 339.1472, Validation Loss: 339.9640\n",
      "Epoch 219/2000, Training Loss: 339.0576, Validation Loss: 339.8797\n",
      "Epoch 220/2000, Training Loss: 338.9711, Validation Loss: 339.7984\n",
      "Epoch 221/2000, Training Loss: 338.8876, Validation Loss: 339.7199\n",
      "Epoch 222/2000, Training Loss: 338.8069, Validation Loss: 339.6442\n",
      "Epoch 223/2000, Training Loss: 338.7291, Validation Loss: 339.5711\n",
      "Epoch 224/2000, Training Loss: 338.6540, Validation Loss: 339.5005\n",
      "Epoch 225/2000, Training Loss: 338.5814, Validation Loss: 339.4326\n",
      "Epoch 226/2000, Training Loss: 338.5114, Validation Loss: 339.3669\n",
      "Epoch 227/2000, Training Loss: 338.4438, Validation Loss: 339.3036\n",
      "Epoch 228/2000, Training Loss: 338.3785, Validation Loss: 339.2424\n",
      "Epoch 229/2000, Training Loss: 338.3155, Validation Loss: 339.1835\n",
      "Epoch 230/2000, Training Loss: 338.2547, Validation Loss: 339.1266\n",
      "Epoch 231/2000, Training Loss: 338.1959, Validation Loss: 339.0717\n",
      "Epoch 232/2000, Training Loss: 338.1392, Validation Loss: 339.0187\n",
      "Epoch 233/2000, Training Loss: 338.0845, Validation Loss: 338.9675\n",
      "Epoch 234/2000, Training Loss: 338.0316, Validation Loss: 338.9182\n",
      "Epoch 235/2000, Training Loss: 337.9806, Validation Loss: 338.8707\n",
      "Epoch 236/2000, Training Loss: 337.9314, Validation Loss: 338.8248\n",
      "Epoch 237/2000, Training Loss: 337.8838, Validation Loss: 338.7805\n",
      "Epoch 238/2000, Training Loss: 337.8379, Validation Loss: 338.7378\n",
      "Epoch 239/2000, Training Loss: 337.7936, Validation Loss: 338.6965\n",
      "Epoch 240/2000, Training Loss: 337.7508, Validation Loss: 338.6567\n",
      "Epoch 241/2000, Training Loss: 337.7095, Validation Loss: 338.6184\n",
      "Epoch 242/2000, Training Loss: 337.6696, Validation Loss: 338.5814\n",
      "Epoch 243/2000, Training Loss: 337.6311, Validation Loss: 338.5456\n",
      "Epoch 244/2000, Training Loss: 337.5940, Validation Loss: 338.5112\n",
      "Epoch 245/2000, Training Loss: 337.5581, Validation Loss: 338.4779\n",
      "Epoch 246/2000, Training Loss: 337.5235, Validation Loss: 338.4459\n",
      "Epoch 247/2000, Training Loss: 337.4900, Validation Loss: 338.4149\n",
      "Epoch 248/2000, Training Loss: 337.4578, Validation Loss: 338.3851\n",
      "Epoch 249/2000, Training Loss: 337.4266, Validation Loss: 338.3563\n",
      "Epoch 250/2000, Training Loss: 337.3965, Validation Loss: 338.3285\n",
      "Epoch 251/2000, Training Loss: 337.3674, Validation Loss: 338.3017\n",
      "Epoch 252/2000, Training Loss: 337.3394, Validation Loss: 338.2758\n",
      "Epoch 253/2000, Training Loss: 337.3123, Validation Loss: 338.2509\n",
      "Epoch 254/2000, Training Loss: 337.2861, Validation Loss: 338.2269\n",
      "Epoch 255/2000, Training Loss: 337.2609, Validation Loss: 338.2036\n",
      "Epoch 256/2000, Training Loss: 337.2365, Validation Loss: 338.1813\n",
      "Epoch 257/2000, Training Loss: 337.2130, Validation Loss: 338.1597\n",
      "Epoch 258/2000, Training Loss: 337.1903, Validation Loss: 338.1388\n",
      "Epoch 259/2000, Training Loss: 337.1684, Validation Loss: 338.1187\n",
      "Epoch 260/2000, Training Loss: 337.1472, Validation Loss: 338.0994\n",
      "Epoch 261/2000, Training Loss: 337.1268, Validation Loss: 338.0807\n",
      "Epoch 262/2000, Training Loss: 337.1070, Validation Loss: 338.0627\n",
      "Epoch 263/2000, Training Loss: 337.0879, Validation Loss: 338.0453\n",
      "Epoch 264/2000, Training Loss: 337.0695, Validation Loss: 338.0285\n",
      "Epoch 265/2000, Training Loss: 337.0518, Validation Loss: 338.0123\n",
      "Epoch 266/2000, Training Loss: 337.0346, Validation Loss: 337.9967\n",
      "Epoch 267/2000, Training Loss: 337.0181, Validation Loss: 337.9817\n",
      "Epoch 268/2000, Training Loss: 337.0021, Validation Loss: 337.9671\n",
      "Epoch 269/2000, Training Loss: 336.9867, Validation Loss: 337.9531\n",
      "Epoch 270/2000, Training Loss: 336.9717, Validation Loss: 337.9396\n",
      "Epoch 271/2000, Training Loss: 336.9574, Validation Loss: 337.9266\n",
      "Epoch 272/2000, Training Loss: 336.9435, Validation Loss: 337.9140\n",
      "Epoch 273/2000, Training Loss: 336.9301, Validation Loss: 337.9019\n",
      "Epoch 274/2000, Training Loss: 336.9171, Validation Loss: 337.8903\n",
      "Epoch 275/2000, Training Loss: 336.9046, Validation Loss: 337.8790\n",
      "Epoch 276/2000, Training Loss: 336.8925, Validation Loss: 337.8681\n",
      "Epoch 277/2000, Training Loss: 336.8809, Validation Loss: 337.8577\n",
      "Epoch 278/2000, Training Loss: 336.8696, Validation Loss: 337.8475\n",
      "Epoch 279/2000, Training Loss: 336.8587, Validation Loss: 337.8378\n",
      "Epoch 280/2000, Training Loss: 336.8482, Validation Loss: 337.8284\n",
      "Epoch 281/2000, Training Loss: 336.8381, Validation Loss: 337.8194\n",
      "Epoch 282/2000, Training Loss: 336.8283, Validation Loss: 337.8106\n",
      "Epoch 283/2000, Training Loss: 336.8189, Validation Loss: 337.8022\n",
      "Epoch 284/2000, Training Loss: 336.8097, Validation Loss: 337.7940\n",
      "Epoch 285/2000, Training Loss: 336.8009, Validation Loss: 337.7862\n",
      "Epoch 286/2000, Training Loss: 336.7924, Validation Loss: 337.7787\n",
      "Epoch 287/2000, Training Loss: 336.7842, Validation Loss: 337.7714\n",
      "Epoch 288/2000, Training Loss: 336.7763, Validation Loss: 337.7644\n",
      "Epoch 289/2000, Training Loss: 336.7686, Validation Loss: 337.7576\n",
      "Epoch 290/2000, Training Loss: 336.7612, Validation Loss: 337.7510\n",
      "Epoch 291/2000, Training Loss: 336.7541, Validation Loss: 337.7448\n",
      "Epoch 292/2000, Training Loss: 336.7472, Validation Loss: 337.7387\n",
      "Epoch 293/2000, Training Loss: 336.7405, Validation Loss: 337.7329\n",
      "Epoch 294/2000, Training Loss: 336.7341, Validation Loss: 337.7272\n",
      "Epoch 295/2000, Training Loss: 336.7278, Validation Loss: 337.7218\n",
      "Epoch 296/2000, Training Loss: 336.7219, Validation Loss: 337.7166\n",
      "Epoch 297/2000, Training Loss: 336.7160, Validation Loss: 337.7115\n",
      "Epoch 298/2000, Training Loss: 336.7104, Validation Loss: 337.7066\n",
      "Epoch 299/2000, Training Loss: 336.7050, Validation Loss: 337.7019\n",
      "Epoch 300/2000, Training Loss: 336.6998, Validation Loss: 337.6974\n",
      "Epoch 301/2000, Training Loss: 336.6948, Validation Loss: 337.6930\n",
      "Epoch 302/2000, Training Loss: 336.6899, Validation Loss: 337.6888\n",
      "Epoch 303/2000, Training Loss: 336.6852, Validation Loss: 337.6848\n",
      "Epoch 304/2000, Training Loss: 336.6807, Validation Loss: 337.6809\n",
      "Epoch 305/2000, Training Loss: 336.6763, Validation Loss: 337.6771\n",
      "Epoch 306/2000, Training Loss: 336.6721, Validation Loss: 337.6735\n",
      "Epoch 307/2000, Training Loss: 336.6680, Validation Loss: 337.6700\n",
      "Epoch 308/2000, Training Loss: 336.6640, Validation Loss: 337.6666\n",
      "Epoch 309/2000, Training Loss: 336.6602, Validation Loss: 337.6633\n",
      "Epoch 310/2000, Training Loss: 336.6566, Validation Loss: 337.6602\n",
      "Epoch 311/2000, Training Loss: 336.6530, Validation Loss: 337.6572\n",
      "Epoch 312/2000, Training Loss: 336.6495, Validation Loss: 337.6543\n",
      "Epoch 313/2000, Training Loss: 336.6462, Validation Loss: 337.6515\n",
      "Epoch 314/2000, Training Loss: 336.6430, Validation Loss: 337.6488\n",
      "Epoch 315/2000, Training Loss: 336.6400, Validation Loss: 337.6462\n",
      "Epoch 316/2000, Training Loss: 336.6369, Validation Loss: 337.6437\n",
      "Epoch 317/2000, Training Loss: 336.6340, Validation Loss: 337.6413\n",
      "Epoch 318/2000, Training Loss: 336.6313, Validation Loss: 337.6389\n",
      "Epoch 319/2000, Training Loss: 336.6285, Validation Loss: 337.6367\n",
      "Epoch 320/2000, Training Loss: 336.6260, Validation Loss: 337.6345\n",
      "Epoch 321/2000, Training Loss: 336.6234, Validation Loss: 337.6324\n",
      "Epoch 322/2000, Training Loss: 336.6210, Validation Loss: 337.6304\n",
      "Epoch 323/2000, Training Loss: 336.6187, Validation Loss: 337.6285\n",
      "Epoch 324/2000, Training Loss: 336.6164, Validation Loss: 337.6266\n",
      "Epoch 325/2000, Training Loss: 336.6142, Validation Loss: 337.6248\n",
      "Epoch 326/2000, Training Loss: 336.6121, Validation Loss: 337.6231\n",
      "Epoch 327/2000, Training Loss: 336.6100, Validation Loss: 337.6214\n",
      "Epoch 328/2000, Training Loss: 336.6080, Validation Loss: 337.6198\n",
      "Epoch 329/2000, Training Loss: 336.6061, Validation Loss: 337.6183\n",
      "Epoch 330/2000, Training Loss: 336.6043, Validation Loss: 337.6168\n",
      "Epoch 331/2000, Training Loss: 336.6025, Validation Loss: 337.6154\n",
      "Epoch 332/2000, Training Loss: 336.6008, Validation Loss: 337.6140\n",
      "Epoch 333/2000, Training Loss: 336.5991, Validation Loss: 337.6127\n",
      "Epoch 334/2000, Training Loss: 336.5975, Validation Loss: 337.6114\n",
      "Epoch 335/2000, Training Loss: 336.5959, Validation Loss: 337.6101\n",
      "Epoch 336/2000, Training Loss: 336.5945, Validation Loss: 337.6089\n",
      "Epoch 337/2000, Training Loss: 336.5930, Validation Loss: 337.6078\n",
      "Epoch 338/2000, Training Loss: 336.5916, Validation Loss: 337.6067\n",
      "Epoch 339/2000, Training Loss: 336.5903, Validation Loss: 337.6056\n",
      "Epoch 340/2000, Training Loss: 336.5889, Validation Loss: 337.6046\n",
      "Epoch 341/2000, Training Loss: 336.5876, Validation Loss: 337.6036\n",
      "Epoch 342/2000, Training Loss: 336.5865, Validation Loss: 337.6026\n",
      "Epoch 343/2000, Training Loss: 336.5853, Validation Loss: 337.6017\n",
      "Epoch 344/2000, Training Loss: 336.5841, Validation Loss: 337.6008\n",
      "Epoch 345/2000, Training Loss: 336.5830, Validation Loss: 337.6000\n",
      "Epoch 346/2000, Training Loss: 336.5819, Validation Loss: 337.5992\n",
      "Epoch 347/2000, Training Loss: 336.5809, Validation Loss: 337.5984\n",
      "Epoch 348/2000, Training Loss: 336.5799, Validation Loss: 337.5977\n",
      "Epoch 349/2000, Training Loss: 336.5789, Validation Loss: 337.5970\n",
      "Epoch 350/2000, Training Loss: 336.5780, Validation Loss: 337.5962\n",
      "Epoch 351/2000, Training Loss: 336.5771, Validation Loss: 337.5956\n",
      "Epoch 352/2000, Training Loss: 336.5763, Validation Loss: 337.5949\n",
      "Epoch 353/2000, Training Loss: 336.5754, Validation Loss: 337.5942\n",
      "Epoch 354/2000, Training Loss: 336.5746, Validation Loss: 337.5937\n",
      "Epoch 355/2000, Training Loss: 336.5738, Validation Loss: 337.5931\n",
      "Epoch 356/2000, Training Loss: 336.5730, Validation Loss: 337.5925\n",
      "Epoch 357/2000, Training Loss: 336.5723, Validation Loss: 337.5920\n",
      "Epoch 358/2000, Training Loss: 336.5716, Validation Loss: 337.5914\n",
      "Epoch 359/2000, Training Loss: 336.5709, Validation Loss: 337.5909\n",
      "Epoch 360/2000, Training Loss: 336.5702, Validation Loss: 337.5905\n",
      "Epoch 361/2000, Training Loss: 336.5696, Validation Loss: 337.5900\n",
      "Epoch 362/2000, Training Loss: 336.5690, Validation Loss: 337.5896\n",
      "Epoch 363/2000, Training Loss: 336.5683, Validation Loss: 337.5891\n",
      "Epoch 364/2000, Training Loss: 336.5678, Validation Loss: 337.5887\n",
      "Epoch 365/2000, Training Loss: 336.5672, Validation Loss: 337.5883\n",
      "Epoch 366/2000, Training Loss: 336.5667, Validation Loss: 337.5880\n",
      "Epoch 367/2000, Training Loss: 336.5661, Validation Loss: 337.5876\n",
      "Epoch 368/2000, Training Loss: 336.5656, Validation Loss: 337.5872\n",
      "Epoch 369/2000, Training Loss: 336.5651, Validation Loss: 337.5869\n",
      "Epoch 370/2000, Training Loss: 336.5646, Validation Loss: 337.5865\n",
      "Epoch 371/2000, Training Loss: 336.5642, Validation Loss: 337.5862\n",
      "Epoch 372/2000, Training Loss: 336.5637, Validation Loss: 337.5859\n",
      "Epoch 373/2000, Training Loss: 336.5633, Validation Loss: 337.5856\n",
      "Epoch 374/2000, Training Loss: 336.5629, Validation Loss: 337.5854\n",
      "Epoch 375/2000, Training Loss: 336.5625, Validation Loss: 337.5851\n",
      "Epoch 376/2000, Training Loss: 336.5620, Validation Loss: 337.5848\n",
      "Epoch 377/2000, Training Loss: 336.5617, Validation Loss: 337.5845\n",
      "Epoch 378/2000, Training Loss: 336.5613, Validation Loss: 337.5843\n",
      "Epoch 379/2000, Training Loss: 336.5610, Validation Loss: 337.5840\n",
      "Epoch 380/2000, Training Loss: 336.5606, Validation Loss: 337.5838\n",
      "Epoch 381/2000, Training Loss: 336.5603, Validation Loss: 337.5836\n",
      "Epoch 382/2000, Training Loss: 336.5599, Validation Loss: 337.5834\n",
      "Epoch 383/2000, Training Loss: 336.5597, Validation Loss: 337.5832\n",
      "Epoch 384/2000, Training Loss: 336.5593, Validation Loss: 337.5830\n",
      "Epoch 385/2000, Training Loss: 336.5591, Validation Loss: 337.5828\n",
      "Epoch 386/2000, Training Loss: 336.5587, Validation Loss: 337.5826\n",
      "Epoch 387/2000, Training Loss: 336.5584, Validation Loss: 337.5825\n",
      "Epoch 388/2000, Training Loss: 336.5582, Validation Loss: 337.5823\n",
      "Epoch 389/2000, Training Loss: 336.5580, Validation Loss: 337.5822\n",
      "Epoch 390/2000, Training Loss: 336.5577, Validation Loss: 337.5820\n",
      "Epoch 391/2000, Training Loss: 336.5574, Validation Loss: 337.5818\n",
      "Epoch 392/2000, Training Loss: 336.5572, Validation Loss: 337.5817\n",
      "Epoch 393/2000, Training Loss: 336.5569, Validation Loss: 337.5815\n",
      "Epoch 394/2000, Training Loss: 336.5567, Validation Loss: 337.5814\n",
      "Epoch 395/2000, Training Loss: 336.5565, Validation Loss: 337.5813\n",
      "Epoch 396/2000, Training Loss: 336.5563, Validation Loss: 337.5811\n",
      "Epoch 397/2000, Training Loss: 336.5561, Validation Loss: 337.5810\n",
      "Epoch 398/2000, Training Loss: 336.5559, Validation Loss: 337.5809\n",
      "Epoch 399/2000, Training Loss: 336.5557, Validation Loss: 337.5808\n",
      "Epoch 400/2000, Training Loss: 336.5555, Validation Loss: 337.5807\n",
      "Epoch 401/2000, Training Loss: 336.5553, Validation Loss: 337.5806\n",
      "Epoch 402/2000, Training Loss: 336.5552, Validation Loss: 337.5805\n",
      "Epoch 403/2000, Training Loss: 336.5550, Validation Loss: 337.5804\n",
      "Epoch 404/2000, Training Loss: 336.5548, Validation Loss: 337.5803\n",
      "Epoch 405/2000, Training Loss: 336.5547, Validation Loss: 337.5802\n",
      "Epoch 406/2000, Training Loss: 336.5545, Validation Loss: 337.5801\n",
      "Epoch 407/2000, Training Loss: 336.5544, Validation Loss: 337.5800\n",
      "Epoch 408/2000, Training Loss: 336.5542, Validation Loss: 337.5800\n",
      "Epoch 409/2000, Training Loss: 336.5541, Validation Loss: 337.5799\n",
      "Epoch 410/2000, Training Loss: 336.5540, Validation Loss: 337.5798\n",
      "Epoch 411/2000, Training Loss: 336.5538, Validation Loss: 337.5797\n",
      "Epoch 412/2000, Training Loss: 336.5537, Validation Loss: 337.5796\n",
      "Epoch 413/2000, Training Loss: 336.5535, Validation Loss: 337.5796\n",
      "Epoch 414/2000, Training Loss: 336.5534, Validation Loss: 337.5795\n",
      "Epoch 415/2000, Training Loss: 336.5533, Validation Loss: 337.5794\n",
      "Epoch 416/2000, Training Loss: 336.5532, Validation Loss: 337.5794\n",
      "Epoch 417/2000, Training Loss: 336.5530, Validation Loss: 337.5793\n",
      "Epoch 418/2000, Training Loss: 336.5529, Validation Loss: 337.5793\n",
      "Epoch 419/2000, Training Loss: 336.5528, Validation Loss: 337.5792\n",
      "Epoch 420/2000, Training Loss: 336.5528, Validation Loss: 337.5791\n",
      "Epoch 421/2000, Training Loss: 336.5526, Validation Loss: 337.5791\n",
      "Epoch 422/2000, Training Loss: 336.5526, Validation Loss: 337.5790\n",
      "Epoch 423/2000, Training Loss: 336.5525, Validation Loss: 337.5789\n",
      "Epoch 424/2000, Training Loss: 336.5523, Validation Loss: 337.5789\n",
      "Epoch 425/2000, Training Loss: 336.5523, Validation Loss: 337.5789\n",
      "Epoch 426/2000, Training Loss: 336.5522, Validation Loss: 337.5788\n",
      "Epoch 427/2000, Training Loss: 336.5521, Validation Loss: 337.5788\n",
      "Epoch 428/2000, Training Loss: 336.5520, Validation Loss: 337.5787\n",
      "Epoch 429/2000, Training Loss: 336.5519, Validation Loss: 337.5787\n",
      "Epoch 430/2000, Training Loss: 336.5518, Validation Loss: 337.5786\n",
      "Epoch 431/2000, Training Loss: 336.5517, Validation Loss: 337.5786\n",
      "Epoch 432/2000, Training Loss: 336.5517, Validation Loss: 337.5786\n",
      "Epoch 433/2000, Training Loss: 336.5516, Validation Loss: 337.5786\n",
      "Epoch 434/2000, Training Loss: 336.5515, Validation Loss: 337.5785\n",
      "Epoch 435/2000, Training Loss: 336.5514, Validation Loss: 337.5785\n",
      "Epoch 436/2000, Training Loss: 336.5514, Validation Loss: 337.5784\n",
      "Epoch 437/2000, Training Loss: 336.5513, Validation Loss: 337.5784\n",
      "Epoch 438/2000, Training Loss: 336.5512, Validation Loss: 337.5783\n",
      "Epoch 439/2000, Training Loss: 336.5512, Validation Loss: 337.5783\n",
      "Epoch 440/2000, Training Loss: 336.5511, Validation Loss: 337.5782\n",
      "Epoch 441/2000, Training Loss: 336.5511, Validation Loss: 337.5782\n",
      "Epoch 442/2000, Training Loss: 336.5510, Validation Loss: 337.5782\n",
      "Epoch 443/2000, Training Loss: 336.5510, Validation Loss: 337.5782\n",
      "Epoch 444/2000, Training Loss: 336.5509, Validation Loss: 337.5781\n",
      "Epoch 445/2000, Training Loss: 336.5508, Validation Loss: 337.5781\n",
      "Epoch 446/2000, Training Loss: 336.5508, Validation Loss: 337.5781\n",
      "Epoch 447/2000, Training Loss: 336.5507, Validation Loss: 337.5780\n",
      "Epoch 448/2000, Training Loss: 336.5507, Validation Loss: 337.5780\n",
      "Epoch 449/2000, Training Loss: 336.5506, Validation Loss: 337.5780\n",
      "Epoch 450/2000, Training Loss: 336.5505, Validation Loss: 337.5779\n",
      "Epoch 451/2000, Training Loss: 336.5505, Validation Loss: 337.5779\n",
      "Epoch 452/2000, Training Loss: 336.5504, Validation Loss: 337.5779\n",
      "Epoch 453/2000, Training Loss: 336.5504, Validation Loss: 337.5779\n",
      "Epoch 454/2000, Training Loss: 336.5503, Validation Loss: 337.5778\n",
      "Epoch 455/2000, Training Loss: 336.5503, Validation Loss: 337.5778\n",
      "Epoch 456/2000, Training Loss: 336.5503, Validation Loss: 337.5778\n",
      "Epoch 457/2000, Training Loss: 336.5502, Validation Loss: 337.5777\n",
      "Epoch 458/2000, Training Loss: 336.5501, Validation Loss: 337.5777\n",
      "Epoch 459/2000, Training Loss: 336.5501, Validation Loss: 337.5777\n",
      "Epoch 460/2000, Training Loss: 336.5500, Validation Loss: 337.5777\n",
      "Epoch 461/2000, Training Loss: 336.5500, Validation Loss: 337.5776\n",
      "Epoch 462/2000, Training Loss: 336.5500, Validation Loss: 337.5776\n",
      "Epoch 463/2000, Training Loss: 336.5499, Validation Loss: 337.5776\n",
      "Epoch 464/2000, Training Loss: 336.5499, Validation Loss: 337.5776\n",
      "Epoch 465/2000, Training Loss: 336.5499, Validation Loss: 337.5775\n",
      "Epoch 466/2000, Training Loss: 336.5498, Validation Loss: 337.5775\n",
      "Epoch 467/2000, Training Loss: 336.5498, Validation Loss: 337.5775\n",
      "Epoch 468/2000, Training Loss: 336.5497, Validation Loss: 337.5775\n",
      "Epoch 469/2000, Training Loss: 336.5497, Validation Loss: 337.5775\n",
      "Epoch 470/2000, Training Loss: 336.5497, Validation Loss: 337.5774\n",
      "Epoch 471/2000, Training Loss: 336.5496, Validation Loss: 337.5774\n",
      "Epoch 472/2000, Training Loss: 336.5496, Validation Loss: 337.5774\n",
      "Epoch 473/2000, Training Loss: 336.5496, Validation Loss: 337.5773\n",
      "Epoch 474/2000, Training Loss: 336.5495, Validation Loss: 337.5773\n",
      "Epoch 475/2000, Training Loss: 336.5495, Validation Loss: 337.5773\n",
      "Epoch 476/2000, Training Loss: 336.5495, Validation Loss: 337.5773\n",
      "Epoch 477/2000, Training Loss: 336.5494, Validation Loss: 337.5772\n",
      "Epoch 478/2000, Training Loss: 336.5494, Validation Loss: 337.5772\n",
      "Epoch 479/2000, Training Loss: 336.5494, Validation Loss: 337.5772\n",
      "Epoch 480/2000, Training Loss: 336.5494, Validation Loss: 337.5772\n",
      "Epoch 481/2000, Training Loss: 336.5493, Validation Loss: 337.5771\n",
      "Epoch 482/2000, Training Loss: 336.5493, Validation Loss: 337.5771\n",
      "Epoch 483/2000, Training Loss: 336.5493, Validation Loss: 337.5771\n",
      "Epoch 484/2000, Training Loss: 336.5493, Validation Loss: 337.5771\n",
      "Epoch 485/2000, Training Loss: 336.5492, Validation Loss: 337.5771\n",
      "Epoch 486/2000, Training Loss: 336.5492, Validation Loss: 337.5770\n",
      "Epoch 487/2000, Training Loss: 336.5492, Validation Loss: 337.5770\n",
      "Epoch 488/2000, Training Loss: 336.5492, Validation Loss: 337.5770\n",
      "Epoch 489/2000, Training Loss: 336.5491, Validation Loss: 337.5770\n",
      "Epoch 490/2000, Training Loss: 336.5491, Validation Loss: 337.5769\n",
      "Epoch 491/2000, Training Loss: 336.5490, Validation Loss: 337.5769\n",
      "Epoch 492/2000, Training Loss: 336.5490, Validation Loss: 337.5769\n",
      "Epoch 493/2000, Training Loss: 336.5490, Validation Loss: 337.5769\n",
      "Epoch 494/2000, Training Loss: 336.5490, Validation Loss: 337.5768\n",
      "Epoch 495/2000, Training Loss: 336.5490, Validation Loss: 337.5768\n",
      "Epoch 496/2000, Training Loss: 336.5489, Validation Loss: 337.5768\n",
      "Epoch 497/2000, Training Loss: 336.5489, Validation Loss: 337.5768\n",
      "Epoch 498/2000, Training Loss: 336.5489, Validation Loss: 337.5768\n",
      "Epoch 499/2000, Training Loss: 336.5489, Validation Loss: 337.5768\n",
      "Epoch 500/2000, Training Loss: 336.5489, Validation Loss: 337.5768\n",
      "Epoch 501/2000, Training Loss: 336.5488, Validation Loss: 337.5768\n",
      "Epoch 502/2000, Training Loss: 336.5488, Validation Loss: 337.5767\n",
      "Epoch 503/2000, Training Loss: 336.5488, Validation Loss: 337.5767\n",
      "Epoch 504/2000, Training Loss: 336.5487, Validation Loss: 337.5766\n",
      "Epoch 505/2000, Training Loss: 336.5487, Validation Loss: 337.5766\n",
      "Epoch 506/2000, Training Loss: 336.5487, Validation Loss: 337.5766\n",
      "Epoch 507/2000, Training Loss: 336.5487, Validation Loss: 337.5766\n",
      "Epoch 508/2000, Training Loss: 336.5486, Validation Loss: 337.5766\n",
      "Epoch 509/2000, Training Loss: 336.5486, Validation Loss: 337.5765\n",
      "Epoch 510/2000, Training Loss: 336.5486, Validation Loss: 337.5765\n",
      "Epoch 511/2000, Training Loss: 336.5486, Validation Loss: 337.5765\n",
      "Epoch 512/2000, Training Loss: 336.5486, Validation Loss: 337.5765\n",
      "Epoch 513/2000, Training Loss: 336.5486, Validation Loss: 337.5764\n",
      "Epoch 514/2000, Training Loss: 336.5485, Validation Loss: 337.5764\n",
      "Epoch 515/2000, Training Loss: 336.5485, Validation Loss: 337.5764\n",
      "Epoch 516/2000, Training Loss: 336.5485, Validation Loss: 337.5764\n",
      "Epoch 517/2000, Training Loss: 336.5484, Validation Loss: 337.5764\n",
      "Epoch 518/2000, Training Loss: 336.5484, Validation Loss: 337.5763\n",
      "Epoch 519/2000, Training Loss: 336.5484, Validation Loss: 337.5763\n",
      "Epoch 520/2000, Training Loss: 336.5484, Validation Loss: 337.5763\n",
      "Epoch 521/2000, Training Loss: 336.5484, Validation Loss: 337.5763\n",
      "Epoch 522/2000, Training Loss: 336.5483, Validation Loss: 337.5762\n",
      "Epoch 523/2000, Training Loss: 336.5483, Validation Loss: 337.5762\n",
      "Epoch 524/2000, Training Loss: 336.5483, Validation Loss: 337.5762\n",
      "Epoch 525/2000, Training Loss: 336.5483, Validation Loss: 337.5762\n",
      "Epoch 526/2000, Training Loss: 336.5483, Validation Loss: 337.5762\n",
      "Epoch 527/2000, Training Loss: 336.5482, Validation Loss: 337.5761\n",
      "Epoch 528/2000, Training Loss: 336.5482, Validation Loss: 337.5761\n",
      "Epoch 529/2000, Training Loss: 336.5482, Validation Loss: 337.5761\n",
      "Epoch 530/2000, Training Loss: 336.5482, Validation Loss: 337.5761\n",
      "Epoch 531/2000, Training Loss: 336.5482, Validation Loss: 337.5760\n",
      "Epoch 532/2000, Training Loss: 336.5482, Validation Loss: 337.5760\n",
      "Epoch 533/2000, Training Loss: 336.5481, Validation Loss: 337.5760\n",
      "Epoch 534/2000, Training Loss: 336.5481, Validation Loss: 337.5760\n",
      "Epoch 535/2000, Training Loss: 336.5481, Validation Loss: 337.5760\n",
      "Epoch 536/2000, Training Loss: 336.5481, Validation Loss: 337.5759\n",
      "Epoch 537/2000, Training Loss: 336.5481, Validation Loss: 337.5759\n",
      "Epoch 538/2000, Training Loss: 336.5481, Validation Loss: 337.5759\n",
      "Epoch 539/2000, Training Loss: 336.5480, Validation Loss: 337.5759\n",
      "Epoch 540/2000, Training Loss: 336.5480, Validation Loss: 337.5759\n",
      "Epoch 541/2000, Training Loss: 336.5480, Validation Loss: 337.5758\n",
      "Epoch 542/2000, Training Loss: 336.5480, Validation Loss: 337.5758\n",
      "Epoch 543/2000, Training Loss: 336.5480, Validation Loss: 337.5758\n",
      "Epoch 544/2000, Training Loss: 336.5479, Validation Loss: 337.5758\n",
      "Epoch 545/2000, Training Loss: 336.5479, Validation Loss: 337.5757\n",
      "Epoch 546/2000, Training Loss: 336.5479, Validation Loss: 337.5757\n",
      "Epoch 547/2000, Training Loss: 336.5479, Validation Loss: 337.5757\n",
      "Epoch 548/2000, Training Loss: 336.5479, Validation Loss: 337.5757\n",
      "Epoch 549/2000, Training Loss: 336.5479, Validation Loss: 337.5757\n",
      "Epoch 550/2000, Training Loss: 336.5479, Validation Loss: 337.5756\n",
      "Epoch 551/2000, Training Loss: 336.5478, Validation Loss: 337.5756\n",
      "Epoch 552/2000, Training Loss: 336.5478, Validation Loss: 337.5756\n",
      "Epoch 553/2000, Training Loss: 336.5478, Validation Loss: 337.5756\n",
      "Epoch 554/2000, Training Loss: 336.5478, Validation Loss: 337.5755\n",
      "Epoch 555/2000, Training Loss: 336.5478, Validation Loss: 337.5755\n",
      "Epoch 556/2000, Training Loss: 336.5478, Validation Loss: 337.5755\n",
      "Epoch 557/2000, Training Loss: 336.5478, Validation Loss: 337.5755\n",
      "Epoch 558/2000, Training Loss: 336.5477, Validation Loss: 337.5754\n",
      "Epoch 559/2000, Training Loss: 336.5477, Validation Loss: 337.5754\n",
      "Epoch 560/2000, Training Loss: 336.5477, Validation Loss: 337.5754\n",
      "Epoch 561/2000, Training Loss: 336.5477, Validation Loss: 337.5754\n",
      "Epoch 562/2000, Training Loss: 336.5477, Validation Loss: 337.5754\n",
      "Epoch 563/2000, Training Loss: 336.5477, Validation Loss: 337.5753\n",
      "Epoch 564/2000, Training Loss: 336.5477, Validation Loss: 337.5753\n",
      "Epoch 565/2000, Training Loss: 336.5476, Validation Loss: 337.5753\n",
      "Epoch 566/2000, Training Loss: 336.5476, Validation Loss: 337.5753\n",
      "Epoch 567/2000, Training Loss: 336.5476, Validation Loss: 337.5753\n",
      "Epoch 568/2000, Training Loss: 336.5476, Validation Loss: 337.5752\n",
      "Epoch 569/2000, Training Loss: 336.5476, Validation Loss: 337.5752\n",
      "Epoch 570/2000, Training Loss: 336.5476, Validation Loss: 337.5752\n",
      "Epoch 571/2000, Training Loss: 336.5475, Validation Loss: 337.5752\n",
      "Epoch 572/2000, Training Loss: 336.5475, Validation Loss: 337.5752\n",
      "Epoch 573/2000, Training Loss: 336.5475, Validation Loss: 337.5751\n",
      "Epoch 574/2000, Training Loss: 336.5475, Validation Loss: 337.5751\n",
      "Epoch 575/2000, Training Loss: 336.5475, Validation Loss: 337.5751\n",
      "Epoch 576/2000, Training Loss: 336.5475, Validation Loss: 337.5751\n",
      "Epoch 577/2000, Training Loss: 336.5475, Validation Loss: 337.5751\n",
      "Epoch 578/2000, Training Loss: 336.5474, Validation Loss: 337.5751\n",
      "Epoch 579/2000, Training Loss: 336.5474, Validation Loss: 337.5750\n",
      "Epoch 580/2000, Training Loss: 336.5474, Validation Loss: 337.5750\n",
      "Epoch 581/2000, Training Loss: 336.5474, Validation Loss: 337.5750\n",
      "Epoch 582/2000, Training Loss: 336.5474, Validation Loss: 337.5750\n",
      "Epoch 583/2000, Training Loss: 336.5474, Validation Loss: 337.5750\n",
      "Epoch 584/2000, Training Loss: 336.5474, Validation Loss: 337.5749\n",
      "Epoch 585/2000, Training Loss: 336.5474, Validation Loss: 337.5749\n",
      "Epoch 586/2000, Training Loss: 336.5474, Validation Loss: 337.5749\n",
      "Epoch 587/2000, Training Loss: 336.5473, Validation Loss: 337.5749\n",
      "Epoch 588/2000, Training Loss: 336.5473, Validation Loss: 337.5749\n",
      "Epoch 589/2000, Training Loss: 336.5473, Validation Loss: 337.5748\n",
      "Epoch 590/2000, Training Loss: 336.5473, Validation Loss: 337.5748\n",
      "Epoch 591/2000, Training Loss: 336.5473, Validation Loss: 337.5748\n",
      "Epoch 592/2000, Training Loss: 336.5472, Validation Loss: 337.5748\n",
      "Epoch 593/2000, Training Loss: 336.5472, Validation Loss: 337.5748\n",
      "Epoch 594/2000, Training Loss: 336.5472, Validation Loss: 337.5747\n",
      "Epoch 595/2000, Training Loss: 336.5472, Validation Loss: 337.5747\n",
      "Epoch 596/2000, Training Loss: 336.5472, Validation Loss: 337.5747\n",
      "Epoch 597/2000, Training Loss: 336.5472, Validation Loss: 337.5747\n",
      "Epoch 598/2000, Training Loss: 336.5472, Validation Loss: 337.5746\n",
      "Epoch 599/2000, Training Loss: 336.5472, Validation Loss: 337.5746\n",
      "Epoch 600/2000, Training Loss: 336.5471, Validation Loss: 337.5746\n",
      "Epoch 601/2000, Training Loss: 336.5471, Validation Loss: 337.5746\n",
      "Epoch 602/2000, Training Loss: 336.5471, Validation Loss: 337.5746\n",
      "Epoch 603/2000, Training Loss: 336.5471, Validation Loss: 337.5746\n",
      "Epoch 604/2000, Training Loss: 336.5471, Validation Loss: 337.5745\n",
      "Epoch 605/2000, Training Loss: 336.5471, Validation Loss: 337.5745\n",
      "Epoch 606/2000, Training Loss: 336.5471, Validation Loss: 337.5745\n",
      "Epoch 607/2000, Training Loss: 336.5471, Validation Loss: 337.5745\n",
      "Epoch 608/2000, Training Loss: 336.5471, Validation Loss: 337.5745\n",
      "Epoch 609/2000, Training Loss: 336.5471, Validation Loss: 337.5744\n",
      "Epoch 610/2000, Training Loss: 336.5471, Validation Loss: 337.5744\n",
      "Epoch 611/2000, Training Loss: 336.5471, Validation Loss: 337.5744\n",
      "Epoch 612/2000, Training Loss: 336.5470, Validation Loss: 337.5744\n",
      "Epoch 613/2000, Training Loss: 336.5470, Validation Loss: 337.5743\n",
      "Epoch 614/2000, Training Loss: 336.5470, Validation Loss: 337.5743\n",
      "Epoch 615/2000, Training Loss: 336.5470, Validation Loss: 337.5743\n",
      "Epoch 616/2000, Training Loss: 336.5470, Validation Loss: 337.5743\n",
      "Epoch 617/2000, Training Loss: 336.5470, Validation Loss: 337.5743\n",
      "Epoch 618/2000, Training Loss: 336.5470, Validation Loss: 337.5742\n",
      "Epoch 619/2000, Training Loss: 336.5469, Validation Loss: 337.5742\n",
      "Epoch 620/2000, Training Loss: 336.5469, Validation Loss: 337.5742\n",
      "Epoch 621/2000, Training Loss: 336.5469, Validation Loss: 337.5742\n",
      "Epoch 622/2000, Training Loss: 336.5469, Validation Loss: 337.5742\n",
      "Epoch 623/2000, Training Loss: 336.5469, Validation Loss: 337.5742\n",
      "Epoch 624/2000, Training Loss: 336.5469, Validation Loss: 337.5742\n",
      "Epoch 625/2000, Training Loss: 336.5469, Validation Loss: 337.5741\n",
      "Epoch 626/2000, Training Loss: 336.5469, Validation Loss: 337.5741\n",
      "Epoch 627/2000, Training Loss: 336.5468, Validation Loss: 337.5741\n",
      "Epoch 628/2000, Training Loss: 336.5468, Validation Loss: 337.5741\n",
      "Epoch 629/2000, Training Loss: 336.5468, Validation Loss: 337.5740\n",
      "Epoch 630/2000, Training Loss: 336.5468, Validation Loss: 337.5740\n",
      "Epoch 631/2000, Training Loss: 336.5468, Validation Loss: 337.5740\n",
      "Epoch 632/2000, Training Loss: 336.5468, Validation Loss: 337.5740\n",
      "Epoch 633/2000, Training Loss: 336.5468, Validation Loss: 337.5740\n",
      "Epoch 634/2000, Training Loss: 336.5468, Validation Loss: 337.5740\n",
      "Epoch 635/2000, Training Loss: 336.5468, Validation Loss: 337.5740\n",
      "Epoch 636/2000, Training Loss: 336.5468, Validation Loss: 337.5739\n",
      "Epoch 637/2000, Training Loss: 336.5467, Validation Loss: 337.5739\n",
      "Epoch 638/2000, Training Loss: 336.5467, Validation Loss: 337.5739\n",
      "Epoch 639/2000, Training Loss: 336.5467, Validation Loss: 337.5739\n",
      "Epoch 640/2000, Training Loss: 336.5467, Validation Loss: 337.5739\n",
      "Epoch 641/2000, Training Loss: 336.5467, Validation Loss: 337.5738\n",
      "Epoch 642/2000, Training Loss: 336.5467, Validation Loss: 337.5738\n",
      "Epoch 643/2000, Training Loss: 336.5467, Validation Loss: 337.5738\n",
      "Epoch 644/2000, Training Loss: 336.5467, Validation Loss: 337.5738\n",
      "Epoch 645/2000, Training Loss: 336.5466, Validation Loss: 337.5738\n",
      "Epoch 646/2000, Training Loss: 336.5466, Validation Loss: 337.5737\n",
      "Epoch 647/2000, Training Loss: 336.5466, Validation Loss: 337.5737\n",
      "Epoch 648/2000, Training Loss: 336.5466, Validation Loss: 337.5737\n",
      "Epoch 649/2000, Training Loss: 336.5466, Validation Loss: 337.5737\n",
      "Epoch 650/2000, Training Loss: 336.5466, Validation Loss: 337.5737\n",
      "Epoch 651/2000, Training Loss: 336.5466, Validation Loss: 337.5736\n",
      "Epoch 652/2000, Training Loss: 336.5466, Validation Loss: 337.5736\n",
      "Epoch 653/2000, Training Loss: 336.5466, Validation Loss: 337.5736\n",
      "Epoch 654/2000, Training Loss: 336.5466, Validation Loss: 337.5736\n",
      "Epoch 655/2000, Training Loss: 336.5465, Validation Loss: 337.5736\n",
      "Epoch 656/2000, Training Loss: 336.5465, Validation Loss: 337.5736\n",
      "Epoch 657/2000, Training Loss: 336.5465, Validation Loss: 337.5735\n",
      "Epoch 658/2000, Training Loss: 336.5465, Validation Loss: 337.5735\n",
      "Epoch 659/2000, Training Loss: 336.5465, Validation Loss: 337.5735\n",
      "Epoch 660/2000, Training Loss: 336.5465, Validation Loss: 337.5735\n",
      "Epoch 661/2000, Training Loss: 336.5465, Validation Loss: 337.5735\n",
      "Epoch 662/2000, Training Loss: 336.5465, Validation Loss: 337.5735\n",
      "Epoch 663/2000, Training Loss: 336.5465, Validation Loss: 337.5735\n",
      "Epoch 664/2000, Training Loss: 336.5465, Validation Loss: 337.5734\n",
      "Epoch 665/2000, Training Loss: 336.5464, Validation Loss: 337.5734\n",
      "Epoch 666/2000, Training Loss: 336.5464, Validation Loss: 337.5734\n",
      "Epoch 667/2000, Training Loss: 336.5464, Validation Loss: 337.5734\n",
      "Epoch 668/2000, Training Loss: 336.5464, Validation Loss: 337.5734\n",
      "Epoch 669/2000, Training Loss: 336.5464, Validation Loss: 337.5734\n",
      "Epoch 670/2000, Training Loss: 336.5464, Validation Loss: 337.5734\n",
      "Epoch 671/2000, Training Loss: 336.5464, Validation Loss: 337.5733\n",
      "Epoch 672/2000, Training Loss: 336.5464, Validation Loss: 337.5733\n",
      "Epoch 673/2000, Training Loss: 336.5464, Validation Loss: 337.5733\n",
      "Epoch 674/2000, Training Loss: 336.5464, Validation Loss: 337.5733\n",
      "Epoch 675/2000, Training Loss: 336.5464, Validation Loss: 337.5733\n",
      "Epoch 676/2000, Training Loss: 336.5464, Validation Loss: 337.5733\n",
      "Epoch 677/2000, Training Loss: 336.5464, Validation Loss: 337.5733\n",
      "Epoch 678/2000, Training Loss: 336.5464, Validation Loss: 337.5733\n",
      "Epoch 679/2000, Training Loss: 336.5463, Validation Loss: 337.5732\n",
      "Epoch 680/2000, Training Loss: 336.5463, Validation Loss: 337.5732\n",
      "Epoch 681/2000, Training Loss: 336.5463, Validation Loss: 337.5732\n",
      "Epoch 682/2000, Training Loss: 336.5463, Validation Loss: 337.5732\n",
      "Epoch 683/2000, Training Loss: 336.5463, Validation Loss: 337.5732\n",
      "Epoch 684/2000, Training Loss: 336.5463, Validation Loss: 337.5732\n",
      "Epoch 685/2000, Training Loss: 336.5463, Validation Loss: 337.5732\n",
      "Epoch 686/2000, Training Loss: 336.5463, Validation Loss: 337.5731\n",
      "Epoch 687/2000, Training Loss: 336.5463, Validation Loss: 337.5731\n",
      "Epoch 688/2000, Training Loss: 336.5463, Validation Loss: 337.5731\n",
      "Epoch 689/2000, Training Loss: 336.5463, Validation Loss: 337.5731\n",
      "Epoch 690/2000, Training Loss: 336.5462, Validation Loss: 337.5730\n",
      "Epoch 691/2000, Training Loss: 336.5462, Validation Loss: 337.5730\n",
      "Epoch 692/2000, Training Loss: 336.5462, Validation Loss: 337.5730\n",
      "Epoch 693/2000, Training Loss: 336.5462, Validation Loss: 337.5730\n",
      "Epoch 694/2000, Training Loss: 336.5462, Validation Loss: 337.5730\n",
      "Epoch 695/2000, Training Loss: 336.5462, Validation Loss: 337.5730\n",
      "Epoch 696/2000, Training Loss: 336.5462, Validation Loss: 337.5730\n",
      "Epoch 697/2000, Training Loss: 336.5462, Validation Loss: 337.5729\n",
      "Epoch 698/2000, Training Loss: 336.5462, Validation Loss: 337.5729\n",
      "Epoch 699/2000, Training Loss: 336.5462, Validation Loss: 337.5729\n",
      "Epoch 700/2000, Training Loss: 336.5461, Validation Loss: 337.5729\n",
      "Epoch 701/2000, Training Loss: 336.5461, Validation Loss: 337.5729\n",
      "Epoch 702/2000, Training Loss: 336.5461, Validation Loss: 337.5729\n",
      "Epoch 703/2000, Training Loss: 336.5461, Validation Loss: 337.5728\n",
      "Epoch 704/2000, Training Loss: 336.5461, Validation Loss: 337.5728\n",
      "Epoch 705/2000, Training Loss: 336.5461, Validation Loss: 337.5728\n",
      "Epoch 706/2000, Training Loss: 336.5461, Validation Loss: 337.5728\n",
      "Epoch 707/2000, Training Loss: 336.5461, Validation Loss: 337.5728\n",
      "Epoch 708/2000, Training Loss: 336.5461, Validation Loss: 337.5728\n",
      "Epoch 709/2000, Training Loss: 336.5461, Validation Loss: 337.5728\n",
      "Epoch 710/2000, Training Loss: 336.5461, Validation Loss: 337.5728\n",
      "Epoch 711/2000, Training Loss: 336.5461, Validation Loss: 337.5728\n",
      "Epoch 712/2000, Training Loss: 336.5461, Validation Loss: 337.5727\n",
      "Epoch 713/2000, Training Loss: 336.5461, Validation Loss: 337.5727\n",
      "Epoch 714/2000, Training Loss: 336.5461, Validation Loss: 337.5727\n",
      "Epoch 715/2000, Training Loss: 336.5461, Validation Loss: 337.5727\n",
      "Epoch 716/2000, Training Loss: 336.5460, Validation Loss: 337.5727\n",
      "Epoch 717/2000, Training Loss: 336.5460, Validation Loss: 337.5727\n",
      "Epoch 718/2000, Training Loss: 336.5460, Validation Loss: 337.5727\n",
      "Epoch 719/2000, Training Loss: 336.5460, Validation Loss: 337.5727\n",
      "Epoch 720/2000, Training Loss: 336.5460, Validation Loss: 337.5726\n",
      "Epoch 721/2000, Training Loss: 336.5460, Validation Loss: 337.5726\n",
      "Epoch 722/2000, Training Loss: 336.5460, Validation Loss: 337.5726\n",
      "Epoch 723/2000, Training Loss: 336.5460, Validation Loss: 337.5726\n",
      "Epoch 724/2000, Training Loss: 336.5460, Validation Loss: 337.5726\n",
      "Epoch 725/2000, Training Loss: 336.5460, Validation Loss: 337.5726\n",
      "Epoch 726/2000, Training Loss: 336.5460, Validation Loss: 337.5725\n",
      "Epoch 727/2000, Training Loss: 336.5460, Validation Loss: 337.5725\n",
      "Epoch 728/2000, Training Loss: 336.5460, Validation Loss: 337.5725\n",
      "Epoch 729/2000, Training Loss: 336.5459, Validation Loss: 337.5725\n",
      "Epoch 730/2000, Training Loss: 336.5459, Validation Loss: 337.5725\n",
      "Epoch 731/2000, Training Loss: 336.5459, Validation Loss: 337.5725\n",
      "Epoch 732/2000, Training Loss: 336.5459, Validation Loss: 337.5725\n",
      "Epoch 733/2000, Training Loss: 336.5459, Validation Loss: 337.5724\n",
      "Epoch 734/2000, Training Loss: 336.5459, Validation Loss: 337.5724\n",
      "Epoch 735/2000, Training Loss: 336.5459, Validation Loss: 337.5724\n",
      "Epoch 736/2000, Training Loss: 336.5459, Validation Loss: 337.5724\n",
      "Epoch 737/2000, Training Loss: 336.5459, Validation Loss: 337.5724\n",
      "Epoch 738/2000, Training Loss: 336.5459, Validation Loss: 337.5724\n",
      "Epoch 739/2000, Training Loss: 336.5459, Validation Loss: 337.5724\n",
      "Epoch 740/2000, Training Loss: 336.5459, Validation Loss: 337.5724\n",
      "Epoch 741/2000, Training Loss: 336.5459, Validation Loss: 337.5724\n",
      "Epoch 742/2000, Training Loss: 336.5459, Validation Loss: 337.5724\n",
      "Epoch 743/2000, Training Loss: 336.5459, Validation Loss: 337.5723\n",
      "Epoch 744/2000, Training Loss: 336.5459, Validation Loss: 337.5723\n",
      "Epoch 745/2000, Training Loss: 336.5459, Validation Loss: 337.5723\n",
      "Epoch 746/2000, Training Loss: 336.5459, Validation Loss: 337.5723\n",
      "Epoch 747/2000, Training Loss: 336.5458, Validation Loss: 337.5723\n",
      "Epoch 748/2000, Training Loss: 336.5458, Validation Loss: 337.5722\n",
      "Epoch 749/2000, Training Loss: 336.5458, Validation Loss: 337.5722\n",
      "Epoch 750/2000, Training Loss: 336.5458, Validation Loss: 337.5722\n",
      "Epoch 751/2000, Training Loss: 336.5458, Validation Loss: 337.5722\n",
      "Epoch 752/2000, Training Loss: 336.5458, Validation Loss: 337.5722\n",
      "Epoch 753/2000, Training Loss: 336.5458, Validation Loss: 337.5722\n",
      "Epoch 754/2000, Training Loss: 336.5458, Validation Loss: 337.5722\n",
      "Epoch 755/2000, Training Loss: 336.5458, Validation Loss: 337.5722\n",
      "Epoch 756/2000, Training Loss: 336.5458, Validation Loss: 337.5721\n",
      "Epoch 757/2000, Training Loss: 336.5458, Validation Loss: 337.5721\n",
      "Epoch 758/2000, Training Loss: 336.5458, Validation Loss: 337.5721\n",
      "Epoch 759/2000, Training Loss: 336.5457, Validation Loss: 337.5721\n",
      "Epoch 760/2000, Training Loss: 336.5457, Validation Loss: 337.5721\n",
      "Epoch 761/2000, Training Loss: 336.5457, Validation Loss: 337.5721\n",
      "Epoch 762/2000, Training Loss: 336.5457, Validation Loss: 337.5721\n",
      "Epoch 763/2000, Training Loss: 336.5457, Validation Loss: 337.5721\n",
      "Epoch 764/2000, Training Loss: 336.5457, Validation Loss: 337.5721\n",
      "Epoch 765/2000, Training Loss: 336.5457, Validation Loss: 337.5721\n",
      "Epoch 766/2000, Training Loss: 336.5457, Validation Loss: 337.5720\n",
      "Epoch 767/2000, Training Loss: 336.5457, Validation Loss: 337.5720\n",
      "Epoch 768/2000, Training Loss: 336.5457, Validation Loss: 337.5720\n",
      "Epoch 769/2000, Training Loss: 336.5457, Validation Loss: 337.5720\n",
      "Epoch 770/2000, Training Loss: 336.5457, Validation Loss: 337.5720\n",
      "Epoch 771/2000, Training Loss: 336.5457, Validation Loss: 337.5720\n",
      "Epoch 772/2000, Training Loss: 336.5457, Validation Loss: 337.5719\n",
      "Epoch 773/2000, Training Loss: 336.5457, Validation Loss: 337.5719\n",
      "Epoch 774/2000, Training Loss: 336.5457, Validation Loss: 337.5719\n",
      "Epoch 775/2000, Training Loss: 336.5457, Validation Loss: 337.5719\n",
      "Epoch 776/2000, Training Loss: 336.5457, Validation Loss: 337.5719\n",
      "Epoch 777/2000, Training Loss: 336.5457, Validation Loss: 337.5719\n",
      "Epoch 778/2000, Training Loss: 336.5456, Validation Loss: 337.5719\n",
      "Epoch 779/2000, Training Loss: 336.5456, Validation Loss: 337.5719\n",
      "Epoch 780/2000, Training Loss: 336.5456, Validation Loss: 337.5719\n",
      "Epoch 781/2000, Training Loss: 336.5456, Validation Loss: 337.5719\n",
      "Epoch 782/2000, Training Loss: 336.5456, Validation Loss: 337.5718\n",
      "Epoch 783/2000, Training Loss: 336.5456, Validation Loss: 337.5718\n",
      "Epoch 784/2000, Training Loss: 336.5456, Validation Loss: 337.5718\n",
      "Epoch 785/2000, Training Loss: 336.5456, Validation Loss: 337.5718\n",
      "Epoch 786/2000, Training Loss: 336.5456, Validation Loss: 337.5718\n",
      "Epoch 787/2000, Training Loss: 336.5456, Validation Loss: 337.5718\n",
      "Epoch 788/2000, Training Loss: 336.5456, Validation Loss: 337.5718\n",
      "Epoch 789/2000, Training Loss: 336.5456, Validation Loss: 337.5717\n",
      "Epoch 790/2000, Training Loss: 336.5456, Validation Loss: 337.5717\n",
      "Epoch 791/2000, Training Loss: 336.5456, Validation Loss: 337.5717\n",
      "Epoch 792/2000, Training Loss: 336.5456, Validation Loss: 337.5717\n",
      "Epoch 793/2000, Training Loss: 336.5455, Validation Loss: 337.5717\n",
      "Epoch 794/2000, Training Loss: 336.5456, Validation Loss: 337.5717\n",
      "Epoch 795/2000, Training Loss: 336.5456, Validation Loss: 337.5717\n",
      "Epoch 796/2000, Training Loss: 336.5455, Validation Loss: 337.5717\n",
      "Epoch 797/2000, Training Loss: 336.5455, Validation Loss: 337.5717\n",
      "Epoch 798/2000, Training Loss: 336.5455, Validation Loss: 337.5717\n",
      "Epoch 799/2000, Training Loss: 336.5455, Validation Loss: 337.5717\n",
      "Epoch 800/2000, Training Loss: 336.5455, Validation Loss: 337.5717\n",
      "Epoch 801/2000, Training Loss: 336.5455, Validation Loss: 337.5717\n",
      "Epoch 802/2000, Training Loss: 336.5455, Validation Loss: 337.5717\n",
      "Epoch 803/2000, Training Loss: 336.5455, Validation Loss: 337.5717\n",
      "Epoch 804/2000, Training Loss: 336.5455, Validation Loss: 337.5717\n",
      "Epoch 805/2000, Training Loss: 336.5455, Validation Loss: 337.5717\n",
      "Epoch 806/2000, Training Loss: 336.5455, Validation Loss: 337.5716\n",
      "Epoch 807/2000, Training Loss: 336.5455, Validation Loss: 337.5716\n",
      "Epoch 808/2000, Training Loss: 336.5455, Validation Loss: 337.5716\n",
      "Epoch 809/2000, Training Loss: 336.5455, Validation Loss: 337.5716\n",
      "Epoch 810/2000, Training Loss: 336.5454, Validation Loss: 337.5716\n",
      "Epoch 811/2000, Training Loss: 336.5454, Validation Loss: 337.5716\n",
      "Epoch 812/2000, Training Loss: 336.5454, Validation Loss: 337.5716\n",
      "Epoch 813/2000, Training Loss: 336.5454, Validation Loss: 337.5716\n",
      "Epoch 814/2000, Training Loss: 336.5454, Validation Loss: 337.5715\n",
      "Epoch 815/2000, Training Loss: 336.5454, Validation Loss: 337.5715\n",
      "Epoch 816/2000, Training Loss: 336.5454, Validation Loss: 337.5715\n",
      "Epoch 817/2000, Training Loss: 336.5454, Validation Loss: 337.5715\n",
      "Epoch 818/2000, Training Loss: 336.5454, Validation Loss: 337.5715\n",
      "Epoch 819/2000, Training Loss: 336.5454, Validation Loss: 337.5715\n",
      "Epoch 820/2000, Training Loss: 336.5454, Validation Loss: 337.5715\n",
      "Epoch 821/2000, Training Loss: 336.5454, Validation Loss: 337.5715\n",
      "Epoch 822/2000, Training Loss: 336.5454, Validation Loss: 337.5715\n",
      "Epoch 823/2000, Training Loss: 336.5454, Validation Loss: 337.5715\n",
      "Epoch 824/2000, Training Loss: 336.5454, Validation Loss: 337.5714\n",
      "Epoch 825/2000, Training Loss: 336.5454, Validation Loss: 337.5714\n",
      "Epoch 826/2000, Training Loss: 336.5454, Validation Loss: 337.5714\n",
      "Epoch 827/2000, Training Loss: 336.5454, Validation Loss: 337.5714\n",
      "Epoch 828/2000, Training Loss: 336.5454, Validation Loss: 337.5714\n",
      "Epoch 829/2000, Training Loss: 336.5454, Validation Loss: 337.5714\n",
      "Epoch 830/2000, Training Loss: 336.5454, Validation Loss: 337.5714\n",
      "Epoch 831/2000, Training Loss: 336.5453, Validation Loss: 337.5714\n",
      "Epoch 832/2000, Training Loss: 336.5453, Validation Loss: 337.5714\n",
      "Epoch 833/2000, Training Loss: 336.5453, Validation Loss: 337.5714\n",
      "Epoch 834/2000, Training Loss: 336.5453, Validation Loss: 337.5714\n",
      "Epoch 835/2000, Training Loss: 336.5453, Validation Loss: 337.5714\n",
      "Epoch 836/2000, Training Loss: 336.5453, Validation Loss: 337.5714\n",
      "Epoch 837/2000, Training Loss: 336.5453, Validation Loss: 337.5714\n",
      "Epoch 838/2000, Training Loss: 336.5453, Validation Loss: 337.5713\n",
      "Epoch 839/2000, Training Loss: 336.5453, Validation Loss: 337.5713\n",
      "Epoch 840/2000, Training Loss: 336.5453, Validation Loss: 337.5713\n",
      "Epoch 841/2000, Training Loss: 336.5453, Validation Loss: 337.5713\n",
      "Epoch 842/2000, Training Loss: 336.5453, Validation Loss: 337.5713\n",
      "Epoch 843/2000, Training Loss: 336.5453, Validation Loss: 337.5713\n",
      "Epoch 844/2000, Training Loss: 336.5453, Validation Loss: 337.5713\n",
      "Epoch 845/2000, Training Loss: 336.5453, Validation Loss: 337.5713\n",
      "Epoch 846/2000, Training Loss: 336.5453, Validation Loss: 337.5712\n",
      "Epoch 847/2000, Training Loss: 336.5453, Validation Loss: 337.5712\n",
      "Epoch 848/2000, Training Loss: 336.5453, Validation Loss: 337.5712\n",
      "Epoch 849/2000, Training Loss: 336.5453, Validation Loss: 337.5712\n",
      "Epoch 850/2000, Training Loss: 336.5453, Validation Loss: 337.5712\n",
      "Epoch 851/2000, Training Loss: 336.5453, Validation Loss: 337.5712\n",
      "Epoch 852/2000, Training Loss: 336.5453, Validation Loss: 337.5712\n",
      "Epoch 853/2000, Training Loss: 336.5453, Validation Loss: 337.5712\n",
      "Epoch 854/2000, Training Loss: 336.5453, Validation Loss: 337.5712\n",
      "Epoch 855/2000, Training Loss: 336.5452, Validation Loss: 337.5712\n",
      "Epoch 856/2000, Training Loss: 336.5453, Validation Loss: 337.5712\n",
      "Epoch 857/2000, Training Loss: 336.5452, Validation Loss: 337.5712\n",
      "Epoch 858/2000, Training Loss: 336.5452, Validation Loss: 337.5712\n",
      "Epoch 859/2000, Training Loss: 336.5452, Validation Loss: 337.5711\n",
      "Epoch 860/2000, Training Loss: 336.5452, Validation Loss: 337.5711\n",
      "Epoch 861/2000, Training Loss: 336.5452, Validation Loss: 337.5711\n",
      "Epoch 862/2000, Training Loss: 336.5452, Validation Loss: 337.5711\n",
      "Epoch 863/2000, Training Loss: 336.5452, Validation Loss: 337.5711\n",
      "Epoch 864/2000, Training Loss: 336.5452, Validation Loss: 337.5711\n",
      "Epoch 865/2000, Training Loss: 336.5452, Validation Loss: 337.5711\n",
      "Epoch 866/2000, Training Loss: 336.5452, Validation Loss: 337.5711\n",
      "Epoch 867/2000, Training Loss: 336.5452, Validation Loss: 337.5711\n",
      "Epoch 868/2000, Training Loss: 336.5452, Validation Loss: 337.5710\n",
      "Epoch 869/2000, Training Loss: 336.5452, Validation Loss: 337.5710\n",
      "Epoch 870/2000, Training Loss: 336.5452, Validation Loss: 337.5710\n",
      "Epoch 871/2000, Training Loss: 336.5452, Validation Loss: 337.5710\n",
      "Epoch 872/2000, Training Loss: 336.5452, Validation Loss: 337.5710\n",
      "Epoch 873/2000, Training Loss: 336.5452, Validation Loss: 337.5710\n",
      "Epoch 874/2000, Training Loss: 336.5452, Validation Loss: 337.5710\n",
      "Epoch 875/2000, Training Loss: 336.5452, Validation Loss: 337.5710\n",
      "Epoch 876/2000, Training Loss: 336.5451, Validation Loss: 337.5710\n",
      "Epoch 877/2000, Training Loss: 336.5451, Validation Loss: 337.5710\n",
      "Epoch 878/2000, Training Loss: 336.5451, Validation Loss: 337.5710\n",
      "Epoch 879/2000, Training Loss: 336.5451, Validation Loss: 337.5710\n",
      "Epoch 880/2000, Training Loss: 336.5451, Validation Loss: 337.5710\n",
      "Epoch 881/2000, Training Loss: 336.5451, Validation Loss: 337.5710\n",
      "Epoch 882/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 883/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 884/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 885/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 886/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 887/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 888/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 889/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 890/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 891/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 892/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 893/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 894/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 895/2000, Training Loss: 336.5451, Validation Loss: 337.5709\n",
      "Epoch 896/2000, Training Loss: 336.5450, Validation Loss: 337.5708\n",
      "Epoch 897/2000, Training Loss: 336.5450, Validation Loss: 337.5708\n",
      "Epoch 898/2000, Training Loss: 336.5450, Validation Loss: 337.5708\n",
      "Epoch 899/2000, Training Loss: 336.5451, Validation Loss: 337.5708\n",
      "Epoch 900/2000, Training Loss: 336.5451, Validation Loss: 337.5708\n",
      "Epoch 901/2000, Training Loss: 336.5450, Validation Loss: 337.5708\n",
      "Epoch 902/2000, Training Loss: 336.5450, Validation Loss: 337.5708\n",
      "Epoch 903/2000, Training Loss: 336.5450, Validation Loss: 337.5708\n",
      "Epoch 904/2000, Training Loss: 336.5450, Validation Loss: 337.5708\n",
      "Epoch 905/2000, Training Loss: 336.5450, Validation Loss: 337.5708\n",
      "Epoch 906/2000, Training Loss: 336.5450, Validation Loss: 337.5708\n",
      "Epoch 907/2000, Training Loss: 336.5450, Validation Loss: 337.5708\n",
      "Epoch 908/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 909/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 910/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 911/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 912/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 913/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 914/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 915/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 916/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 917/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 918/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 919/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 920/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 921/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 922/2000, Training Loss: 336.5450, Validation Loss: 337.5707\n",
      "Epoch 923/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 924/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 925/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 926/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 927/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 928/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 929/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 930/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 931/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 932/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 933/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 934/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 935/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 936/2000, Training Loss: 336.5450, Validation Loss: 337.5706\n",
      "Epoch 937/2000, Training Loss: 336.5449, Validation Loss: 337.5706\n",
      "Epoch 938/2000, Training Loss: 336.5449, Validation Loss: 337.5706\n",
      "Epoch 939/2000, Training Loss: 336.5449, Validation Loss: 337.5706\n",
      "Epoch 940/2000, Training Loss: 336.5449, Validation Loss: 337.5706\n",
      "Epoch 941/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 942/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 943/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 944/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 945/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 946/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 947/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 948/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 949/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 950/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 951/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 952/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 953/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 954/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 955/2000, Training Loss: 336.5449, Validation Loss: 337.5705\n",
      "Epoch 956/2000, Training Loss: 336.5449, Validation Loss: 337.5704\n",
      "Epoch 957/2000, Training Loss: 336.5449, Validation Loss: 337.5704\n",
      "Epoch 958/2000, Training Loss: 336.5449, Validation Loss: 337.5704\n",
      "Epoch 959/2000, Training Loss: 336.5448, Validation Loss: 337.5704\n",
      "Epoch 960/2000, Training Loss: 336.5448, Validation Loss: 337.5704\n",
      "Epoch 961/2000, Training Loss: 336.5449, Validation Loss: 337.5704\n",
      "Epoch 962/2000, Training Loss: 336.5449, Validation Loss: 337.5704\n",
      "Epoch 963/2000, Training Loss: 336.5448, Validation Loss: 337.5704\n",
      "Epoch 964/2000, Training Loss: 336.5448, Validation Loss: 337.5704\n",
      "Epoch 965/2000, Training Loss: 336.5448, Validation Loss: 337.5704\n",
      "Epoch 966/2000, Training Loss: 336.5448, Validation Loss: 337.5704\n",
      "Epoch 967/2000, Training Loss: 336.5448, Validation Loss: 337.5704\n",
      "Epoch 968/2000, Training Loss: 336.5448, Validation Loss: 337.5704\n",
      "Epoch 969/2000, Training Loss: 336.5448, Validation Loss: 337.5704\n",
      "Epoch 970/2000, Training Loss: 336.5448, Validation Loss: 337.5704\n",
      "Epoch 971/2000, Training Loss: 336.5448, Validation Loss: 337.5704\n",
      "Epoch 972/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 973/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 974/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 975/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 976/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 977/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 978/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 979/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 980/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 981/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 982/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 983/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 984/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 985/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 986/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 987/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 988/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 989/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 990/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 991/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 992/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 993/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 994/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 995/2000, Training Loss: 336.5448, Validation Loss: 337.5702\n",
      "Epoch 996/2000, Training Loss: 336.5448, Validation Loss: 337.5703\n",
      "Epoch 997/2000, Training Loss: 336.5448, Validation Loss: 337.5702\n",
      "Epoch 998/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 999/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 1000/2000, Training Loss: 336.5448, Validation Loss: 337.5702\n",
      "Epoch 1001/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 1002/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 1003/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 1004/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 1005/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 1006/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 1007/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 1008/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 1009/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 1010/2000, Training Loss: 336.5447, Validation Loss: 337.5702\n",
      "Epoch 1011/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1012/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1013/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1014/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1015/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1016/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1017/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1018/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1019/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1020/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1021/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1022/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1023/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1024/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1025/2000, Training Loss: 336.5446, Validation Loss: 337.5701\n",
      "Epoch 1026/2000, Training Loss: 336.5447, Validation Loss: 337.5701\n",
      "Epoch 1027/2000, Training Loss: 336.5446, Validation Loss: 337.5701\n",
      "Epoch 1028/2000, Training Loss: 336.5446, Validation Loss: 337.5701\n",
      "Epoch 1029/2000, Training Loss: 336.5446, Validation Loss: 337.5701\n",
      "Epoch 1030/2000, Training Loss: 336.5446, Validation Loss: 337.5701\n",
      "Epoch 1031/2000, Training Loss: 336.5446, Validation Loss: 337.5701\n",
      "Epoch 1032/2000, Training Loss: 336.5446, Validation Loss: 337.5701\n",
      "Epoch 1033/2000, Training Loss: 336.5446, Validation Loss: 337.5701\n",
      "Epoch 1034/2000, Training Loss: 336.5446, Validation Loss: 337.5701\n",
      "Epoch 1035/2000, Training Loss: 336.5446, Validation Loss: 337.5701\n",
      "Epoch 1036/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1037/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1038/2000, Training Loss: 336.5446, Validation Loss: 337.5701\n",
      "Epoch 1039/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1040/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1041/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1042/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1043/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1044/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1045/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1046/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1047/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1048/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1049/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1050/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1051/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1052/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1053/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1054/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1055/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1056/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1057/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1058/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1059/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1060/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1061/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1062/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1063/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1064/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1065/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1066/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1067/2000, Training Loss: 336.5446, Validation Loss: 337.5700\n",
      "Epoch 1068/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1069/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1070/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1071/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1072/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1073/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1074/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1075/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1076/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1077/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1078/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1079/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1080/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1081/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1082/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1083/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1084/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1085/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1086/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1087/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1088/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1089/2000, Training Loss: 336.5446, Validation Loss: 337.5699\n",
      "Epoch 1090/2000, Training Loss: 336.5445, Validation Loss: 337.5699\n",
      "Epoch 1091/2000, Training Loss: 336.5445, Validation Loss: 337.5699\n",
      "Epoch 1092/2000, Training Loss: 336.5445, Validation Loss: 337.5699\n",
      "Epoch 1093/2000, Training Loss: 336.5445, Validation Loss: 337.5699\n",
      "Epoch 1094/2000, Training Loss: 336.5445, Validation Loss: 337.5699\n",
      "Epoch 1095/2000, Training Loss: 336.5445, Validation Loss: 337.5699\n",
      "Epoch 1096/2000, Training Loss: 336.5445, Validation Loss: 337.5699\n",
      "Epoch 1097/2000, Training Loss: 336.5445, Validation Loss: 337.5699\n",
      "Epoch 1098/2000, Training Loss: 336.5445, Validation Loss: 337.5699\n",
      "Epoch 1099/2000, Training Loss: 336.5445, Validation Loss: 337.5699\n",
      "Epoch 1100/2000, Training Loss: 336.5445, Validation Loss: 337.5699\n",
      "Epoch 1101/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1102/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1103/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1104/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1105/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1106/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1107/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1108/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1109/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1110/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1111/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1112/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1113/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1114/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1115/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1116/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1117/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1118/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1119/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1120/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1121/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1122/2000, Training Loss: 336.5445, Validation Loss: 337.5698\n",
      "Epoch 1123/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1124/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1125/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1126/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1127/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1128/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1129/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1130/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1131/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1132/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1133/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1134/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1135/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1136/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1137/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1138/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1139/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1140/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1141/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1142/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1143/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1144/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1145/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1146/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1147/2000, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1148/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1149/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1150/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1151/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1152/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1153/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1154/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1155/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1156/2000, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1157/2000, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1158/2000, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1159/2000, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1160/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1161/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1162/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1163/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1164/2000, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1165/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1166/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1167/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1168/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1169/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1170/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1171/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1172/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1173/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1174/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1175/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1176/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1177/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1178/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1179/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1180/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1181/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1182/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1183/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1184/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1185/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1186/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1187/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1188/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1189/2000, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1190/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1191/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1192/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1193/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1194/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1195/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1196/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1197/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1198/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1199/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1200/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1201/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1202/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1203/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1204/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1205/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1206/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1207/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1208/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1209/2000, Training Loss: 336.5444, Validation Loss: 337.5695\n",
      "Epoch 1210/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1211/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1212/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1213/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1214/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1215/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1216/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1217/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1218/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1219/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1220/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1221/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1222/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1223/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1224/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1225/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1226/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1227/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1228/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1229/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1230/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1231/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1232/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1233/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1234/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1235/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1236/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1237/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1238/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1239/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1240/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1241/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1242/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1243/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1244/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1245/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1246/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1247/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1248/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1249/2000, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1250/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1251/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1252/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1253/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1254/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1255/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1256/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1257/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1258/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1259/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1260/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1261/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1262/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1263/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1264/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1265/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1266/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1267/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1268/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1269/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1270/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1271/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1272/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1273/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1274/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1275/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1276/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1277/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1278/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1279/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1280/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1281/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1282/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1283/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1284/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1285/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1286/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1287/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1288/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1289/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1290/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1291/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1292/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1293/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1294/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1295/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1296/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1297/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1298/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1299/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1300/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1301/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1302/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1303/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1304/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1305/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1306/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1307/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1308/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1309/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1310/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1311/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1312/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1313/2000, Training Loss: 336.5443, Validation Loss: 337.5694\n",
      "Epoch 1314/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1315/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1316/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1317/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1318/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1319/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1320/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1321/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1322/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1323/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1324/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1325/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1326/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1327/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1328/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1329/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1330/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1331/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1332/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1333/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1334/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1335/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1336/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1337/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1338/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1339/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1340/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1341/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1342/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1343/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1344/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1345/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1346/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1347/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1348/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1349/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1350/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1351/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1352/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1353/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1354/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1355/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1356/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1357/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1358/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1359/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1360/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1361/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1362/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1363/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1364/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1365/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1366/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1367/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1368/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1369/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1370/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1371/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1372/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1373/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1374/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1375/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1376/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1377/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1378/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1379/2000, Training Loss: 336.5443, Validation Loss: 337.5693\n",
      "Epoch 1380/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1381/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1382/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1383/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1384/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1385/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1386/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1387/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1388/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1389/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1390/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1391/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1392/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1393/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1394/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1395/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1396/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1397/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1398/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1399/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1400/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1401/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1402/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1403/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1404/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1405/2000, Training Loss: 336.5443, Validation Loss: 337.5692\n",
      "Epoch 1406/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1407/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1408/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1409/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1410/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1411/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1412/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1413/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1414/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1415/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1416/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1417/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1418/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1419/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1420/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1421/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1422/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1423/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1424/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1425/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1426/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1427/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1428/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1429/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1430/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1431/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1432/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1433/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1434/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1435/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1436/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1437/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1438/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1439/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1440/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1441/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1442/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1443/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1444/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1445/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1446/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1447/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1448/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1449/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1450/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1451/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1452/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1453/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1454/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1455/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1456/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1457/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1458/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1459/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1460/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1461/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1462/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1463/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1464/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1465/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1466/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1467/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1468/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1469/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1470/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1471/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1472/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1473/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1474/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1475/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1476/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1477/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1478/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1479/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1480/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1481/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1482/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1483/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1484/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1485/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1486/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1487/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1488/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1489/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1490/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1491/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1492/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1493/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1494/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1495/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1496/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1497/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1498/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1499/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1500/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1501/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1502/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1503/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1504/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1505/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1506/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1507/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1508/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1509/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1510/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1511/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1512/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1513/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1514/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1515/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1516/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1517/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1518/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1519/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1520/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1521/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1522/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1523/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1524/2000, Training Loss: 336.5442, Validation Loss: 337.5692\n",
      "Epoch 1525/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1526/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1527/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1528/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1529/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1530/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1531/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1532/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1533/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1534/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1535/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1536/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1537/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1538/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1539/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1540/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1541/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1542/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1543/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1544/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1545/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1546/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1547/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1548/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1549/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1550/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1551/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1552/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1553/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1554/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1555/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1556/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1557/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1558/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1559/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1560/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1561/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1562/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1563/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1564/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1565/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1566/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1567/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1568/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1569/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1570/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1571/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1572/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1573/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1574/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1575/2000, Training Loss: 336.5442, Validation Loss: 337.5691\n",
      "Epoch 1576/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1577/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1578/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1579/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1580/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1581/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1582/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1583/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1584/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1585/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1586/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1587/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1588/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1589/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1590/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1591/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1592/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1593/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1594/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1595/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1596/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1597/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1598/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1599/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1600/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1601/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1602/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1603/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1604/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1605/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1606/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1607/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1608/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1609/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1610/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1611/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1612/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1613/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1614/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1615/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1616/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1617/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1618/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1619/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1620/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1621/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1622/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1623/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1624/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1625/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1626/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1627/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1628/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1629/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1630/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1631/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1632/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1633/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1634/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1635/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1636/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1637/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1638/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1639/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1640/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1641/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1642/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1643/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1644/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1645/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1646/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1647/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1648/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1649/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1650/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1651/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1652/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1653/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1654/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1655/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1656/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1657/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1658/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1659/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1660/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1661/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1662/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1663/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1664/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1665/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1666/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1667/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1668/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1669/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1670/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1671/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1672/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1673/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1674/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1675/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1676/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1677/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1678/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1679/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1680/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1681/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1682/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1683/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1684/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1685/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1686/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1687/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1688/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1689/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1690/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1691/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1692/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1693/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1694/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1695/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1696/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1697/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1698/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1699/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1700/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1701/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1702/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1703/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1704/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1705/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1706/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1707/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1708/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1709/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1710/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1711/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1712/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1713/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1714/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1715/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1716/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1717/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1718/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1719/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1720/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1721/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1722/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1723/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1724/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1725/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1726/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1727/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1728/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1729/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1730/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1731/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1732/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1733/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1734/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1735/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1736/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1737/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1738/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1739/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1740/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1741/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1742/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1743/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1744/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1745/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1746/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1747/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1748/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1749/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1750/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1751/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1752/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1753/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1754/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1755/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1756/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1757/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1758/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1759/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1760/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1761/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1762/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1763/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1764/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1765/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1766/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1767/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1768/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1769/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1770/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1771/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1772/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1773/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1774/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1775/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1776/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1777/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1778/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1779/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1780/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1781/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1782/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1783/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1784/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1785/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1786/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1787/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1788/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1789/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1790/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1791/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1792/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1793/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1794/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1795/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1796/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1797/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1798/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1799/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1800/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1801/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1802/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1803/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1804/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1805/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1806/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1807/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1808/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1809/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1810/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1811/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1812/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1813/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1814/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1815/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1816/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1817/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1818/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1819/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1820/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1821/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1822/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1823/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1824/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1825/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1826/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1827/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1828/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1829/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1830/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1831/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1832/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1833/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1834/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1835/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1836/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1837/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1838/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1839/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1840/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1841/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1842/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1843/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1844/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1845/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1846/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1847/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1848/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1849/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1850/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1851/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1852/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1853/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1854/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1855/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1856/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1857/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1858/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1859/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1860/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1861/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1862/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1863/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1864/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1865/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1866/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1867/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1868/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1869/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1870/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1871/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1872/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1873/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1874/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1875/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1876/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1877/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1878/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1879/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1880/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1881/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1882/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1883/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1884/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1885/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1886/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1887/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1888/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1889/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1890/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1891/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1892/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1893/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1894/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1895/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1896/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1897/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1898/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1899/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1900/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1901/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1902/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1903/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1904/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1905/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1906/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1907/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1908/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1909/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1910/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1911/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1912/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1913/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1914/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1915/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1916/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1917/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1918/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1919/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1920/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1921/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1922/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1923/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1924/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1925/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1926/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1927/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1928/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1929/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1930/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1931/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1932/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1933/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1934/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1935/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1936/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1937/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1938/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1939/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1940/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1941/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1942/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1943/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1944/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1945/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1946/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1947/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1948/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1949/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1950/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1951/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1952/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1953/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1954/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1955/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1956/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1957/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1958/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1959/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1960/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1961/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1962/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1963/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1964/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1965/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1966/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1967/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1968/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1969/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1970/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1971/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1972/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1973/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1974/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1975/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1976/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1977/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1978/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1979/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1980/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1981/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1982/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1983/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1984/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1985/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1986/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1987/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1988/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1989/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1990/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1991/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1992/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1993/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 1994/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1995/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1996/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1997/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1998/2000, Training Loss: 336.5441, Validation Loss: 337.5690\n",
      "Epoch 1999/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n",
      "Epoch 2000/2000, Training Loss: 336.5441, Validation Loss: 337.5691\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.25%\n",
      "Mean Percent Accuracy (Yield): 89.19%\n",
      "Median Percent Accuracy (Days to Harvest): 79.59%\n",
      "Median Percent Accuracy (Yield): 92.57%\n",
      "Accuracy Within 10% (Days to Harvest): 23.71%\n",
      "Accuracy Within 10% (Yield): 62.49%\n",
      "MAE (Days to Harvest): 22.45\n",
      "MAE (Yield): 0.40\n",
      "RMSE (Days to Harvest): 25.94\n",
      "RMSE (Yield): 0.50\n",
      "\n",
      "Validation Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.17%\n",
      "Mean Percent Accuracy (Yield): 89.00%\n",
      "Median Percent Accuracy (Days to Harvest): 79.53%\n",
      "Median Percent Accuracy (Yield): 92.52%\n",
      "Accuracy Within 10% (Days to Harvest): 23.60%\n",
      "Accuracy Within 10% (Yield): 62.38%\n",
      "MAE (Days to Harvest): 22.49\n",
      "MAE (Yield): 0.40\n",
      "RMSE (Days to Harvest): 25.98\n",
      "RMSE (Yield): 0.50\n",
      "\n",
      "Training completed in 3.12 seconds\n",
      "Final Training Loss: 336.5441\n",
      "Final Validation Loss: 337.5691\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y.values, test_size=0.2, random_state=88)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Move data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor, X_val_tensor = X_train_tensor.to(device), X_val_tensor.to(device)\n",
    "y_train_tensor, y_val_tensor = y_train_tensor.to(device), y_val_tensor.to(device)\n",
    "\n",
    "# Define Linear Regression model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 2)  # Two outputs: Days to Harvest and Yield\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = LinearRegressionModel(input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 2000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_train_tensor)\n",
    "    train_loss = criterion(predictions, y_train_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_val_tensor)\n",
    "        val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "    # Store losses\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Final training and validation loss\n",
    "final_train_loss = train_losses[-1]\n",
    "final_val_loss = val_losses[-1]\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate metrics\n",
    "def evaluate_model_with_metrics(y_actual, y_pred, tolerance=0.1):\n",
    "    # Percent accuracy\n",
    "    percent_accuracies = 100 * (1 - np.abs(y_actual - y_pred) / y_actual)\n",
    "    mean_percent_accuracy = np.mean(percent_accuracies, axis=0)\n",
    "    median_percent_accuracy = np.median(percent_accuracies, axis=0)\n",
    "\n",
    "    # Accuracy within tolerance\n",
    "    within_tolerance = np.abs(y_actual - y_pred) <= (tolerance * y_actual)\n",
    "    percent_within_tolerance = np.mean(within_tolerance, axis=0) * 100\n",
    "\n",
    "    # MAE\n",
    "    mae = np.mean(np.abs(y_actual - y_pred), axis=0)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(np.mean((y_actual - y_pred) ** 2, axis=0))\n",
    "\n",
    "    return {\n",
    "        \"mean_percent_accuracy\": mean_percent_accuracy,\n",
    "        \"median_percent_accuracy\": median_percent_accuracy,\n",
    "        \"percent_within_tolerance\": percent_within_tolerance,\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "    }\n",
    "\n",
    "# Evaluate metrics\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_preds = model(X_train_tensor).cpu().numpy()\n",
    "    val_preds = model(X_val_tensor).cpu().numpy()\n",
    "    y_train_np = y_train_tensor.cpu().numpy()\n",
    "    y_val_np = y_val_tensor.cpu().numpy()\n",
    "\n",
    "    # Training metrics\n",
    "    train_metrics = evaluate_model_with_metrics(y_train_np, train_preds, tolerance=0.1)\n",
    "    \n",
    "    # Validation metrics\n",
    "    val_metrics = evaluate_model_with_metrics(y_val_np, val_preds, tolerance=0.1)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {train_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {train_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {train_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {train_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {train_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {train_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {train_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {train_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {train_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {train_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {val_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {val_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {val_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {val_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {val_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {val_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {val_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {val_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {val_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {val_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
