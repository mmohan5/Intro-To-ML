{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"crop_yield.csv\")\n",
    "\n",
    "# Remove rows with Cotton and Soybean crops and the Region column\n",
    "data = data[~data['Crop'].isin(['Cotton', 'Soybean'])]\n",
    "data = data.drop(columns=['Region'])\n",
    "\n",
    "# Split features and targets\n",
    "X = data.drop(columns=['Days_to_Harvest', 'Yield_tons_per_hectare'])\n",
    "y = data[['Days_to_Harvest', 'Yield_tons_per_hectare']]\n",
    "\n",
    "# Encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "categorical_features = ['Soil_Type', 'Crop', 'Weather_Condition']\n",
    "encoded_categorical = encoder.fit_transform(X[categorical_features])\n",
    "\n",
    "# Convert boolean features to integers\n",
    "X['Fertilizer_Used'] = X['Fertilizer_Used'].astype(int)\n",
    "X['Irrigation_Used'] = X['Irrigation_Used'].astype(int)\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['Rainfall_mm', 'Temperature_Celsius', 'Fertilizer_Used', 'Irrigation_Used']\n",
    "normalized_numerical = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "# Combine processed features\n",
    "X_processed = np.hstack([normalized_numerical, encoded_categorical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Neural Network (FCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/700, Training Loss: 5803.6196, Validation Loss: 5779.6406\n",
      "Epoch 2/700, Training Loss: 5784.7852, Validation Loss: 5738.1553\n",
      "Epoch 3/700, Training Loss: 5743.2715, Validation Loss: 5617.7153\n",
      "Epoch 4/700, Training Loss: 5622.7500, Validation Loss: 5319.5039\n",
      "Epoch 5/700, Training Loss: 5324.3438, Validation Loss: 4674.1665\n",
      "Epoch 6/700, Training Loss: 4678.6045, Validation Loss: 3478.9539\n",
      "Epoch 7/700, Training Loss: 3482.5815, Validation Loss: 1713.7843\n",
      "Epoch 8/700, Training Loss: 1715.8907, Validation Loss: 618.3808\n",
      "Epoch 9/700, Training Loss: 617.6699, Validation Loss: 2928.6741\n",
      "Epoch 10/700, Training Loss: 2924.8274, Validation Loss: 1109.8668\n",
      "Epoch 11/700, Training Loss: 1107.3033, Validation Loss: 394.4665\n",
      "Epoch 12/700, Training Loss: 393.5162, Validation Loss: 780.6130\n",
      "Epoch 13/700, Training Loss: 780.8320, Validation Loss: 1271.7244\n",
      "Epoch 14/700, Training Loss: 1272.5891, Validation Loss: 1535.5160\n",
      "Epoch 15/700, Training Loss: 1536.6130, Validation Loss: 1542.1158\n",
      "Epoch 16/700, Training Loss: 1543.1135, Validation Loss: 1329.2314\n",
      "Epoch 17/700, Training Loss: 1329.8381, Validation Loss: 969.4608\n",
      "Epoch 18/700, Training Loss: 969.3926, Validation Loss: 605.6512\n",
      "Epoch 19/700, Training Loss: 604.6494, Validation Loss: 473.2060\n",
      "Epoch 20/700, Training Loss: 471.1088, Validation Loss: 698.7149\n",
      "Epoch 21/700, Training Loss: 695.6296, Validation Loss: 880.2460\n",
      "Epoch 22/700, Training Loss: 876.7341, Validation Loss: 685.3676\n",
      "Epoch 23/700, Training Loss: 682.2596, Validation Loss: 436.0368\n",
      "Epoch 24/700, Training Loss: 433.9660, Validation Loss: 407.5355\n",
      "Epoch 25/700, Training Loss: 406.6412, Validation Loss: 523.5081\n",
      "Epoch 26/700, Training Loss: 523.5499, Validation Loss: 622.8406\n",
      "Epoch 27/700, Training Loss: 623.4518, Validation Loss: 629.9163\n",
      "Epoch 28/700, Training Loss: 630.7462, Validation Loss: 549.4282\n",
      "Epoch 29/700, Training Loss: 550.1591, Validation Loss: 434.0857\n",
      "Epoch 30/700, Training Loss: 434.4116, Validation Loss: 365.5366\n",
      "Epoch 31/700, Training Loss: 365.1927, Validation Loss: 396.5818\n",
      "Epoch 32/700, Training Loss: 395.4396, Validation Loss: 479.0735\n",
      "Epoch 33/700, Training Loss: 477.2953, Validation Loss: 494.0959\n",
      "Epoch 34/700, Training Loss: 492.1453, Validation Loss: 423.5509\n",
      "Epoch 35/700, Training Loss: 421.9109, Validation Loss: 361.8423\n",
      "Epoch 36/700, Training Loss: 360.7358, Validation Loss: 363.3923\n",
      "Epoch 37/700, Training Loss: 362.7666, Validation Loss: 400.0947\n",
      "Epoch 38/700, Training Loss: 399.7527, Validation Loss: 423.2930\n",
      "Epoch 39/700, Training Loss: 422.9996, Validation Loss: 410.9979\n",
      "Epoch 40/700, Training Loss: 410.5295, Validation Loss: 376.1616\n",
      "Epoch 41/700, Training Loss: 375.3318, Validation Loss: 352.0821\n",
      "Epoch 42/700, Training Loss: 350.7743, Validation Loss: 362.0511\n",
      "Epoch 43/700, Training Loss: 360.2672, Validation Loss: 390.3169\n",
      "Epoch 44/700, Training Loss: 388.2150, Validation Loss: 396.9925\n",
      "Epoch 45/700, Training Loss: 394.8535, Validation Loss: 373.0434\n",
      "Epoch 46/700, Training Loss: 371.1510, Validation Loss: 349.2335\n",
      "Epoch 47/700, Training Loss: 347.7531, Validation Loss: 348.1635\n",
      "Epoch 48/700, Training Loss: 347.1060, Validation Loss: 361.4161\n",
      "Epoch 49/700, Training Loss: 360.6764, Validation Loss: 369.1364\n",
      "Epoch 50/700, Training Loss: 368.5511, Validation Loss: 362.7108\n",
      "Epoch 51/700, Training Loss: 362.1047, Validation Loss: 350.0217\n",
      "Epoch 52/700, Training Loss: 349.2442, Validation Loss: 345.1221\n",
      "Epoch 53/700, Training Loss: 344.0843, Validation Loss: 352.0493\n",
      "Epoch 54/700, Training Loss: 350.7590, Validation Loss: 359.5471\n",
      "Epoch 55/700, Training Loss: 358.1161, Validation Loss: 356.4973\n",
      "Epoch 56/700, Training Loss: 355.0912, Validation Loss: 347.2771\n",
      "Epoch 57/700, Training Loss: 346.0301, Validation Loss: 343.3606\n",
      "Epoch 58/700, Training Loss: 342.3178, Validation Loss: 347.1405\n",
      "Epoch 59/700, Training Loss: 346.2580, Validation Loss: 351.5075\n",
      "Epoch 60/700, Training Loss: 350.6876, Validation Loss: 350.3671\n",
      "Epoch 61/700, Training Loss: 349.4987, Validation Loss: 345.1536\n",
      "Epoch 62/700, Training Loss: 344.1458, Validation Loss: 342.0139\n",
      "Epoch 63/700, Training Loss: 340.8217, Validation Loss: 343.9794\n",
      "Epoch 64/700, Training Loss: 342.6196, Validation Loss: 347.2852\n",
      "Epoch 65/700, Training Loss: 345.8326, Validation Loss: 346.9716\n",
      "Epoch 66/700, Training Loss: 345.5299, Validation Loss: 343.8341\n",
      "Epoch 67/700, Training Loss: 342.4922, Validation Loss: 342.2683\n",
      "Epoch 68/700, Training Loss: 341.0674, Validation Loss: 343.5674\n",
      "Epoch 69/700, Training Loss: 342.4935, Validation Loss: 344.9743\n",
      "Epoch 70/700, Training Loss: 343.9727, Validation Loss: 344.1261\n",
      "Epoch 71/700, Training Loss: 343.1267, Validation Loss: 342.0089\n",
      "Epoch 72/700, Training Loss: 340.9520, Validation Loss: 341.2347\n",
      "Epoch 73/700, Training Loss: 340.0924, Validation Loss: 342.3918\n",
      "Epoch 74/700, Training Loss: 341.1785, Validation Loss: 343.3953\n",
      "Epoch 75/700, Training Loss: 342.1616, Validation Loss: 342.6903\n",
      "Epoch 76/700, Training Loss: 341.4981, Validation Loss: 341.3597\n",
      "Epoch 77/700, Training Loss: 340.2530, Validation Loss: 341.1112\n",
      "Epoch 78/700, Training Loss: 340.0980, Validation Loss: 341.8021\n",
      "Epoch 79/700, Training Loss: 340.8548, Validation Loss: 342.0710\n",
      "Epoch 80/700, Training Loss: 341.1417, Validation Loss: 341.4533\n",
      "Epoch 81/700, Training Loss: 340.4930, Validation Loss: 340.8465\n",
      "Epoch 82/700, Training Loss: 339.8229, Validation Loss: 341.0220\n",
      "Epoch 83/700, Training Loss: 339.9307, Validation Loss: 341.5035\n",
      "Epoch 84/700, Training Loss: 340.3677, Validation Loss: 341.4120\n",
      "Epoch 85/700, Training Loss: 340.2709, Validation Loss: 340.8337\n",
      "Epoch 86/700, Training Loss: 339.7231, Validation Loss: 340.5499\n",
      "Epoch 87/700, Training Loss: 339.4869, Validation Loss: 340.7822\n",
      "Epoch 88/700, Training Loss: 339.7599, Validation Loss: 340.9951\n",
      "Epoch 89/700, Training Loss: 339.9889, Validation Loss: 340.8140\n",
      "Epoch 90/700, Training Loss: 339.7943, Validation Loss: 340.5239\n",
      "Epoch 91/700, Training Loss: 339.4697, Validation Loss: 340.5255\n",
      "Epoch 92/700, Training Loss: 339.4332, Validation Loss: 340.7066\n",
      "Epoch 93/700, Training Loss: 339.5909, Validation Loss: 340.6838\n",
      "Epoch 94/700, Training Loss: 339.5702, Validation Loss: 340.4358\n",
      "Epoch 95/700, Training Loss: 339.3479, Validation Loss: 340.2818\n",
      "Epoch 96/700, Training Loss: 339.2307, Validation Loss: 340.3400\n",
      "Epoch 97/700, Training Loss: 339.3201, Validation Loss: 340.3960\n",
      "Epoch 98/700, Training Loss: 339.3893, Validation Loss: 340.2991\n",
      "Epoch 99/700, Training Loss: 339.2840, Validation Loss: 340.1827\n",
      "Epoch 100/700, Training Loss: 339.1438, Validation Loss: 340.2050\n",
      "Epoch 101/700, Training Loss: 339.1393, Validation Loss: 340.2856\n",
      "Epoch 102/700, Training Loss: 339.2027, Validation Loss: 340.2566\n",
      "Epoch 103/700, Training Loss: 339.1728, Validation Loss: 340.1355\n",
      "Epoch 104/700, Training Loss: 339.0653, Validation Loss: 340.0668\n",
      "Epoch 105/700, Training Loss: 339.0157, Validation Loss: 340.0796\n",
      "Epoch 106/700, Training Loss: 339.0421, Validation Loss: 340.0774\n",
      "Epoch 107/700, Training Loss: 339.0407, Validation Loss: 340.0246\n",
      "Epoch 108/700, Training Loss: 338.9756, Validation Loss: 339.9933\n",
      "Epoch 109/700, Training Loss: 338.9245, Validation Loss: 340.0199\n",
      "Epoch 110/700, Training Loss: 338.9330, Validation Loss: 340.0383\n",
      "Epoch 111/700, Training Loss: 338.9425, Validation Loss: 339.9944\n",
      "Epoch 112/700, Training Loss: 338.9018, Validation Loss: 339.9303\n",
      "Epoch 113/700, Training Loss: 338.8495, Validation Loss: 339.9032\n",
      "Epoch 114/700, Training Loss: 338.8361, Validation Loss: 339.8986\n",
      "Epoch 115/700, Training Loss: 338.8401, Validation Loss: 339.8759\n",
      "Epoch 116/700, Training Loss: 338.8177, Validation Loss: 339.8441\n",
      "Epoch 117/700, Training Loss: 338.7791, Validation Loss: 339.8348\n",
      "Epoch 118/700, Training Loss: 338.7608, Validation Loss: 339.8391\n",
      "Epoch 119/700, Training Loss: 338.7597, Validation Loss: 339.8224\n",
      "Epoch 120/700, Training Loss: 338.7446, Validation Loss: 339.7845\n",
      "Epoch 121/700, Training Loss: 338.7144, Validation Loss: 339.7554\n",
      "Epoch 122/700, Training Loss: 338.6950, Validation Loss: 339.7437\n",
      "Epoch 123/700, Training Loss: 338.6900, Validation Loss: 339.7315\n",
      "Epoch 124/700, Training Loss: 338.6783, Validation Loss: 339.7132\n",
      "Epoch 125/700, Training Loss: 338.6541, Validation Loss: 339.7027\n",
      "Epoch 126/700, Training Loss: 338.6344, Validation Loss: 339.7023\n",
      "Epoch 127/700, Training Loss: 338.6261, Validation Loss: 339.6960\n",
      "Epoch 128/700, Training Loss: 338.6162, Validation Loss: 339.6763\n",
      "Epoch 129/700, Training Loss: 338.5981, Validation Loss: 339.6551\n",
      "Epoch 130/700, Training Loss: 338.5817, Validation Loss: 339.6413\n",
      "Epoch 131/700, Training Loss: 338.5726, Validation Loss: 339.6298\n",
      "Epoch 132/700, Training Loss: 338.5628, Validation Loss: 339.6161\n",
      "Epoch 133/700, Training Loss: 338.5471, Validation Loss: 339.6055\n",
      "Epoch 134/700, Training Loss: 338.5321, Validation Loss: 339.5998\n",
      "Epoch 135/700, Training Loss: 338.5226, Validation Loss: 339.5918\n",
      "Epoch 136/700, Training Loss: 338.5134, Validation Loss: 339.5768\n",
      "Epoch 137/700, Training Loss: 338.5003, Validation Loss: 339.5600\n",
      "Epoch 138/700, Training Loss: 338.4874, Validation Loss: 339.5473\n",
      "Epoch 139/700, Training Loss: 338.4781, Validation Loss: 339.5374\n",
      "Epoch 140/700, Training Loss: 338.4693, Validation Loss: 339.5281\n",
      "Epoch 141/700, Training Loss: 338.4580, Validation Loss: 339.5209\n",
      "Epoch 142/700, Training Loss: 338.4468, Validation Loss: 339.5160\n",
      "Epoch 143/700, Training Loss: 338.4380, Validation Loss: 339.5097\n",
      "Epoch 144/700, Training Loss: 338.4294, Validation Loss: 339.4994\n",
      "Epoch 145/700, Training Loss: 338.4192, Validation Loss: 339.4881\n",
      "Epoch 146/700, Training Loss: 338.4092, Validation Loss: 339.4786\n",
      "Epoch 147/700, Training Loss: 338.4011, Validation Loss: 339.4709\n",
      "Epoch 148/700, Training Loss: 338.3932, Validation Loss: 339.4637\n",
      "Epoch 149/700, Training Loss: 338.3842, Validation Loss: 339.4578\n",
      "Epoch 150/700, Training Loss: 338.3755, Validation Loss: 339.4526\n",
      "Epoch 151/700, Training Loss: 338.3678, Validation Loss: 339.4460\n",
      "Epoch 152/700, Training Loss: 338.3603, Validation Loss: 339.4372\n",
      "Epoch 153/700, Training Loss: 338.3521, Validation Loss: 339.4278\n",
      "Epoch 154/700, Training Loss: 338.3443, Validation Loss: 339.4194\n",
      "Epoch 155/700, Training Loss: 338.3373, Validation Loss: 339.4121\n",
      "Epoch 156/700, Training Loss: 338.3303, Validation Loss: 339.4056\n",
      "Epoch 157/700, Training Loss: 338.3229, Validation Loss: 339.4002\n",
      "Epoch 158/700, Training Loss: 338.3159, Validation Loss: 339.3951\n",
      "Epoch 159/700, Training Loss: 338.3094, Validation Loss: 339.3891\n",
      "Epoch 160/700, Training Loss: 338.3029, Validation Loss: 339.3820\n",
      "Epoch 161/700, Training Loss: 338.2963, Validation Loss: 339.3750\n",
      "Epoch 162/700, Training Loss: 338.2899, Validation Loss: 339.3687\n",
      "Epoch 163/700, Training Loss: 338.2839, Validation Loss: 339.3632\n",
      "Epoch 164/700, Training Loss: 338.2779, Validation Loss: 339.3583\n",
      "Epoch 165/700, Training Loss: 338.2718, Validation Loss: 339.3540\n",
      "Epoch 166/700, Training Loss: 338.2661, Validation Loss: 339.3493\n",
      "Epoch 167/700, Training Loss: 338.2605, Validation Loss: 339.3438\n",
      "Epoch 168/700, Training Loss: 338.2549, Validation Loss: 339.3377\n",
      "Epoch 169/700, Training Loss: 338.2495, Validation Loss: 339.3318\n",
      "Epoch 170/700, Training Loss: 338.2442, Validation Loss: 339.3263\n",
      "Epoch 171/700, Training Loss: 338.2390, Validation Loss: 339.3215\n",
      "Epoch 172/700, Training Loss: 338.2339, Validation Loss: 339.3172\n",
      "Epoch 173/700, Training Loss: 338.2288, Validation Loss: 339.3130\n",
      "Epoch 174/700, Training Loss: 338.2239, Validation Loss: 339.3086\n",
      "Epoch 175/700, Training Loss: 338.2192, Validation Loss: 339.3036\n",
      "Epoch 176/700, Training Loss: 338.2144, Validation Loss: 339.2986\n",
      "Epoch 177/700, Training Loss: 338.2098, Validation Loss: 339.2939\n",
      "Epoch 178/700, Training Loss: 338.2054, Validation Loss: 339.2897\n",
      "Epoch 179/700, Training Loss: 338.2009, Validation Loss: 339.2858\n",
      "Epoch 180/700, Training Loss: 338.1965, Validation Loss: 339.2822\n",
      "Epoch 181/700, Training Loss: 338.1923, Validation Loss: 339.2785\n",
      "Epoch 182/700, Training Loss: 338.1881, Validation Loss: 339.2745\n",
      "Epoch 183/700, Training Loss: 338.1840, Validation Loss: 339.2702\n",
      "Epoch 184/700, Training Loss: 338.1799, Validation Loss: 339.2661\n",
      "Epoch 185/700, Training Loss: 338.1760, Validation Loss: 339.2622\n",
      "Epoch 186/700, Training Loss: 338.1721, Validation Loss: 339.2587\n",
      "Epoch 187/700, Training Loss: 338.1682, Validation Loss: 339.2553\n",
      "Epoch 188/700, Training Loss: 338.1645, Validation Loss: 339.2520\n",
      "Epoch 189/700, Training Loss: 338.1608, Validation Loss: 339.2485\n",
      "Epoch 190/700, Training Loss: 338.1572, Validation Loss: 339.2447\n",
      "Epoch 191/700, Training Loss: 338.1536, Validation Loss: 339.2410\n",
      "Epoch 192/700, Training Loss: 338.1501, Validation Loss: 339.2374\n",
      "Epoch 193/700, Training Loss: 338.1467, Validation Loss: 339.2341\n",
      "Epoch 194/700, Training Loss: 338.1433, Validation Loss: 339.2310\n",
      "Epoch 195/700, Training Loss: 338.1400, Validation Loss: 339.2279\n",
      "Epoch 196/700, Training Loss: 338.1367, Validation Loss: 339.2249\n",
      "Epoch 197/700, Training Loss: 338.1335, Validation Loss: 339.2216\n",
      "Epoch 198/700, Training Loss: 338.1303, Validation Loss: 339.2183\n",
      "Epoch 199/700, Training Loss: 338.1272, Validation Loss: 339.2151\n",
      "Epoch 200/700, Training Loss: 338.1241, Validation Loss: 339.2122\n",
      "Epoch 201/700, Training Loss: 338.1211, Validation Loss: 339.2093\n",
      "Epoch 202/700, Training Loss: 338.1181, Validation Loss: 339.2065\n",
      "Epoch 203/700, Training Loss: 338.1152, Validation Loss: 339.2037\n",
      "Epoch 204/700, Training Loss: 338.1124, Validation Loss: 339.2008\n",
      "Epoch 205/700, Training Loss: 338.1095, Validation Loss: 339.1978\n",
      "Epoch 206/700, Training Loss: 338.1067, Validation Loss: 339.1949\n",
      "Epoch 207/700, Training Loss: 338.1040, Validation Loss: 339.1920\n",
      "Epoch 208/700, Training Loss: 338.1013, Validation Loss: 339.1894\n",
      "Epoch 209/700, Training Loss: 338.0986, Validation Loss: 339.1868\n",
      "Epoch 210/700, Training Loss: 338.0959, Validation Loss: 339.1842\n",
      "Epoch 211/700, Training Loss: 338.0934, Validation Loss: 339.1815\n",
      "Epoch 212/700, Training Loss: 338.0908, Validation Loss: 339.1789\n",
      "Epoch 213/700, Training Loss: 338.0883, Validation Loss: 339.1762\n",
      "Epoch 214/700, Training Loss: 338.0858, Validation Loss: 339.1736\n",
      "Epoch 215/700, Training Loss: 338.0833, Validation Loss: 339.1712\n",
      "Epoch 216/700, Training Loss: 338.0809, Validation Loss: 339.1688\n",
      "Epoch 217/700, Training Loss: 338.0786, Validation Loss: 339.1664\n",
      "Epoch 218/700, Training Loss: 338.0762, Validation Loss: 339.1640\n",
      "Epoch 219/700, Training Loss: 338.0739, Validation Loss: 339.1617\n",
      "Epoch 220/700, Training Loss: 338.0716, Validation Loss: 339.1592\n",
      "Epoch 221/700, Training Loss: 338.0694, Validation Loss: 339.1569\n",
      "Epoch 222/700, Training Loss: 338.0671, Validation Loss: 339.1546\n",
      "Epoch 223/700, Training Loss: 338.0649, Validation Loss: 339.1523\n",
      "Epoch 224/700, Training Loss: 338.0628, Validation Loss: 339.1501\n",
      "Epoch 225/700, Training Loss: 338.0606, Validation Loss: 339.1479\n",
      "Epoch 226/700, Training Loss: 338.0585, Validation Loss: 339.1457\n",
      "Epoch 227/700, Training Loss: 338.0564, Validation Loss: 339.1435\n",
      "Epoch 228/700, Training Loss: 338.0544, Validation Loss: 339.1413\n",
      "Epoch 229/700, Training Loss: 338.0523, Validation Loss: 339.1392\n",
      "Epoch 230/700, Training Loss: 338.0504, Validation Loss: 339.1372\n",
      "Epoch 231/700, Training Loss: 338.0484, Validation Loss: 339.1351\n",
      "Epoch 232/700, Training Loss: 338.0464, Validation Loss: 339.1331\n",
      "Epoch 233/700, Training Loss: 338.0445, Validation Loss: 339.1310\n",
      "Epoch 234/700, Training Loss: 338.0426, Validation Loss: 339.1290\n",
      "Epoch 235/700, Training Loss: 338.0407, Validation Loss: 339.1270\n",
      "Epoch 236/700, Training Loss: 338.0389, Validation Loss: 339.1251\n",
      "Epoch 237/700, Training Loss: 338.0370, Validation Loss: 339.1232\n",
      "Epoch 238/700, Training Loss: 338.0352, Validation Loss: 339.1213\n",
      "Epoch 239/700, Training Loss: 338.0334, Validation Loss: 339.1194\n",
      "Epoch 240/700, Training Loss: 338.0317, Validation Loss: 339.1175\n",
      "Epoch 241/700, Training Loss: 338.0299, Validation Loss: 339.1157\n",
      "Epoch 242/700, Training Loss: 338.0282, Validation Loss: 339.1139\n",
      "Epoch 243/700, Training Loss: 338.0265, Validation Loss: 339.1121\n",
      "Epoch 244/700, Training Loss: 338.0248, Validation Loss: 339.1104\n",
      "Epoch 245/700, Training Loss: 338.0232, Validation Loss: 339.1086\n",
      "Epoch 246/700, Training Loss: 338.0216, Validation Loss: 339.1069\n",
      "Epoch 247/700, Training Loss: 338.0200, Validation Loss: 339.1051\n",
      "Epoch 248/700, Training Loss: 338.0183, Validation Loss: 339.1034\n",
      "Epoch 249/700, Training Loss: 338.0168, Validation Loss: 339.1017\n",
      "Epoch 250/700, Training Loss: 338.0152, Validation Loss: 339.1001\n",
      "Epoch 251/700, Training Loss: 338.0137, Validation Loss: 339.0985\n",
      "Epoch 252/700, Training Loss: 338.0121, Validation Loss: 339.0969\n",
      "Epoch 253/700, Training Loss: 338.0106, Validation Loss: 339.0952\n",
      "Epoch 254/700, Training Loss: 338.0091, Validation Loss: 339.0936\n",
      "Epoch 255/700, Training Loss: 338.0077, Validation Loss: 339.0920\n",
      "Epoch 256/700, Training Loss: 338.0062, Validation Loss: 339.0905\n",
      "Epoch 257/700, Training Loss: 338.0048, Validation Loss: 339.0890\n",
      "Epoch 258/700, Training Loss: 338.0034, Validation Loss: 339.0875\n",
      "Epoch 259/700, Training Loss: 338.0019, Validation Loss: 339.0860\n",
      "Epoch 260/700, Training Loss: 338.0005, Validation Loss: 339.0845\n",
      "Epoch 261/700, Training Loss: 337.9992, Validation Loss: 339.0830\n",
      "Epoch 262/700, Training Loss: 337.9978, Validation Loss: 339.0815\n",
      "Epoch 263/700, Training Loss: 337.9965, Validation Loss: 339.0801\n",
      "Epoch 264/700, Training Loss: 337.9951, Validation Loss: 339.0787\n",
      "Epoch 265/700, Training Loss: 337.9938, Validation Loss: 339.0773\n",
      "Epoch 266/700, Training Loss: 337.9926, Validation Loss: 339.0759\n",
      "Epoch 267/700, Training Loss: 337.9912, Validation Loss: 339.0745\n",
      "Epoch 268/700, Training Loss: 337.9900, Validation Loss: 339.0731\n",
      "Epoch 269/700, Training Loss: 337.9887, Validation Loss: 339.0717\n",
      "Epoch 270/700, Training Loss: 337.9875, Validation Loss: 339.0704\n",
      "Epoch 271/700, Training Loss: 337.9863, Validation Loss: 339.0692\n",
      "Epoch 272/700, Training Loss: 337.9850, Validation Loss: 339.0679\n",
      "Epoch 273/700, Training Loss: 337.9838, Validation Loss: 339.0665\n",
      "Epoch 274/700, Training Loss: 337.9826, Validation Loss: 339.0653\n",
      "Epoch 275/700, Training Loss: 337.9814, Validation Loss: 339.0640\n",
      "Epoch 276/700, Training Loss: 337.9803, Validation Loss: 339.0627\n",
      "Epoch 277/700, Training Loss: 337.9791, Validation Loss: 339.0615\n",
      "Epoch 278/700, Training Loss: 337.9780, Validation Loss: 339.0602\n",
      "Epoch 279/700, Training Loss: 337.9768, Validation Loss: 339.0591\n",
      "Epoch 280/700, Training Loss: 337.9757, Validation Loss: 339.0578\n",
      "Epoch 281/700, Training Loss: 337.9746, Validation Loss: 339.0566\n",
      "Epoch 282/700, Training Loss: 337.9735, Validation Loss: 339.0555\n",
      "Epoch 283/700, Training Loss: 337.9724, Validation Loss: 339.0543\n",
      "Epoch 284/700, Training Loss: 337.9713, Validation Loss: 339.0532\n",
      "Epoch 285/700, Training Loss: 337.9702, Validation Loss: 339.0520\n",
      "Epoch 286/700, Training Loss: 337.9692, Validation Loss: 339.0509\n",
      "Epoch 287/700, Training Loss: 337.9681, Validation Loss: 339.0498\n",
      "Epoch 288/700, Training Loss: 337.9671, Validation Loss: 339.0487\n",
      "Epoch 289/700, Training Loss: 337.9661, Validation Loss: 339.0476\n",
      "Epoch 290/700, Training Loss: 337.9651, Validation Loss: 339.0465\n",
      "Epoch 291/700, Training Loss: 337.9641, Validation Loss: 339.0454\n",
      "Epoch 292/700, Training Loss: 337.9630, Validation Loss: 339.0443\n",
      "Epoch 293/700, Training Loss: 337.9621, Validation Loss: 339.0433\n",
      "Epoch 294/700, Training Loss: 337.9611, Validation Loss: 339.0422\n",
      "Epoch 295/700, Training Loss: 337.9601, Validation Loss: 339.0412\n",
      "Epoch 296/700, Training Loss: 337.9591, Validation Loss: 339.0402\n",
      "Epoch 297/700, Training Loss: 337.9582, Validation Loss: 339.0392\n",
      "Epoch 298/700, Training Loss: 337.9573, Validation Loss: 339.0382\n",
      "Epoch 299/700, Training Loss: 337.9563, Validation Loss: 339.0372\n",
      "Epoch 300/700, Training Loss: 337.9554, Validation Loss: 339.0362\n",
      "Epoch 301/700, Training Loss: 337.9545, Validation Loss: 339.0353\n",
      "Epoch 302/700, Training Loss: 337.9536, Validation Loss: 339.0343\n",
      "Epoch 303/700, Training Loss: 337.9527, Validation Loss: 339.0334\n",
      "Epoch 304/700, Training Loss: 337.9518, Validation Loss: 339.0324\n",
      "Epoch 305/700, Training Loss: 337.9509, Validation Loss: 339.0315\n",
      "Epoch 306/700, Training Loss: 337.9500, Validation Loss: 339.0305\n",
      "Epoch 307/700, Training Loss: 337.9491, Validation Loss: 339.0296\n",
      "Epoch 308/700, Training Loss: 337.9482, Validation Loss: 339.0287\n",
      "Epoch 309/700, Training Loss: 337.9474, Validation Loss: 339.0278\n",
      "Epoch 310/700, Training Loss: 337.9465, Validation Loss: 339.0269\n",
      "Epoch 311/700, Training Loss: 337.9456, Validation Loss: 339.0260\n",
      "Epoch 312/700, Training Loss: 337.9449, Validation Loss: 339.0251\n",
      "Epoch 313/700, Training Loss: 337.9440, Validation Loss: 339.0242\n",
      "Epoch 314/700, Training Loss: 337.9432, Validation Loss: 339.0233\n",
      "Epoch 315/700, Training Loss: 337.9424, Validation Loss: 339.0225\n",
      "Epoch 316/700, Training Loss: 337.9416, Validation Loss: 339.0216\n",
      "Epoch 317/700, Training Loss: 337.9408, Validation Loss: 339.0208\n",
      "Epoch 318/700, Training Loss: 337.9400, Validation Loss: 339.0200\n",
      "Epoch 319/700, Training Loss: 337.9392, Validation Loss: 339.0192\n",
      "Epoch 320/700, Training Loss: 337.9384, Validation Loss: 339.0183\n",
      "Epoch 321/700, Training Loss: 337.9376, Validation Loss: 339.0175\n",
      "Epoch 322/700, Training Loss: 337.9368, Validation Loss: 339.0167\n",
      "Epoch 323/700, Training Loss: 337.9360, Validation Loss: 339.0159\n",
      "Epoch 324/700, Training Loss: 337.9353, Validation Loss: 339.0151\n",
      "Epoch 325/700, Training Loss: 337.9345, Validation Loss: 339.0143\n",
      "Epoch 326/700, Training Loss: 337.9337, Validation Loss: 339.0135\n",
      "Epoch 327/700, Training Loss: 337.9330, Validation Loss: 339.0128\n",
      "Epoch 328/700, Training Loss: 337.9322, Validation Loss: 339.0120\n",
      "Epoch 329/700, Training Loss: 337.9315, Validation Loss: 339.0113\n",
      "Epoch 330/700, Training Loss: 337.9307, Validation Loss: 339.0106\n",
      "Epoch 331/700, Training Loss: 337.9300, Validation Loss: 339.0099\n",
      "Epoch 332/700, Training Loss: 337.9293, Validation Loss: 339.0092\n",
      "Epoch 333/700, Training Loss: 337.9285, Validation Loss: 339.0085\n",
      "Epoch 334/700, Training Loss: 337.9278, Validation Loss: 339.0077\n",
      "Epoch 335/700, Training Loss: 337.9271, Validation Loss: 339.0070\n",
      "Epoch 336/700, Training Loss: 337.9264, Validation Loss: 339.0062\n",
      "Epoch 337/700, Training Loss: 337.9257, Validation Loss: 339.0055\n",
      "Epoch 338/700, Training Loss: 337.9250, Validation Loss: 339.0048\n",
      "Epoch 339/700, Training Loss: 337.9243, Validation Loss: 339.0042\n",
      "Epoch 340/700, Training Loss: 337.9236, Validation Loss: 339.0034\n",
      "Epoch 341/700, Training Loss: 337.9229, Validation Loss: 339.0027\n",
      "Epoch 342/700, Training Loss: 337.9222, Validation Loss: 339.0020\n",
      "Epoch 343/700, Training Loss: 337.9215, Validation Loss: 339.0014\n",
      "Epoch 344/700, Training Loss: 337.9208, Validation Loss: 339.0008\n",
      "Epoch 345/700, Training Loss: 337.9201, Validation Loss: 339.0001\n",
      "Epoch 346/700, Training Loss: 337.9195, Validation Loss: 338.9994\n",
      "Epoch 347/700, Training Loss: 337.9188, Validation Loss: 338.9987\n",
      "Epoch 348/700, Training Loss: 337.9181, Validation Loss: 338.9980\n",
      "Epoch 349/700, Training Loss: 337.9174, Validation Loss: 338.9973\n",
      "Epoch 350/700, Training Loss: 337.9167, Validation Loss: 338.9966\n",
      "Epoch 351/700, Training Loss: 337.9161, Validation Loss: 338.9960\n",
      "Epoch 352/700, Training Loss: 337.9154, Validation Loss: 338.9953\n",
      "Epoch 353/700, Training Loss: 337.9148, Validation Loss: 338.9945\n",
      "Epoch 354/700, Training Loss: 337.9141, Validation Loss: 338.9938\n",
      "Epoch 355/700, Training Loss: 337.9135, Validation Loss: 338.9931\n",
      "Epoch 356/700, Training Loss: 337.9128, Validation Loss: 338.9926\n",
      "Epoch 357/700, Training Loss: 337.9121, Validation Loss: 338.9920\n",
      "Epoch 358/700, Training Loss: 337.9114, Validation Loss: 338.9915\n",
      "Epoch 359/700, Training Loss: 337.9108, Validation Loss: 338.9909\n",
      "Epoch 360/700, Training Loss: 337.9101, Validation Loss: 338.9904\n",
      "Epoch 361/700, Training Loss: 337.9094, Validation Loss: 338.9896\n",
      "Epoch 362/700, Training Loss: 337.9087, Validation Loss: 338.9889\n",
      "Epoch 363/700, Training Loss: 337.9081, Validation Loss: 338.9883\n",
      "Epoch 364/700, Training Loss: 337.9074, Validation Loss: 338.9877\n",
      "Epoch 365/700, Training Loss: 337.9067, Validation Loss: 338.9873\n",
      "Epoch 366/700, Training Loss: 337.9061, Validation Loss: 338.9869\n",
      "Epoch 367/700, Training Loss: 337.9054, Validation Loss: 338.9862\n",
      "Epoch 368/700, Training Loss: 337.9048, Validation Loss: 338.9854\n",
      "Epoch 369/700, Training Loss: 337.9041, Validation Loss: 338.9845\n",
      "Epoch 370/700, Training Loss: 337.9034, Validation Loss: 338.9839\n",
      "Epoch 371/700, Training Loss: 337.9027, Validation Loss: 338.9832\n",
      "Epoch 372/700, Training Loss: 337.9020, Validation Loss: 338.9825\n",
      "Epoch 373/700, Training Loss: 337.9013, Validation Loss: 338.9818\n",
      "Epoch 374/700, Training Loss: 337.9007, Validation Loss: 338.9810\n",
      "Epoch 375/700, Training Loss: 337.9000, Validation Loss: 338.9803\n",
      "Epoch 376/700, Training Loss: 337.8993, Validation Loss: 338.9796\n",
      "Epoch 377/700, Training Loss: 337.8985, Validation Loss: 338.9789\n",
      "Epoch 378/700, Training Loss: 337.8978, Validation Loss: 338.9781\n",
      "Epoch 379/700, Training Loss: 337.8971, Validation Loss: 338.9773\n",
      "Epoch 380/700, Training Loss: 337.8963, Validation Loss: 338.9764\n",
      "Epoch 381/700, Training Loss: 337.8956, Validation Loss: 338.9756\n",
      "Epoch 382/700, Training Loss: 337.8948, Validation Loss: 338.9749\n",
      "Epoch 383/700, Training Loss: 337.8940, Validation Loss: 338.9742\n",
      "Epoch 384/700, Training Loss: 337.8932, Validation Loss: 338.9735\n",
      "Epoch 385/700, Training Loss: 337.8924, Validation Loss: 338.9729\n",
      "Epoch 386/700, Training Loss: 337.8916, Validation Loss: 338.9723\n",
      "Epoch 387/700, Training Loss: 337.8907, Validation Loss: 338.9714\n",
      "Epoch 388/700, Training Loss: 337.8899, Validation Loss: 338.9704\n",
      "Epoch 389/700, Training Loss: 337.8889, Validation Loss: 338.9694\n",
      "Epoch 390/700, Training Loss: 337.8880, Validation Loss: 338.9684\n",
      "Epoch 391/700, Training Loss: 337.8870, Validation Loss: 338.9676\n",
      "Epoch 392/700, Training Loss: 337.8859, Validation Loss: 338.9667\n",
      "Epoch 393/700, Training Loss: 337.8847, Validation Loss: 338.9653\n",
      "Epoch 394/700, Training Loss: 337.8833, Validation Loss: 338.9638\n",
      "Epoch 395/700, Training Loss: 337.8817, Validation Loss: 338.9623\n",
      "Epoch 396/700, Training Loss: 337.8798, Validation Loss: 338.9606\n",
      "Epoch 397/700, Training Loss: 337.8773, Validation Loss: 338.9576\n",
      "Epoch 398/700, Training Loss: 337.8741, Validation Loss: 338.9533\n",
      "Epoch 399/700, Training Loss: 337.8701, Validation Loss: 338.9474\n",
      "Epoch 400/700, Training Loss: 337.8648, Validation Loss: 338.9408\n",
      "Epoch 401/700, Training Loss: 337.8592, Validation Loss: 338.9360\n",
      "Epoch 402/700, Training Loss: 337.8544, Validation Loss: 338.9296\n",
      "Epoch 403/700, Training Loss: 337.8484, Validation Loss: 338.9225\n",
      "Epoch 404/700, Training Loss: 337.8412, Validation Loss: 338.9148\n",
      "Epoch 405/700, Training Loss: 337.8326, Validation Loss: 338.9059\n",
      "Epoch 406/700, Training Loss: 337.8223, Validation Loss: 338.8946\n",
      "Epoch 407/700, Training Loss: 337.8100, Validation Loss: 338.8803\n",
      "Epoch 408/700, Training Loss: 337.7953, Validation Loss: 338.8636\n",
      "Epoch 409/700, Training Loss: 337.7778, Validation Loss: 338.8437\n",
      "Epoch 410/700, Training Loss: 337.7568, Validation Loss: 338.8190\n",
      "Epoch 411/700, Training Loss: 337.7317, Validation Loss: 338.7886\n",
      "Epoch 412/700, Training Loss: 337.7012, Validation Loss: 338.7507\n",
      "Epoch 413/700, Training Loss: 337.6633, Validation Loss: 338.7037\n",
      "Epoch 414/700, Training Loss: 337.6155, Validation Loss: 338.6503\n",
      "Epoch 415/700, Training Loss: 337.5620, Validation Loss: 338.5905\n",
      "Epoch 416/700, Training Loss: 337.5012, Validation Loss: 338.5189\n",
      "Epoch 417/700, Training Loss: 337.4303, Validation Loss: 338.4369\n",
      "Epoch 418/700, Training Loss: 337.3495, Validation Loss: 338.3536\n",
      "Epoch 419/700, Training Loss: 337.2652, Validation Loss: 338.2727\n",
      "Epoch 420/700, Training Loss: 337.1855, Validation Loss: 338.2050\n",
      "Epoch 421/700, Training Loss: 337.1078, Validation Loss: 338.1252\n",
      "Epoch 422/700, Training Loss: 337.0429, Validation Loss: 338.1206\n",
      "Epoch 423/700, Training Loss: 337.0189, Validation Loss: 337.9922\n",
      "Epoch 424/700, Training Loss: 336.9087, Validation Loss: 337.9091\n",
      "Epoch 425/700, Training Loss: 336.8107, Validation Loss: 337.8601\n",
      "Epoch 426/700, Training Loss: 336.7665, Validation Loss: 337.8677\n",
      "Epoch 427/700, Training Loss: 336.7886, Validation Loss: 337.9601\n",
      "Epoch 428/700, Training Loss: 336.8623, Validation Loss: 337.8863\n",
      "Epoch 429/700, Training Loss: 336.8091, Validation Loss: 337.7965\n",
      "Epoch 430/700, Training Loss: 336.7050, Validation Loss: 337.7843\n",
      "Epoch 431/700, Training Loss: 336.6947, Validation Loss: 337.8044\n",
      "Epoch 432/700, Training Loss: 336.7288, Validation Loss: 337.7784\n",
      "Epoch 433/700, Training Loss: 336.6790, Validation Loss: 337.7252\n",
      "Epoch 434/700, Training Loss: 336.6346, Validation Loss: 337.7512\n",
      "Epoch 435/700, Training Loss: 336.6774, Validation Loss: 337.7388\n",
      "Epoch 436/700, Training Loss: 336.6435, Validation Loss: 337.7058\n",
      "Epoch 437/700, Training Loss: 336.6146, Validation Loss: 337.7217\n",
      "Epoch 438/700, Training Loss: 336.6481, Validation Loss: 337.7089\n",
      "Epoch 439/700, Training Loss: 336.6208, Validation Loss: 337.6954\n",
      "Epoch 440/700, Training Loss: 336.6074, Validation Loss: 337.7081\n",
      "Epoch 441/700, Training Loss: 336.6341, Validation Loss: 337.6896\n",
      "Epoch 442/700, Training Loss: 336.6068, Validation Loss: 337.6947\n",
      "Epoch 443/700, Training Loss: 336.6110, Validation Loss: 337.6958\n",
      "Epoch 444/700, Training Loss: 336.6222, Validation Loss: 337.6821\n",
      "Epoch 445/700, Training Loss: 336.6004, Validation Loss: 337.6985\n",
      "Epoch 446/700, Training Loss: 336.6141, Validation Loss: 337.6836\n",
      "Epoch 447/700, Training Loss: 336.6081, Validation Loss: 337.6832\n",
      "Epoch 448/700, Training Loss: 336.6006, Validation Loss: 337.6965\n",
      "Epoch 449/700, Training Loss: 336.6082, Validation Loss: 337.6770\n",
      "Epoch 450/700, Training Loss: 336.5988, Validation Loss: 337.6779\n",
      "Epoch 451/700, Training Loss: 336.5978, Validation Loss: 337.6889\n",
      "Epoch 452/700, Training Loss: 336.6003, Validation Loss: 337.6707\n",
      "Epoch 453/700, Training Loss: 336.5909, Validation Loss: 337.6699\n",
      "Epoch 454/700, Training Loss: 336.5943, Validation Loss: 337.6773\n",
      "Epoch 455/700, Training Loss: 336.5914, Validation Loss: 337.6682\n",
      "Epoch 456/700, Training Loss: 336.5858, Validation Loss: 337.6651\n",
      "Epoch 457/700, Training Loss: 336.5901, Validation Loss: 337.6671\n",
      "Epoch 458/700, Training Loss: 336.5836, Validation Loss: 337.6691\n",
      "Epoch 459/700, Training Loss: 336.5837, Validation Loss: 337.6617\n",
      "Epoch 460/700, Training Loss: 336.5841, Validation Loss: 337.6613\n",
      "Epoch 461/700, Training Loss: 336.5797, Validation Loss: 337.6673\n",
      "Epoch 462/700, Training Loss: 336.5815, Validation Loss: 337.6595\n",
      "Epoch 463/700, Training Loss: 336.5796, Validation Loss: 337.6578\n",
      "Epoch 464/700, Training Loss: 336.5777, Validation Loss: 337.6633\n",
      "Epoch 465/700, Training Loss: 336.5789, Validation Loss: 337.6573\n",
      "Epoch 466/700, Training Loss: 336.5765, Validation Loss: 337.6556\n",
      "Epoch 467/700, Training Loss: 336.5762, Validation Loss: 337.6594\n",
      "Epoch 468/700, Training Loss: 336.5763, Validation Loss: 337.6555\n",
      "Epoch 469/700, Training Loss: 336.5740, Validation Loss: 337.6543\n",
      "Epoch 470/700, Training Loss: 336.5747, Validation Loss: 337.6562\n",
      "Epoch 471/700, Training Loss: 336.5733, Validation Loss: 337.6545\n",
      "Epoch 472/700, Training Loss: 336.5722, Validation Loss: 337.6526\n",
      "Epoch 473/700, Training Loss: 336.5724, Validation Loss: 337.6540\n",
      "Epoch 474/700, Training Loss: 336.5707, Validation Loss: 337.6537\n",
      "Epoch 475/700, Training Loss: 336.5704, Validation Loss: 337.6505\n",
      "Epoch 476/700, Training Loss: 336.5699, Validation Loss: 337.6515\n",
      "Epoch 477/700, Training Loss: 336.5686, Validation Loss: 337.6520\n",
      "Epoch 478/700, Training Loss: 336.5683, Validation Loss: 337.6483\n",
      "Epoch 479/700, Training Loss: 336.5676, Validation Loss: 337.6486\n",
      "Epoch 480/700, Training Loss: 336.5666, Validation Loss: 337.6502\n",
      "Epoch 481/700, Training Loss: 336.5664, Validation Loss: 337.6468\n",
      "Epoch 482/700, Training Loss: 336.5655, Validation Loss: 337.6465\n",
      "Epoch 483/700, Training Loss: 336.5648, Validation Loss: 337.6486\n",
      "Epoch 484/700, Training Loss: 336.5645, Validation Loss: 337.6460\n",
      "Epoch 485/700, Training Loss: 336.5636, Validation Loss: 337.6451\n",
      "Epoch 486/700, Training Loss: 336.5632, Validation Loss: 337.6468\n",
      "Epoch 487/700, Training Loss: 336.5627, Validation Loss: 337.6451\n",
      "Epoch 488/700, Training Loss: 336.5620, Validation Loss: 337.6438\n",
      "Epoch 489/700, Training Loss: 336.5615, Validation Loss: 337.6450\n",
      "Epoch 490/700, Training Loss: 336.5610, Validation Loss: 337.6440\n",
      "Epoch 491/700, Training Loss: 336.5604, Validation Loss: 337.6427\n",
      "Epoch 492/700, Training Loss: 336.5599, Validation Loss: 337.6435\n",
      "Epoch 493/700, Training Loss: 336.5594, Validation Loss: 337.6427\n",
      "Epoch 494/700, Training Loss: 336.5588, Validation Loss: 337.6417\n",
      "Epoch 495/700, Training Loss: 336.5584, Validation Loss: 337.6421\n",
      "Epoch 496/700, Training Loss: 336.5578, Validation Loss: 337.6414\n",
      "Epoch 497/700, Training Loss: 336.5573, Validation Loss: 337.6405\n",
      "Epoch 498/700, Training Loss: 336.5569, Validation Loss: 337.6410\n",
      "Epoch 499/700, Training Loss: 336.5563, Validation Loss: 337.6404\n",
      "Epoch 500/700, Training Loss: 336.5558, Validation Loss: 337.6395\n",
      "Epoch 501/700, Training Loss: 336.5554, Validation Loss: 337.6399\n",
      "Epoch 502/700, Training Loss: 336.5548, Validation Loss: 337.6393\n",
      "Epoch 503/700, Training Loss: 336.5544, Validation Loss: 337.6383\n",
      "Epoch 504/700, Training Loss: 336.5540, Validation Loss: 337.6385\n",
      "Epoch 505/700, Training Loss: 336.5534, Validation Loss: 337.6380\n",
      "Epoch 506/700, Training Loss: 336.5530, Validation Loss: 337.6369\n",
      "Epoch 507/700, Training Loss: 336.5526, Validation Loss: 337.6371\n",
      "Epoch 508/700, Training Loss: 336.5521, Validation Loss: 337.6368\n",
      "Epoch 509/700, Training Loss: 336.5516, Validation Loss: 337.6358\n",
      "Epoch 510/700, Training Loss: 336.5512, Validation Loss: 337.6359\n",
      "Epoch 511/700, Training Loss: 336.5508, Validation Loss: 337.6357\n",
      "Epoch 512/700, Training Loss: 336.5503, Validation Loss: 337.6348\n",
      "Epoch 513/700, Training Loss: 336.5499, Validation Loss: 337.6348\n",
      "Epoch 514/700, Training Loss: 336.5494, Validation Loss: 337.6345\n",
      "Epoch 515/700, Training Loss: 336.5490, Validation Loss: 337.6338\n",
      "Epoch 516/700, Training Loss: 336.5486, Validation Loss: 337.6336\n",
      "Epoch 517/700, Training Loss: 336.5482, Validation Loss: 337.6333\n",
      "Epoch 518/700, Training Loss: 336.5478, Validation Loss: 337.6327\n",
      "Epoch 519/700, Training Loss: 336.5474, Validation Loss: 337.6326\n",
      "Epoch 520/700, Training Loss: 336.5469, Validation Loss: 337.6321\n",
      "Epoch 521/700, Training Loss: 336.5465, Validation Loss: 337.6317\n",
      "Epoch 522/700, Training Loss: 336.5461, Validation Loss: 337.6317\n",
      "Epoch 523/700, Training Loss: 336.5457, Validation Loss: 337.6313\n",
      "Epoch 524/700, Training Loss: 336.5453, Validation Loss: 337.6309\n",
      "Epoch 525/700, Training Loss: 336.5450, Validation Loss: 337.6309\n",
      "Epoch 526/700, Training Loss: 336.5446, Validation Loss: 337.6304\n",
      "Epoch 527/700, Training Loss: 336.5442, Validation Loss: 337.6300\n",
      "Epoch 528/700, Training Loss: 336.5438, Validation Loss: 337.6299\n",
      "Epoch 529/700, Training Loss: 336.5435, Validation Loss: 337.6294\n",
      "Epoch 530/700, Training Loss: 336.5430, Validation Loss: 337.6292\n",
      "Epoch 531/700, Training Loss: 336.5427, Validation Loss: 337.6291\n",
      "Epoch 532/700, Training Loss: 336.5423, Validation Loss: 337.6287\n",
      "Epoch 533/700, Training Loss: 336.5420, Validation Loss: 337.6284\n",
      "Epoch 534/700, Training Loss: 336.5416, Validation Loss: 337.6282\n",
      "Epoch 535/700, Training Loss: 336.5412, Validation Loss: 337.6278\n",
      "Epoch 536/700, Training Loss: 336.5409, Validation Loss: 337.6276\n",
      "Epoch 537/700, Training Loss: 336.5405, Validation Loss: 337.6274\n",
      "Epoch 538/700, Training Loss: 336.5402, Validation Loss: 337.6271\n",
      "Epoch 539/700, Training Loss: 336.5398, Validation Loss: 337.6270\n",
      "Epoch 540/700, Training Loss: 336.5394, Validation Loss: 337.6267\n",
      "Epoch 541/700, Training Loss: 336.5391, Validation Loss: 337.6264\n",
      "Epoch 542/700, Training Loss: 336.5388, Validation Loss: 337.6263\n",
      "Epoch 543/700, Training Loss: 336.5384, Validation Loss: 337.6259\n",
      "Epoch 544/700, Training Loss: 336.5381, Validation Loss: 337.6256\n",
      "Epoch 545/700, Training Loss: 336.5377, Validation Loss: 337.6255\n",
      "Epoch 546/700, Training Loss: 336.5374, Validation Loss: 337.6252\n",
      "Epoch 547/700, Training Loss: 336.5371, Validation Loss: 337.6249\n",
      "Epoch 548/700, Training Loss: 336.5367, Validation Loss: 337.6248\n",
      "Epoch 549/700, Training Loss: 336.5364, Validation Loss: 337.6245\n",
      "Epoch 550/700, Training Loss: 336.5361, Validation Loss: 337.6242\n",
      "Epoch 551/700, Training Loss: 336.5358, Validation Loss: 337.6241\n",
      "Epoch 552/700, Training Loss: 336.5354, Validation Loss: 337.6238\n",
      "Epoch 553/700, Training Loss: 336.5351, Validation Loss: 337.6236\n",
      "Epoch 554/700, Training Loss: 336.5348, Validation Loss: 337.6234\n",
      "Epoch 555/700, Training Loss: 336.5345, Validation Loss: 337.6231\n",
      "Epoch 556/700, Training Loss: 336.5341, Validation Loss: 337.6229\n",
      "Epoch 557/700, Training Loss: 336.5339, Validation Loss: 337.6227\n",
      "Epoch 558/700, Training Loss: 336.5336, Validation Loss: 337.6224\n",
      "Epoch 559/700, Training Loss: 336.5332, Validation Loss: 337.6222\n",
      "Epoch 560/700, Training Loss: 336.5329, Validation Loss: 337.6219\n",
      "Epoch 561/700, Training Loss: 336.5327, Validation Loss: 337.6217\n",
      "Epoch 562/700, Training Loss: 336.5323, Validation Loss: 337.6216\n",
      "Epoch 563/700, Training Loss: 336.5320, Validation Loss: 337.6213\n",
      "Epoch 564/700, Training Loss: 336.5317, Validation Loss: 337.6211\n",
      "Epoch 565/700, Training Loss: 336.5314, Validation Loss: 337.6209\n",
      "Epoch 566/700, Training Loss: 336.5312, Validation Loss: 337.6206\n",
      "Epoch 567/700, Training Loss: 336.5309, Validation Loss: 337.6204\n",
      "Epoch 568/700, Training Loss: 336.5306, Validation Loss: 337.6202\n",
      "Epoch 569/700, Training Loss: 336.5303, Validation Loss: 337.6200\n",
      "Epoch 570/700, Training Loss: 336.5300, Validation Loss: 337.6198\n",
      "Epoch 571/700, Training Loss: 336.5297, Validation Loss: 337.6195\n",
      "Epoch 572/700, Training Loss: 336.5294, Validation Loss: 337.6194\n",
      "Epoch 573/700, Training Loss: 336.5291, Validation Loss: 337.6192\n",
      "Epoch 574/700, Training Loss: 336.5289, Validation Loss: 337.6189\n",
      "Epoch 575/700, Training Loss: 336.5286, Validation Loss: 337.6188\n",
      "Epoch 576/700, Training Loss: 336.5283, Validation Loss: 337.6186\n",
      "Epoch 577/700, Training Loss: 336.5280, Validation Loss: 337.6183\n",
      "Epoch 578/700, Training Loss: 336.5277, Validation Loss: 337.6182\n",
      "Epoch 579/700, Training Loss: 336.5274, Validation Loss: 337.6180\n",
      "Epoch 580/700, Training Loss: 336.5272, Validation Loss: 337.6178\n",
      "Epoch 581/700, Training Loss: 336.5269, Validation Loss: 337.6176\n",
      "Epoch 582/700, Training Loss: 336.5267, Validation Loss: 337.6174\n",
      "Epoch 583/700, Training Loss: 336.5264, Validation Loss: 337.6173\n",
      "Epoch 584/700, Training Loss: 336.5261, Validation Loss: 337.6170\n",
      "Epoch 585/700, Training Loss: 336.5259, Validation Loss: 337.6169\n",
      "Epoch 586/700, Training Loss: 336.5256, Validation Loss: 337.6167\n",
      "Epoch 587/700, Training Loss: 336.5254, Validation Loss: 337.6164\n",
      "Epoch 588/700, Training Loss: 336.5251, Validation Loss: 337.6163\n",
      "Epoch 589/700, Training Loss: 336.5248, Validation Loss: 337.6161\n",
      "Epoch 590/700, Training Loss: 336.5246, Validation Loss: 337.6159\n",
      "Epoch 591/700, Training Loss: 336.5243, Validation Loss: 337.6158\n",
      "Epoch 592/700, Training Loss: 336.5240, Validation Loss: 337.6155\n",
      "Epoch 593/700, Training Loss: 336.5238, Validation Loss: 337.6154\n",
      "Epoch 594/700, Training Loss: 336.5236, Validation Loss: 337.6152\n",
      "Epoch 595/700, Training Loss: 336.5233, Validation Loss: 337.6150\n",
      "Epoch 596/700, Training Loss: 336.5230, Validation Loss: 337.6149\n",
      "Epoch 597/700, Training Loss: 336.5228, Validation Loss: 337.6147\n",
      "Epoch 598/700, Training Loss: 336.5225, Validation Loss: 337.6146\n",
      "Epoch 599/700, Training Loss: 336.5223, Validation Loss: 337.6144\n",
      "Epoch 600/700, Training Loss: 336.5221, Validation Loss: 337.6142\n",
      "Epoch 601/700, Training Loss: 336.5218, Validation Loss: 337.6141\n",
      "Epoch 602/700, Training Loss: 336.5216, Validation Loss: 337.6139\n",
      "Epoch 603/700, Training Loss: 336.5213, Validation Loss: 337.6137\n",
      "Epoch 604/700, Training Loss: 336.5211, Validation Loss: 337.6137\n",
      "Epoch 605/700, Training Loss: 336.5208, Validation Loss: 337.6134\n",
      "Epoch 606/700, Training Loss: 336.5206, Validation Loss: 337.6133\n",
      "Epoch 607/700, Training Loss: 336.5204, Validation Loss: 337.6132\n",
      "Epoch 608/700, Training Loss: 336.5201, Validation Loss: 337.6129\n",
      "Epoch 609/700, Training Loss: 336.5199, Validation Loss: 337.6129\n",
      "Epoch 610/700, Training Loss: 336.5197, Validation Loss: 337.6127\n",
      "Epoch 611/700, Training Loss: 336.5194, Validation Loss: 337.6125\n",
      "Epoch 612/700, Training Loss: 336.5192, Validation Loss: 337.6124\n",
      "Epoch 613/700, Training Loss: 336.5189, Validation Loss: 337.6122\n",
      "Epoch 614/700, Training Loss: 336.5187, Validation Loss: 337.6121\n",
      "Epoch 615/700, Training Loss: 336.5185, Validation Loss: 337.6119\n",
      "Epoch 616/700, Training Loss: 336.5182, Validation Loss: 337.6117\n",
      "Epoch 617/700, Training Loss: 336.5180, Validation Loss: 337.6116\n",
      "Epoch 618/700, Training Loss: 336.5178, Validation Loss: 337.6114\n",
      "Epoch 619/700, Training Loss: 336.5175, Validation Loss: 337.6113\n",
      "Epoch 620/700, Training Loss: 336.5173, Validation Loss: 337.6112\n",
      "Epoch 621/700, Training Loss: 336.5171, Validation Loss: 337.6110\n",
      "Epoch 622/700, Training Loss: 336.5169, Validation Loss: 337.6109\n",
      "Epoch 623/700, Training Loss: 336.5167, Validation Loss: 337.6107\n",
      "Epoch 624/700, Training Loss: 336.5164, Validation Loss: 337.6106\n",
      "Epoch 625/700, Training Loss: 336.5162, Validation Loss: 337.6104\n",
      "Epoch 626/700, Training Loss: 336.5160, Validation Loss: 337.6104\n",
      "Epoch 627/700, Training Loss: 336.5157, Validation Loss: 337.6102\n",
      "Epoch 628/700, Training Loss: 336.5155, Validation Loss: 337.6101\n",
      "Epoch 629/700, Training Loss: 336.5153, Validation Loss: 337.6099\n",
      "Epoch 630/700, Training Loss: 336.5151, Validation Loss: 337.6098\n",
      "Epoch 631/700, Training Loss: 336.5149, Validation Loss: 337.6096\n",
      "Epoch 632/700, Training Loss: 336.5147, Validation Loss: 337.6095\n",
      "Epoch 633/700, Training Loss: 336.5144, Validation Loss: 337.6095\n",
      "Epoch 634/700, Training Loss: 336.5142, Validation Loss: 337.6093\n",
      "Epoch 635/700, Training Loss: 336.5140, Validation Loss: 337.6093\n",
      "Epoch 636/700, Training Loss: 336.5138, Validation Loss: 337.6091\n",
      "Epoch 637/700, Training Loss: 336.5136, Validation Loss: 337.6089\n",
      "Epoch 638/700, Training Loss: 336.5134, Validation Loss: 337.6088\n",
      "Epoch 639/700, Training Loss: 336.5132, Validation Loss: 337.6086\n",
      "Epoch 640/700, Training Loss: 336.5129, Validation Loss: 337.6085\n",
      "Epoch 641/700, Training Loss: 336.5128, Validation Loss: 337.6084\n",
      "Epoch 642/700, Training Loss: 336.5125, Validation Loss: 337.6081\n",
      "Epoch 643/700, Training Loss: 336.5123, Validation Loss: 337.6080\n",
      "Epoch 644/700, Training Loss: 336.5121, Validation Loss: 337.6078\n",
      "Epoch 645/700, Training Loss: 336.5119, Validation Loss: 337.6078\n",
      "Epoch 646/700, Training Loss: 336.5117, Validation Loss: 337.6077\n",
      "Epoch 647/700, Training Loss: 336.5115, Validation Loss: 337.6077\n",
      "Epoch 648/700, Training Loss: 336.5113, Validation Loss: 337.6076\n",
      "Epoch 649/700, Training Loss: 336.5111, Validation Loss: 337.6075\n",
      "Epoch 650/700, Training Loss: 336.5109, Validation Loss: 337.6073\n",
      "Epoch 651/700, Training Loss: 336.5107, Validation Loss: 337.6071\n",
      "Epoch 652/700, Training Loss: 336.5105, Validation Loss: 337.6070\n",
      "Epoch 653/700, Training Loss: 336.5102, Validation Loss: 337.6069\n",
      "Epoch 654/700, Training Loss: 336.5101, Validation Loss: 337.6068\n",
      "Epoch 655/700, Training Loss: 336.5099, Validation Loss: 337.6068\n",
      "Epoch 656/700, Training Loss: 336.5096, Validation Loss: 337.6067\n",
      "Epoch 657/700, Training Loss: 336.5095, Validation Loss: 337.6065\n",
      "Epoch 658/700, Training Loss: 336.5092, Validation Loss: 337.6065\n",
      "Epoch 659/700, Training Loss: 336.5090, Validation Loss: 337.6064\n",
      "Epoch 660/700, Training Loss: 336.5089, Validation Loss: 337.6063\n",
      "Epoch 661/700, Training Loss: 336.5087, Validation Loss: 337.6063\n",
      "Epoch 662/700, Training Loss: 336.5084, Validation Loss: 337.6062\n",
      "Epoch 663/700, Training Loss: 336.5083, Validation Loss: 337.6060\n",
      "Epoch 664/700, Training Loss: 336.5081, Validation Loss: 337.6058\n",
      "Epoch 665/700, Training Loss: 336.5079, Validation Loss: 337.6058\n",
      "Epoch 666/700, Training Loss: 336.5077, Validation Loss: 337.6057\n",
      "Epoch 667/700, Training Loss: 336.5075, Validation Loss: 337.6057\n",
      "Epoch 668/700, Training Loss: 336.5073, Validation Loss: 337.6057\n",
      "Epoch 669/700, Training Loss: 336.5071, Validation Loss: 337.6057\n",
      "Epoch 670/700, Training Loss: 336.5069, Validation Loss: 337.6056\n",
      "Epoch 671/700, Training Loss: 336.5067, Validation Loss: 337.6055\n",
      "Epoch 672/700, Training Loss: 336.5065, Validation Loss: 337.6052\n",
      "Epoch 673/700, Training Loss: 336.5063, Validation Loss: 337.6051\n",
      "Epoch 674/700, Training Loss: 336.5062, Validation Loss: 337.6052\n",
      "Epoch 675/700, Training Loss: 336.5060, Validation Loss: 337.6050\n",
      "Epoch 676/700, Training Loss: 336.5058, Validation Loss: 337.6051\n",
      "Epoch 677/700, Training Loss: 336.5056, Validation Loss: 337.6049\n",
      "Epoch 678/700, Training Loss: 336.5054, Validation Loss: 337.6050\n",
      "Epoch 679/700, Training Loss: 336.5052, Validation Loss: 337.6049\n",
      "Epoch 680/700, Training Loss: 336.5050, Validation Loss: 337.6049\n",
      "Epoch 681/700, Training Loss: 336.5049, Validation Loss: 337.6047\n",
      "Epoch 682/700, Training Loss: 336.5047, Validation Loss: 337.6046\n",
      "Epoch 683/700, Training Loss: 336.5045, Validation Loss: 337.6045\n",
      "Epoch 684/700, Training Loss: 336.5043, Validation Loss: 337.6045\n",
      "Epoch 685/700, Training Loss: 336.5041, Validation Loss: 337.6043\n",
      "Epoch 686/700, Training Loss: 336.5039, Validation Loss: 337.6045\n",
      "Epoch 687/700, Training Loss: 336.5038, Validation Loss: 337.6042\n",
      "Epoch 688/700, Training Loss: 336.5036, Validation Loss: 337.6046\n",
      "Epoch 689/700, Training Loss: 336.5034, Validation Loss: 337.6039\n",
      "Epoch 690/700, Training Loss: 336.5033, Validation Loss: 337.6044\n",
      "Epoch 691/700, Training Loss: 336.5031, Validation Loss: 337.6037\n",
      "Epoch 692/700, Training Loss: 336.5029, Validation Loss: 337.6045\n",
      "Epoch 693/700, Training Loss: 336.5027, Validation Loss: 337.6035\n",
      "Epoch 694/700, Training Loss: 336.5026, Validation Loss: 337.6047\n",
      "Epoch 695/700, Training Loss: 336.5025, Validation Loss: 337.6032\n",
      "Epoch 696/700, Training Loss: 336.5023, Validation Loss: 337.6046\n",
      "Epoch 697/700, Training Loss: 336.5022, Validation Loss: 337.6030\n",
      "Epoch 698/700, Training Loss: 336.5020, Validation Loss: 337.6048\n",
      "Epoch 699/700, Training Loss: 336.5019, Validation Loss: 337.6028\n",
      "Epoch 700/700, Training Loss: 336.5018, Validation Loss: 337.6053\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.25%\n",
      "Mean Percent Accuracy (Yield): 89.03%\n",
      "Median Percent Accuracy (Days to Harvest): 79.45%\n",
      "Median Percent Accuracy (Yield): 92.42%\n",
      "Accuracy Within 10% (Days to Harvest): 23.59%\n",
      "Accuracy Within 10% (Yield): 61.77%\n",
      "MAE (Days to Harvest): 22.45\n",
      "MAE (Yield): 0.41\n",
      "RMSE (Days to Harvest): 25.94\n",
      "RMSE (Yield): 0.51\n",
      "\n",
      "Validation Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.17%\n",
      "Mean Percent Accuracy (Yield): 88.74%\n",
      "Median Percent Accuracy (Days to Harvest): 79.42%\n",
      "Median Percent Accuracy (Yield): 92.39%\n",
      "Accuracy Within 10% (Days to Harvest): 23.46%\n",
      "Accuracy Within 10% (Yield): 61.63%\n",
      "MAE (Days to Harvest): 22.49\n",
      "MAE (Yield): 0.41\n",
      "RMSE (Days to Harvest): 25.98\n",
      "RMSE (Yield): 0.51\n",
      "\n",
      "Training completed in 40.08 seconds\n",
      "Final Training Loss: 336.5018\n",
      "Final Validation Loss: 337.6053\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y.values, test_size=0.2, random_state=88)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Define the neural network\n",
    "class CropYieldNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CropYieldNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 128)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 2)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.swish = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.swish(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.leaky_relu(self.fc4(x))\n",
    "        x = self.leaky_relu(self.fc5(x))\n",
    "        return self.output(x)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move data tensors to the same device as the model\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = CropYieldNN(input_dim).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.008, weight_decay=2e-6)\n",
    "\n",
    "\n",
    "# Train the neural network\n",
    "n_epochs = 700\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_train_tensor)\n",
    "    train_loss = criterion(predictions, y_train_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_val_tensor)\n",
    "        val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "    # Store losses\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Final training and validation loss\n",
    "final_train_loss = train_losses[-1]\n",
    "final_val_loss = val_losses[-1]\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, n_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate metrics\n",
    "def evaluate_model_with_metrics(y_actual, y_pred, tolerance=0.1):\n",
    "    # Percent accuracy\n",
    "    percent_accuracies = 100 * (1 - np.abs(y_actual - y_pred) / y_actual)\n",
    "    mean_percent_accuracy = np.mean(percent_accuracies, axis=0)\n",
    "    median_percent_accuracy = np.median(percent_accuracies, axis=0)  # Median for robustness\n",
    "\n",
    "    # Accuracy within tolerance\n",
    "    within_tolerance = np.abs(y_actual - y_pred) <= (tolerance * y_actual)\n",
    "    percent_within_tolerance = np.mean(within_tolerance, axis=0) * 100\n",
    "\n",
    "    # MAE\n",
    "    mae = np.mean(np.abs(y_actual - y_pred), axis=0)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(np.mean((y_actual - y_pred) ** 2, axis=0))\n",
    "\n",
    "    return {\n",
    "        \"mean_percent_accuracy\": mean_percent_accuracy,\n",
    "        \"median_percent_accuracy\": median_percent_accuracy,\n",
    "        \"percent_within_tolerance\": percent_within_tolerance,\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "    }\n",
    "\n",
    "# Evaluate metrics\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_preds = model(X_train_tensor).cpu().numpy()\n",
    "    val_preds = model(X_val_tensor).cpu().numpy()\n",
    "    y_train_np = y_train_tensor.cpu().numpy()\n",
    "    y_val_np = y_val_tensor.cpu().numpy()\n",
    "\n",
    "    # Training metrics\n",
    "    train_metrics = evaluate_model_with_metrics(y_train_np, train_preds, tolerance=0.1)\n",
    "    \n",
    "    # Validation metrics\n",
    "    val_metrics = evaluate_model_with_metrics(y_val_np, val_preds, tolerance=0.1)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {train_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {train_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {train_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {train_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {train_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {train_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {train_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {train_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {train_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {train_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {val_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {val_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {val_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {val_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {val_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {val_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {val_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {val_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {val_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {val_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {final_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6000, Training Loss: 54.2268, Validation Loss: 54.1538\n",
      "Epoch 2/6000, Training Loss: 54.1868, Validation Loss: 54.1138\n",
      "Epoch 3/6000, Training Loss: 54.1468, Validation Loss: 54.0739\n",
      "Epoch 4/6000, Training Loss: 54.1068, Validation Loss: 54.0339\n",
      "Epoch 5/6000, Training Loss: 54.0668, Validation Loss: 53.9940\n",
      "Epoch 6/6000, Training Loss: 54.0267, Validation Loss: 53.9540\n",
      "Epoch 7/6000, Training Loss: 53.9867, Validation Loss: 53.9141\n",
      "Epoch 8/6000, Training Loss: 53.9467, Validation Loss: 53.8742\n",
      "Epoch 9/6000, Training Loss: 53.9067, Validation Loss: 53.8342\n",
      "Epoch 10/6000, Training Loss: 53.8667, Validation Loss: 53.7943\n",
      "Epoch 11/6000, Training Loss: 53.8267, Validation Loss: 53.7543\n",
      "Epoch 12/6000, Training Loss: 53.7867, Validation Loss: 53.7144\n",
      "Epoch 13/6000, Training Loss: 53.7467, Validation Loss: 53.6744\n",
      "Epoch 14/6000, Training Loss: 53.7067, Validation Loss: 53.6345\n",
      "Epoch 15/6000, Training Loss: 53.6666, Validation Loss: 53.5946\n",
      "Epoch 16/6000, Training Loss: 53.6266, Validation Loss: 53.5546\n",
      "Epoch 17/6000, Training Loss: 53.5866, Validation Loss: 53.5147\n",
      "Epoch 18/6000, Training Loss: 53.5466, Validation Loss: 53.4747\n",
      "Epoch 19/6000, Training Loss: 53.5066, Validation Loss: 53.4348\n",
      "Epoch 20/6000, Training Loss: 53.4666, Validation Loss: 53.3949\n",
      "Epoch 21/6000, Training Loss: 53.4266, Validation Loss: 53.3549\n",
      "Epoch 22/6000, Training Loss: 53.3866, Validation Loss: 53.3150\n",
      "Epoch 23/6000, Training Loss: 53.3466, Validation Loss: 53.2750\n",
      "Epoch 24/6000, Training Loss: 53.3065, Validation Loss: 53.2351\n",
      "Epoch 25/6000, Training Loss: 53.2665, Validation Loss: 53.1951\n",
      "Epoch 26/6000, Training Loss: 53.2265, Validation Loss: 53.1552\n",
      "Epoch 27/6000, Training Loss: 53.1865, Validation Loss: 53.1153\n",
      "Epoch 28/6000, Training Loss: 53.1465, Validation Loss: 53.0753\n",
      "Epoch 29/6000, Training Loss: 53.1065, Validation Loss: 53.0354\n",
      "Epoch 30/6000, Training Loss: 53.0665, Validation Loss: 52.9954\n",
      "Epoch 31/6000, Training Loss: 53.0265, Validation Loss: 52.9555\n",
      "Epoch 32/6000, Training Loss: 52.9865, Validation Loss: 52.9156\n",
      "Epoch 33/6000, Training Loss: 52.9465, Validation Loss: 52.8756\n",
      "Epoch 34/6000, Training Loss: 52.9065, Validation Loss: 52.8357\n",
      "Epoch 35/6000, Training Loss: 52.8664, Validation Loss: 52.7957\n",
      "Epoch 36/6000, Training Loss: 52.8264, Validation Loss: 52.7558\n",
      "Epoch 37/6000, Training Loss: 52.7864, Validation Loss: 52.7159\n",
      "Epoch 38/6000, Training Loss: 52.7464, Validation Loss: 52.6759\n",
      "Epoch 39/6000, Training Loss: 52.7064, Validation Loss: 52.6360\n",
      "Epoch 40/6000, Training Loss: 52.6664, Validation Loss: 52.5960\n",
      "Epoch 41/6000, Training Loss: 52.6264, Validation Loss: 52.5561\n",
      "Epoch 42/6000, Training Loss: 52.5864, Validation Loss: 52.5162\n",
      "Epoch 43/6000, Training Loss: 52.5464, Validation Loss: 52.4762\n",
      "Epoch 44/6000, Training Loss: 52.5064, Validation Loss: 52.4363\n",
      "Epoch 45/6000, Training Loss: 52.4664, Validation Loss: 52.3964\n",
      "Epoch 46/6000, Training Loss: 52.4264, Validation Loss: 52.3564\n",
      "Epoch 47/6000, Training Loss: 52.3864, Validation Loss: 52.3165\n",
      "Epoch 48/6000, Training Loss: 52.3463, Validation Loss: 52.2766\n",
      "Epoch 49/6000, Training Loss: 52.3063, Validation Loss: 52.2366\n",
      "Epoch 50/6000, Training Loss: 52.2663, Validation Loss: 52.1967\n",
      "Epoch 51/6000, Training Loss: 52.2263, Validation Loss: 52.1567\n",
      "Epoch 52/6000, Training Loss: 52.1863, Validation Loss: 52.1168\n",
      "Epoch 53/6000, Training Loss: 52.1463, Validation Loss: 52.0769\n",
      "Epoch 54/6000, Training Loss: 52.1063, Validation Loss: 52.0370\n",
      "Epoch 55/6000, Training Loss: 52.0663, Validation Loss: 51.9970\n",
      "Epoch 56/6000, Training Loss: 52.0263, Validation Loss: 51.9571\n",
      "Epoch 57/6000, Training Loss: 51.9864, Validation Loss: 51.9172\n",
      "Epoch 58/6000, Training Loss: 51.9464, Validation Loss: 51.8773\n",
      "Epoch 59/6000, Training Loss: 51.9064, Validation Loss: 51.8373\n",
      "Epoch 60/6000, Training Loss: 51.8664, Validation Loss: 51.7974\n",
      "Epoch 61/6000, Training Loss: 51.8264, Validation Loss: 51.7575\n",
      "Epoch 62/6000, Training Loss: 51.7864, Validation Loss: 51.7176\n",
      "Epoch 63/6000, Training Loss: 51.7465, Validation Loss: 51.6777\n",
      "Epoch 64/6000, Training Loss: 51.7065, Validation Loss: 51.6378\n",
      "Epoch 65/6000, Training Loss: 51.6665, Validation Loss: 51.5979\n",
      "Epoch 66/6000, Training Loss: 51.6266, Validation Loss: 51.5580\n",
      "Epoch 67/6000, Training Loss: 51.5866, Validation Loss: 51.5182\n",
      "Epoch 68/6000, Training Loss: 51.5467, Validation Loss: 51.4783\n",
      "Epoch 69/6000, Training Loss: 51.5068, Validation Loss: 51.4385\n",
      "Epoch 70/6000, Training Loss: 51.4669, Validation Loss: 51.3986\n",
      "Epoch 71/6000, Training Loss: 51.4270, Validation Loss: 51.3588\n",
      "Epoch 72/6000, Training Loss: 51.3871, Validation Loss: 51.3190\n",
      "Epoch 73/6000, Training Loss: 51.3473, Validation Loss: 51.2793\n",
      "Epoch 74/6000, Training Loss: 51.3074, Validation Loss: 51.2395\n",
      "Epoch 75/6000, Training Loss: 51.2676, Validation Loss: 51.1998\n",
      "Epoch 76/6000, Training Loss: 51.2279, Validation Loss: 51.1602\n",
      "Epoch 77/6000, Training Loss: 51.1881, Validation Loss: 51.1205\n",
      "Epoch 78/6000, Training Loss: 51.1484, Validation Loss: 51.0809\n",
      "Epoch 79/6000, Training Loss: 51.1088, Validation Loss: 51.0414\n",
      "Epoch 80/6000, Training Loss: 51.0692, Validation Loss: 51.0020\n",
      "Epoch 81/6000, Training Loss: 51.0297, Validation Loss: 50.9626\n",
      "Epoch 82/6000, Training Loss: 50.9903, Validation Loss: 50.9234\n",
      "Epoch 83/6000, Training Loss: 50.9510, Validation Loss: 50.8843\n",
      "Epoch 84/6000, Training Loss: 50.9119, Validation Loss: 50.8454\n",
      "Epoch 85/6000, Training Loss: 50.8729, Validation Loss: 50.8068\n",
      "Epoch 86/6000, Training Loss: 50.8342, Validation Loss: 50.7684\n",
      "Epoch 87/6000, Training Loss: 50.7958, Validation Loss: 50.7303\n",
      "Epoch 88/6000, Training Loss: 50.7576, Validation Loss: 50.6926\n",
      "Epoch 89/6000, Training Loss: 50.7198, Validation Loss: 50.6553\n",
      "Epoch 90/6000, Training Loss: 50.6825, Validation Loss: 50.6184\n",
      "Epoch 91/6000, Training Loss: 50.6455, Validation Loss: 50.5819\n",
      "Epoch 92/6000, Training Loss: 50.6090, Validation Loss: 50.5460\n",
      "Epoch 93/6000, Training Loss: 50.5730, Validation Loss: 50.5106\n",
      "Epoch 94/6000, Training Loss: 50.5375, Validation Loss: 50.4757\n",
      "Epoch 95/6000, Training Loss: 50.5025, Validation Loss: 50.4413\n",
      "Epoch 96/6000, Training Loss: 50.4681, Validation Loss: 50.4077\n",
      "Epoch 97/6000, Training Loss: 50.4344, Validation Loss: 50.3747\n",
      "Epoch 98/6000, Training Loss: 50.4014, Validation Loss: 50.3426\n",
      "Epoch 99/6000, Training Loss: 50.3692, Validation Loss: 50.3113\n",
      "Epoch 100/6000, Training Loss: 50.3378, Validation Loss: 50.2808\n",
      "Epoch 101/6000, Training Loss: 50.3072, Validation Loss: 50.2511\n",
      "Epoch 102/6000, Training Loss: 50.2775, Validation Loss: 50.2223\n",
      "Epoch 103/6000, Training Loss: 50.2485, Validation Loss: 50.1942\n",
      "Epoch 104/6000, Training Loss: 50.2203, Validation Loss: 50.1669\n",
      "Epoch 105/6000, Training Loss: 50.1928, Validation Loss: 50.1403\n",
      "Epoch 106/6000, Training Loss: 50.1661, Validation Loss: 50.1144\n",
      "Epoch 107/6000, Training Loss: 50.1402, Validation Loss: 50.0893\n",
      "Epoch 108/6000, Training Loss: 50.1150, Validation Loss: 50.0649\n",
      "Epoch 109/6000, Training Loss: 50.0905, Validation Loss: 50.0411\n",
      "Epoch 110/6000, Training Loss: 50.0666, Validation Loss: 50.0180\n",
      "Epoch 111/6000, Training Loss: 50.0433, Validation Loss: 49.9953\n",
      "Epoch 112/6000, Training Loss: 50.0206, Validation Loss: 49.9731\n",
      "Epoch 113/6000, Training Loss: 49.9983, Validation Loss: 49.9513\n",
      "Epoch 114/6000, Training Loss: 49.9764, Validation Loss: 49.9299\n",
      "Epoch 115/6000, Training Loss: 49.9549, Validation Loss: 49.9088\n",
      "Epoch 116/6000, Training Loss: 49.9337, Validation Loss: 49.8880\n",
      "Epoch 117/6000, Training Loss: 49.9128, Validation Loss: 49.8674\n",
      "Epoch 118/6000, Training Loss: 49.8922, Validation Loss: 49.8471\n",
      "Epoch 119/6000, Training Loss: 49.8717, Validation Loss: 49.8269\n",
      "Epoch 120/6000, Training Loss: 49.8514, Validation Loss: 49.8067\n",
      "Epoch 121/6000, Training Loss: 49.8312, Validation Loss: 49.7867\n",
      "Epoch 122/6000, Training Loss: 49.8111, Validation Loss: 49.7667\n",
      "Epoch 123/6000, Training Loss: 49.7911, Validation Loss: 49.7468\n",
      "Epoch 124/6000, Training Loss: 49.7711, Validation Loss: 49.7269\n",
      "Epoch 125/6000, Training Loss: 49.7511, Validation Loss: 49.7070\n",
      "Epoch 126/6000, Training Loss: 49.7312, Validation Loss: 49.6871\n",
      "Epoch 127/6000, Training Loss: 49.7112, Validation Loss: 49.6672\n",
      "Epoch 128/6000, Training Loss: 49.6913, Validation Loss: 49.6473\n",
      "Epoch 129/6000, Training Loss: 49.6713, Validation Loss: 49.6274\n",
      "Epoch 130/6000, Training Loss: 49.6513, Validation Loss: 49.6074\n",
      "Epoch 131/6000, Training Loss: 49.6312, Validation Loss: 49.5874\n",
      "Epoch 132/6000, Training Loss: 49.6112, Validation Loss: 49.5673\n",
      "Epoch 133/6000, Training Loss: 49.5911, Validation Loss: 49.5473\n",
      "Epoch 134/6000, Training Loss: 49.5710, Validation Loss: 49.5272\n",
      "Epoch 135/6000, Training Loss: 49.5509, Validation Loss: 49.5071\n",
      "Epoch 136/6000, Training Loss: 49.5307, Validation Loss: 49.4870\n",
      "Epoch 137/6000, Training Loss: 49.5105, Validation Loss: 49.4668\n",
      "Epoch 138/6000, Training Loss: 49.4904, Validation Loss: 49.4467\n",
      "Epoch 139/6000, Training Loss: 49.4702, Validation Loss: 49.4265\n",
      "Epoch 140/6000, Training Loss: 49.4500, Validation Loss: 49.4063\n",
      "Epoch 141/6000, Training Loss: 49.4298, Validation Loss: 49.3862\n",
      "Epoch 142/6000, Training Loss: 49.4095, Validation Loss: 49.3660\n",
      "Epoch 143/6000, Training Loss: 49.3893, Validation Loss: 49.3459\n",
      "Epoch 144/6000, Training Loss: 49.3691, Validation Loss: 49.3257\n",
      "Epoch 145/6000, Training Loss: 49.3489, Validation Loss: 49.3055\n",
      "Epoch 146/6000, Training Loss: 49.3288, Validation Loss: 49.2854\n",
      "Epoch 147/6000, Training Loss: 49.3086, Validation Loss: 49.2653\n",
      "Epoch 148/6000, Training Loss: 49.2884, Validation Loss: 49.2451\n",
      "Epoch 149/6000, Training Loss: 49.2682, Validation Loss: 49.2250\n",
      "Epoch 150/6000, Training Loss: 49.2481, Validation Loss: 49.2049\n",
      "Epoch 151/6000, Training Loss: 49.2280, Validation Loss: 49.1848\n",
      "Epoch 152/6000, Training Loss: 49.2078, Validation Loss: 49.1648\n",
      "Epoch 153/6000, Training Loss: 49.1877, Validation Loss: 49.1447\n",
      "Epoch 154/6000, Training Loss: 49.1676, Validation Loss: 49.1246\n",
      "Epoch 155/6000, Training Loss: 49.1475, Validation Loss: 49.1046\n",
      "Epoch 156/6000, Training Loss: 49.1274, Validation Loss: 49.0845\n",
      "Epoch 157/6000, Training Loss: 49.1073, Validation Loss: 49.0645\n",
      "Epoch 158/6000, Training Loss: 49.0872, Validation Loss: 49.0445\n",
      "Epoch 159/6000, Training Loss: 49.0672, Validation Loss: 49.0245\n",
      "Epoch 160/6000, Training Loss: 49.0471, Validation Loss: 49.0045\n",
      "Epoch 161/6000, Training Loss: 49.0271, Validation Loss: 48.9844\n",
      "Epoch 162/6000, Training Loss: 49.0070, Validation Loss: 48.9644\n",
      "Epoch 163/6000, Training Loss: 48.9870, Validation Loss: 48.9444\n",
      "Epoch 164/6000, Training Loss: 48.9669, Validation Loss: 48.9244\n",
      "Epoch 165/6000, Training Loss: 48.9469, Validation Loss: 48.9044\n",
      "Epoch 166/6000, Training Loss: 48.9268, Validation Loss: 48.8844\n",
      "Epoch 167/6000, Training Loss: 48.9068, Validation Loss: 48.8645\n",
      "Epoch 168/6000, Training Loss: 48.8868, Validation Loss: 48.8445\n",
      "Epoch 169/6000, Training Loss: 48.8667, Validation Loss: 48.8245\n",
      "Epoch 170/6000, Training Loss: 48.8467, Validation Loss: 48.8045\n",
      "Epoch 171/6000, Training Loss: 48.8267, Validation Loss: 48.7845\n",
      "Epoch 172/6000, Training Loss: 48.8066, Validation Loss: 48.7645\n",
      "Epoch 173/6000, Training Loss: 48.7866, Validation Loss: 48.7445\n",
      "Epoch 174/6000, Training Loss: 48.7666, Validation Loss: 48.7245\n",
      "Epoch 175/6000, Training Loss: 48.7465, Validation Loss: 48.7046\n",
      "Epoch 176/6000, Training Loss: 48.7265, Validation Loss: 48.6846\n",
      "Epoch 177/6000, Training Loss: 48.7065, Validation Loss: 48.6646\n",
      "Epoch 178/6000, Training Loss: 48.6865, Validation Loss: 48.6446\n",
      "Epoch 179/6000, Training Loss: 48.6664, Validation Loss: 48.6246\n",
      "Epoch 180/6000, Training Loss: 48.6464, Validation Loss: 48.6047\n",
      "Epoch 181/6000, Training Loss: 48.6264, Validation Loss: 48.5847\n",
      "Epoch 182/6000, Training Loss: 48.6063, Validation Loss: 48.5647\n",
      "Epoch 183/6000, Training Loss: 48.5863, Validation Loss: 48.5447\n",
      "Epoch 184/6000, Training Loss: 48.5663, Validation Loss: 48.5247\n",
      "Epoch 185/6000, Training Loss: 48.5463, Validation Loss: 48.5048\n",
      "Epoch 186/6000, Training Loss: 48.5263, Validation Loss: 48.4848\n",
      "Epoch 187/6000, Training Loss: 48.5062, Validation Loss: 48.4648\n",
      "Epoch 188/6000, Training Loss: 48.4862, Validation Loss: 48.4448\n",
      "Epoch 189/6000, Training Loss: 48.4662, Validation Loss: 48.4249\n",
      "Epoch 190/6000, Training Loss: 48.4462, Validation Loss: 48.4049\n",
      "Epoch 191/6000, Training Loss: 48.4262, Validation Loss: 48.3849\n",
      "Epoch 192/6000, Training Loss: 48.4061, Validation Loss: 48.3649\n",
      "Epoch 193/6000, Training Loss: 48.3861, Validation Loss: 48.3450\n",
      "Epoch 194/6000, Training Loss: 48.3661, Validation Loss: 48.3250\n",
      "Epoch 195/6000, Training Loss: 48.3461, Validation Loss: 48.3050\n",
      "Epoch 196/6000, Training Loss: 48.3261, Validation Loss: 48.2851\n",
      "Epoch 197/6000, Training Loss: 48.3061, Validation Loss: 48.2651\n",
      "Epoch 198/6000, Training Loss: 48.2860, Validation Loss: 48.2451\n",
      "Epoch 199/6000, Training Loss: 48.2660, Validation Loss: 48.2252\n",
      "Epoch 200/6000, Training Loss: 48.2460, Validation Loss: 48.2052\n",
      "Epoch 201/6000, Training Loss: 48.2260, Validation Loss: 48.1852\n",
      "Epoch 202/6000, Training Loss: 48.2060, Validation Loss: 48.1653\n",
      "Epoch 203/6000, Training Loss: 48.1860, Validation Loss: 48.1453\n",
      "Epoch 204/6000, Training Loss: 48.1660, Validation Loss: 48.1253\n",
      "Epoch 205/6000, Training Loss: 48.1459, Validation Loss: 48.1053\n",
      "Epoch 206/6000, Training Loss: 48.1259, Validation Loss: 48.0854\n",
      "Epoch 207/6000, Training Loss: 48.1059, Validation Loss: 48.0654\n",
      "Epoch 208/6000, Training Loss: 48.0859, Validation Loss: 48.0454\n",
      "Epoch 209/6000, Training Loss: 48.0659, Validation Loss: 48.0255\n",
      "Epoch 210/6000, Training Loss: 48.0459, Validation Loss: 48.0055\n",
      "Epoch 211/6000, Training Loss: 48.0259, Validation Loss: 47.9855\n",
      "Epoch 212/6000, Training Loss: 48.0059, Validation Loss: 47.9656\n",
      "Epoch 213/6000, Training Loss: 47.9858, Validation Loss: 47.9456\n",
      "Epoch 214/6000, Training Loss: 47.9658, Validation Loss: 47.9256\n",
      "Epoch 215/6000, Training Loss: 47.9458, Validation Loss: 47.9057\n",
      "Epoch 216/6000, Training Loss: 47.9258, Validation Loss: 47.8857\n",
      "Epoch 217/6000, Training Loss: 47.9058, Validation Loss: 47.8657\n",
      "Epoch 218/6000, Training Loss: 47.8858, Validation Loss: 47.8458\n",
      "Epoch 219/6000, Training Loss: 47.8658, Validation Loss: 47.8258\n",
      "Epoch 220/6000, Training Loss: 47.8458, Validation Loss: 47.8058\n",
      "Epoch 221/6000, Training Loss: 47.8258, Validation Loss: 47.7859\n",
      "Epoch 222/6000, Training Loss: 47.8058, Validation Loss: 47.7659\n",
      "Epoch 223/6000, Training Loss: 47.7857, Validation Loss: 47.7459\n",
      "Epoch 224/6000, Training Loss: 47.7657, Validation Loss: 47.7260\n",
      "Epoch 225/6000, Training Loss: 47.7457, Validation Loss: 47.7060\n",
      "Epoch 226/6000, Training Loss: 47.7257, Validation Loss: 47.6860\n",
      "Epoch 227/6000, Training Loss: 47.7057, Validation Loss: 47.6661\n",
      "Epoch 228/6000, Training Loss: 47.6857, Validation Loss: 47.6461\n",
      "Epoch 229/6000, Training Loss: 47.6657, Validation Loss: 47.6261\n",
      "Epoch 230/6000, Training Loss: 47.6457, Validation Loss: 47.6062\n",
      "Epoch 231/6000, Training Loss: 47.6257, Validation Loss: 47.5862\n",
      "Epoch 232/6000, Training Loss: 47.6057, Validation Loss: 47.5662\n",
      "Epoch 233/6000, Training Loss: 47.5856, Validation Loss: 47.5463\n",
      "Epoch 234/6000, Training Loss: 47.5656, Validation Loss: 47.5263\n",
      "Epoch 235/6000, Training Loss: 47.5456, Validation Loss: 47.5064\n",
      "Epoch 236/6000, Training Loss: 47.5256, Validation Loss: 47.4864\n",
      "Epoch 237/6000, Training Loss: 47.5056, Validation Loss: 47.4664\n",
      "Epoch 238/6000, Training Loss: 47.4856, Validation Loss: 47.4465\n",
      "Epoch 239/6000, Training Loss: 47.4656, Validation Loss: 47.4265\n",
      "Epoch 240/6000, Training Loss: 47.4456, Validation Loss: 47.4065\n",
      "Epoch 241/6000, Training Loss: 47.4256, Validation Loss: 47.3866\n",
      "Epoch 242/6000, Training Loss: 47.4056, Validation Loss: 47.3666\n",
      "Epoch 243/6000, Training Loss: 47.3856, Validation Loss: 47.3466\n",
      "Epoch 244/6000, Training Loss: 47.3656, Validation Loss: 47.3267\n",
      "Epoch 245/6000, Training Loss: 47.3455, Validation Loss: 47.3067\n",
      "Epoch 246/6000, Training Loss: 47.3255, Validation Loss: 47.2867\n",
      "Epoch 247/6000, Training Loss: 47.3055, Validation Loss: 47.2668\n",
      "Epoch 248/6000, Training Loss: 47.2855, Validation Loss: 47.2468\n",
      "Epoch 249/6000, Training Loss: 47.2655, Validation Loss: 47.2268\n",
      "Epoch 250/6000, Training Loss: 47.2455, Validation Loss: 47.2069\n",
      "Epoch 251/6000, Training Loss: 47.2255, Validation Loss: 47.1869\n",
      "Epoch 252/6000, Training Loss: 47.2055, Validation Loss: 47.1669\n",
      "Epoch 253/6000, Training Loss: 47.1855, Validation Loss: 47.1470\n",
      "Epoch 254/6000, Training Loss: 47.1655, Validation Loss: 47.1270\n",
      "Epoch 255/6000, Training Loss: 47.1455, Validation Loss: 47.1070\n",
      "Epoch 256/6000, Training Loss: 47.1254, Validation Loss: 47.0871\n",
      "Epoch 257/6000, Training Loss: 47.1054, Validation Loss: 47.0671\n",
      "Epoch 258/6000, Training Loss: 47.0854, Validation Loss: 47.0472\n",
      "Epoch 259/6000, Training Loss: 47.0654, Validation Loss: 47.0272\n",
      "Epoch 260/6000, Training Loss: 47.0454, Validation Loss: 47.0072\n",
      "Epoch 261/6000, Training Loss: 47.0254, Validation Loss: 46.9873\n",
      "Epoch 262/6000, Training Loss: 47.0054, Validation Loss: 46.9673\n",
      "Epoch 263/6000, Training Loss: 46.9854, Validation Loss: 46.9473\n",
      "Epoch 264/6000, Training Loss: 46.9654, Validation Loss: 46.9274\n",
      "Epoch 265/6000, Training Loss: 46.9454, Validation Loss: 46.9074\n",
      "Epoch 266/6000, Training Loss: 46.9254, Validation Loss: 46.8874\n",
      "Epoch 267/6000, Training Loss: 46.9053, Validation Loss: 46.8675\n",
      "Epoch 268/6000, Training Loss: 46.8853, Validation Loss: 46.8475\n",
      "Epoch 269/6000, Training Loss: 46.8653, Validation Loss: 46.8275\n",
      "Epoch 270/6000, Training Loss: 46.8453, Validation Loss: 46.8076\n",
      "Epoch 271/6000, Training Loss: 46.8253, Validation Loss: 46.7876\n",
      "Epoch 272/6000, Training Loss: 46.8053, Validation Loss: 46.7676\n",
      "Epoch 273/6000, Training Loss: 46.7853, Validation Loss: 46.7477\n",
      "Epoch 274/6000, Training Loss: 46.7653, Validation Loss: 46.7277\n",
      "Epoch 275/6000, Training Loss: 46.7453, Validation Loss: 46.7077\n",
      "Epoch 276/6000, Training Loss: 46.7253, Validation Loss: 46.6878\n",
      "Epoch 277/6000, Training Loss: 46.7053, Validation Loss: 46.6678\n",
      "Epoch 278/6000, Training Loss: 46.6852, Validation Loss: 46.6478\n",
      "Epoch 279/6000, Training Loss: 46.6652, Validation Loss: 46.6279\n",
      "Epoch 280/6000, Training Loss: 46.6452, Validation Loss: 46.6079\n",
      "Epoch 281/6000, Training Loss: 46.6252, Validation Loss: 46.5880\n",
      "Epoch 282/6000, Training Loss: 46.6052, Validation Loss: 46.5680\n",
      "Epoch 283/6000, Training Loss: 46.5852, Validation Loss: 46.5480\n",
      "Epoch 284/6000, Training Loss: 46.5652, Validation Loss: 46.5281\n",
      "Epoch 285/6000, Training Loss: 46.5452, Validation Loss: 46.5081\n",
      "Epoch 286/6000, Training Loss: 46.5252, Validation Loss: 46.4881\n",
      "Epoch 287/6000, Training Loss: 46.5052, Validation Loss: 46.4682\n",
      "Epoch 288/6000, Training Loss: 46.4852, Validation Loss: 46.4482\n",
      "Epoch 289/6000, Training Loss: 46.4652, Validation Loss: 46.4282\n",
      "Epoch 290/6000, Training Loss: 46.4451, Validation Loss: 46.4083\n",
      "Epoch 291/6000, Training Loss: 46.4251, Validation Loss: 46.3883\n",
      "Epoch 292/6000, Training Loss: 46.4051, Validation Loss: 46.3683\n",
      "Epoch 293/6000, Training Loss: 46.3851, Validation Loss: 46.3484\n",
      "Epoch 294/6000, Training Loss: 46.3651, Validation Loss: 46.3284\n",
      "Epoch 295/6000, Training Loss: 46.3451, Validation Loss: 46.3084\n",
      "Epoch 296/6000, Training Loss: 46.3251, Validation Loss: 46.2885\n",
      "Epoch 297/6000, Training Loss: 46.3051, Validation Loss: 46.2685\n",
      "Epoch 298/6000, Training Loss: 46.2851, Validation Loss: 46.2485\n",
      "Epoch 299/6000, Training Loss: 46.2651, Validation Loss: 46.2286\n",
      "Epoch 300/6000, Training Loss: 46.2451, Validation Loss: 46.2086\n",
      "Epoch 301/6000, Training Loss: 46.2250, Validation Loss: 46.1886\n",
      "Epoch 302/6000, Training Loss: 46.2050, Validation Loss: 46.1687\n",
      "Epoch 303/6000, Training Loss: 46.1850, Validation Loss: 46.1487\n",
      "Epoch 304/6000, Training Loss: 46.1650, Validation Loss: 46.1287\n",
      "Epoch 305/6000, Training Loss: 46.1450, Validation Loss: 46.1088\n",
      "Epoch 306/6000, Training Loss: 46.1250, Validation Loss: 46.0888\n",
      "Epoch 307/6000, Training Loss: 46.1050, Validation Loss: 46.0689\n",
      "Epoch 308/6000, Training Loss: 46.0850, Validation Loss: 46.0489\n",
      "Epoch 309/6000, Training Loss: 46.0650, Validation Loss: 46.0289\n",
      "Epoch 310/6000, Training Loss: 46.0450, Validation Loss: 46.0090\n",
      "Epoch 311/6000, Training Loss: 46.0250, Validation Loss: 45.9890\n",
      "Epoch 312/6000, Training Loss: 46.0050, Validation Loss: 45.9690\n",
      "Epoch 313/6000, Training Loss: 45.9849, Validation Loss: 45.9491\n",
      "Epoch 314/6000, Training Loss: 45.9649, Validation Loss: 45.9291\n",
      "Epoch 315/6000, Training Loss: 45.9449, Validation Loss: 45.9091\n",
      "Epoch 316/6000, Training Loss: 45.9249, Validation Loss: 45.8892\n",
      "Epoch 317/6000, Training Loss: 45.9049, Validation Loss: 45.8692\n",
      "Epoch 318/6000, Training Loss: 45.8849, Validation Loss: 45.8492\n",
      "Epoch 319/6000, Training Loss: 45.8649, Validation Loss: 45.8293\n",
      "Epoch 320/6000, Training Loss: 45.8449, Validation Loss: 45.8093\n",
      "Epoch 321/6000, Training Loss: 45.8249, Validation Loss: 45.7893\n",
      "Epoch 322/6000, Training Loss: 45.8049, Validation Loss: 45.7694\n",
      "Epoch 323/6000, Training Loss: 45.7849, Validation Loss: 45.7494\n",
      "Epoch 324/6000, Training Loss: 45.7649, Validation Loss: 45.7294\n",
      "Epoch 325/6000, Training Loss: 45.7448, Validation Loss: 45.7095\n",
      "Epoch 326/6000, Training Loss: 45.7248, Validation Loss: 45.6895\n",
      "Epoch 327/6000, Training Loss: 45.7048, Validation Loss: 45.6695\n",
      "Epoch 328/6000, Training Loss: 45.6848, Validation Loss: 45.6496\n",
      "Epoch 329/6000, Training Loss: 45.6648, Validation Loss: 45.6296\n",
      "Epoch 330/6000, Training Loss: 45.6448, Validation Loss: 45.6096\n",
      "Epoch 331/6000, Training Loss: 45.6248, Validation Loss: 45.5897\n",
      "Epoch 332/6000, Training Loss: 45.6048, Validation Loss: 45.5697\n",
      "Epoch 333/6000, Training Loss: 45.5848, Validation Loss: 45.5498\n",
      "Epoch 334/6000, Training Loss: 45.5648, Validation Loss: 45.5298\n",
      "Epoch 335/6000, Training Loss: 45.5448, Validation Loss: 45.5098\n",
      "Epoch 336/6000, Training Loss: 45.5247, Validation Loss: 45.4899\n",
      "Epoch 337/6000, Training Loss: 45.5047, Validation Loss: 45.4699\n",
      "Epoch 338/6000, Training Loss: 45.4847, Validation Loss: 45.4499\n",
      "Epoch 339/6000, Training Loss: 45.4647, Validation Loss: 45.4300\n",
      "Epoch 340/6000, Training Loss: 45.4447, Validation Loss: 45.4100\n",
      "Epoch 341/6000, Training Loss: 45.4247, Validation Loss: 45.3900\n",
      "Epoch 342/6000, Training Loss: 45.4047, Validation Loss: 45.3701\n",
      "Epoch 343/6000, Training Loss: 45.3847, Validation Loss: 45.3501\n",
      "Epoch 344/6000, Training Loss: 45.3647, Validation Loss: 45.3301\n",
      "Epoch 345/6000, Training Loss: 45.3447, Validation Loss: 45.3102\n",
      "Epoch 346/6000, Training Loss: 45.3247, Validation Loss: 45.2902\n",
      "Epoch 347/6000, Training Loss: 45.3047, Validation Loss: 45.2702\n",
      "Epoch 348/6000, Training Loss: 45.2846, Validation Loss: 45.2503\n",
      "Epoch 349/6000, Training Loss: 45.2646, Validation Loss: 45.2303\n",
      "Epoch 350/6000, Training Loss: 45.2446, Validation Loss: 45.2103\n",
      "Epoch 351/6000, Training Loss: 45.2246, Validation Loss: 45.1904\n",
      "Epoch 352/6000, Training Loss: 45.2046, Validation Loss: 45.1704\n",
      "Epoch 353/6000, Training Loss: 45.1846, Validation Loss: 45.1504\n",
      "Epoch 354/6000, Training Loss: 45.1646, Validation Loss: 45.1305\n",
      "Epoch 355/6000, Training Loss: 45.1446, Validation Loss: 45.1105\n",
      "Epoch 356/6000, Training Loss: 45.1246, Validation Loss: 45.0905\n",
      "Epoch 357/6000, Training Loss: 45.1046, Validation Loss: 45.0706\n",
      "Epoch 358/6000, Training Loss: 45.0846, Validation Loss: 45.0506\n",
      "Epoch 359/6000, Training Loss: 45.0646, Validation Loss: 45.0307\n",
      "Epoch 360/6000, Training Loss: 45.0445, Validation Loss: 45.0107\n",
      "Epoch 361/6000, Training Loss: 45.0245, Validation Loss: 44.9907\n",
      "Epoch 362/6000, Training Loss: 45.0045, Validation Loss: 44.9708\n",
      "Epoch 363/6000, Training Loss: 44.9845, Validation Loss: 44.9508\n",
      "Epoch 364/6000, Training Loss: 44.9645, Validation Loss: 44.9308\n",
      "Epoch 365/6000, Training Loss: 44.9445, Validation Loss: 44.9109\n",
      "Epoch 366/6000, Training Loss: 44.9245, Validation Loss: 44.8909\n",
      "Epoch 367/6000, Training Loss: 44.9045, Validation Loss: 44.8709\n",
      "Epoch 368/6000, Training Loss: 44.8845, Validation Loss: 44.8510\n",
      "Epoch 369/6000, Training Loss: 44.8645, Validation Loss: 44.8310\n",
      "Epoch 370/6000, Training Loss: 44.8445, Validation Loss: 44.8110\n",
      "Epoch 371/6000, Training Loss: 44.8244, Validation Loss: 44.7911\n",
      "Epoch 372/6000, Training Loss: 44.8044, Validation Loss: 44.7711\n",
      "Epoch 373/6000, Training Loss: 44.7844, Validation Loss: 44.7511\n",
      "Epoch 374/6000, Training Loss: 44.7644, Validation Loss: 44.7312\n",
      "Epoch 375/6000, Training Loss: 44.7444, Validation Loss: 44.7112\n",
      "Epoch 376/6000, Training Loss: 44.7244, Validation Loss: 44.6912\n",
      "Epoch 377/6000, Training Loss: 44.7044, Validation Loss: 44.6713\n",
      "Epoch 378/6000, Training Loss: 44.6844, Validation Loss: 44.6513\n",
      "Epoch 379/6000, Training Loss: 44.6644, Validation Loss: 44.6313\n",
      "Epoch 380/6000, Training Loss: 44.6444, Validation Loss: 44.6114\n",
      "Epoch 381/6000, Training Loss: 44.6244, Validation Loss: 44.5914\n",
      "Epoch 382/6000, Training Loss: 44.6044, Validation Loss: 44.5714\n",
      "Epoch 383/6000, Training Loss: 44.5843, Validation Loss: 44.5515\n",
      "Epoch 384/6000, Training Loss: 44.5643, Validation Loss: 44.5315\n",
      "Epoch 385/6000, Training Loss: 44.5443, Validation Loss: 44.5116\n",
      "Epoch 386/6000, Training Loss: 44.5243, Validation Loss: 44.4916\n",
      "Epoch 387/6000, Training Loss: 44.5043, Validation Loss: 44.4716\n",
      "Epoch 388/6000, Training Loss: 44.4843, Validation Loss: 44.4517\n",
      "Epoch 389/6000, Training Loss: 44.4643, Validation Loss: 44.4317\n",
      "Epoch 390/6000, Training Loss: 44.4443, Validation Loss: 44.4117\n",
      "Epoch 391/6000, Training Loss: 44.4243, Validation Loss: 44.3918\n",
      "Epoch 392/6000, Training Loss: 44.4043, Validation Loss: 44.3718\n",
      "Epoch 393/6000, Training Loss: 44.3843, Validation Loss: 44.3518\n",
      "Epoch 394/6000, Training Loss: 44.3643, Validation Loss: 44.3319\n",
      "Epoch 395/6000, Training Loss: 44.3442, Validation Loss: 44.3119\n",
      "Epoch 396/6000, Training Loss: 44.3242, Validation Loss: 44.2919\n",
      "Epoch 397/6000, Training Loss: 44.3042, Validation Loss: 44.2720\n",
      "Epoch 398/6000, Training Loss: 44.2842, Validation Loss: 44.2520\n",
      "Epoch 399/6000, Training Loss: 44.2642, Validation Loss: 44.2320\n",
      "Epoch 400/6000, Training Loss: 44.2442, Validation Loss: 44.2121\n",
      "Epoch 401/6000, Training Loss: 44.2242, Validation Loss: 44.1921\n",
      "Epoch 402/6000, Training Loss: 44.2042, Validation Loss: 44.1721\n",
      "Epoch 403/6000, Training Loss: 44.1842, Validation Loss: 44.1522\n",
      "Epoch 404/6000, Training Loss: 44.1642, Validation Loss: 44.1322\n",
      "Epoch 405/6000, Training Loss: 44.1441, Validation Loss: 44.1123\n",
      "Epoch 406/6000, Training Loss: 44.1241, Validation Loss: 44.0923\n",
      "Epoch 407/6000, Training Loss: 44.1041, Validation Loss: 44.0723\n",
      "Epoch 408/6000, Training Loss: 44.0841, Validation Loss: 44.0523\n",
      "Epoch 409/6000, Training Loss: 44.0641, Validation Loss: 44.0324\n",
      "Epoch 410/6000, Training Loss: 44.0441, Validation Loss: 44.0124\n",
      "Epoch 411/6000, Training Loss: 44.0241, Validation Loss: 43.9925\n",
      "Epoch 412/6000, Training Loss: 44.0041, Validation Loss: 43.9725\n",
      "Epoch 413/6000, Training Loss: 43.9841, Validation Loss: 43.9525\n",
      "Epoch 414/6000, Training Loss: 43.9641, Validation Loss: 43.9326\n",
      "Epoch 415/6000, Training Loss: 43.9441, Validation Loss: 43.9126\n",
      "Epoch 416/6000, Training Loss: 43.9241, Validation Loss: 43.8926\n",
      "Epoch 417/6000, Training Loss: 43.9040, Validation Loss: 43.8727\n",
      "Epoch 418/6000, Training Loss: 43.8840, Validation Loss: 43.8527\n",
      "Epoch 419/6000, Training Loss: 43.8640, Validation Loss: 43.8327\n",
      "Epoch 420/6000, Training Loss: 43.8440, Validation Loss: 43.8128\n",
      "Epoch 421/6000, Training Loss: 43.8240, Validation Loss: 43.7928\n",
      "Epoch 422/6000, Training Loss: 43.8040, Validation Loss: 43.7728\n",
      "Epoch 423/6000, Training Loss: 43.7840, Validation Loss: 43.7529\n",
      "Epoch 424/6000, Training Loss: 43.7640, Validation Loss: 43.7329\n",
      "Epoch 425/6000, Training Loss: 43.7440, Validation Loss: 43.7129\n",
      "Epoch 426/6000, Training Loss: 43.7240, Validation Loss: 43.6930\n",
      "Epoch 427/6000, Training Loss: 43.7040, Validation Loss: 43.6730\n",
      "Epoch 428/6000, Training Loss: 43.6840, Validation Loss: 43.6530\n",
      "Epoch 429/6000, Training Loss: 43.6639, Validation Loss: 43.6331\n",
      "Epoch 430/6000, Training Loss: 43.6439, Validation Loss: 43.6131\n",
      "Epoch 431/6000, Training Loss: 43.6239, Validation Loss: 43.5931\n",
      "Epoch 432/6000, Training Loss: 43.6039, Validation Loss: 43.5732\n",
      "Epoch 433/6000, Training Loss: 43.5839, Validation Loss: 43.5532\n",
      "Epoch 434/6000, Training Loss: 43.5639, Validation Loss: 43.5332\n",
      "Epoch 435/6000, Training Loss: 43.5439, Validation Loss: 43.5133\n",
      "Epoch 436/6000, Training Loss: 43.5239, Validation Loss: 43.4933\n",
      "Epoch 437/6000, Training Loss: 43.5039, Validation Loss: 43.4734\n",
      "Epoch 438/6000, Training Loss: 43.4839, Validation Loss: 43.4534\n",
      "Epoch 439/6000, Training Loss: 43.4639, Validation Loss: 43.4334\n",
      "Epoch 440/6000, Training Loss: 43.4438, Validation Loss: 43.4135\n",
      "Epoch 441/6000, Training Loss: 43.4238, Validation Loss: 43.3935\n",
      "Epoch 442/6000, Training Loss: 43.4038, Validation Loss: 43.3735\n",
      "Epoch 443/6000, Training Loss: 43.3838, Validation Loss: 43.3536\n",
      "Epoch 444/6000, Training Loss: 43.3638, Validation Loss: 43.3336\n",
      "Epoch 445/6000, Training Loss: 43.3438, Validation Loss: 43.3136\n",
      "Epoch 446/6000, Training Loss: 43.3238, Validation Loss: 43.2937\n",
      "Epoch 447/6000, Training Loss: 43.3038, Validation Loss: 43.2737\n",
      "Epoch 448/6000, Training Loss: 43.2838, Validation Loss: 43.2537\n",
      "Epoch 449/6000, Training Loss: 43.2638, Validation Loss: 43.2338\n",
      "Epoch 450/6000, Training Loss: 43.2438, Validation Loss: 43.2138\n",
      "Epoch 451/6000, Training Loss: 43.2237, Validation Loss: 43.1938\n",
      "Epoch 452/6000, Training Loss: 43.2037, Validation Loss: 43.1739\n",
      "Epoch 453/6000, Training Loss: 43.1837, Validation Loss: 43.1539\n",
      "Epoch 454/6000, Training Loss: 43.1637, Validation Loss: 43.1339\n",
      "Epoch 455/6000, Training Loss: 43.1437, Validation Loss: 43.1140\n",
      "Epoch 456/6000, Training Loss: 43.1237, Validation Loss: 43.0940\n",
      "Epoch 457/6000, Training Loss: 43.1037, Validation Loss: 43.0740\n",
      "Epoch 458/6000, Training Loss: 43.0837, Validation Loss: 43.0541\n",
      "Epoch 459/6000, Training Loss: 43.0637, Validation Loss: 43.0341\n",
      "Epoch 460/6000, Training Loss: 43.0437, Validation Loss: 43.0141\n",
      "Epoch 461/6000, Training Loss: 43.0237, Validation Loss: 42.9942\n",
      "Epoch 462/6000, Training Loss: 43.0037, Validation Loss: 42.9742\n",
      "Epoch 463/6000, Training Loss: 42.9836, Validation Loss: 42.9543\n",
      "Epoch 464/6000, Training Loss: 42.9636, Validation Loss: 42.9343\n",
      "Epoch 465/6000, Training Loss: 42.9436, Validation Loss: 42.9143\n",
      "Epoch 466/6000, Training Loss: 42.9236, Validation Loss: 42.8944\n",
      "Epoch 467/6000, Training Loss: 42.9036, Validation Loss: 42.8744\n",
      "Epoch 468/6000, Training Loss: 42.8836, Validation Loss: 42.8544\n",
      "Epoch 469/6000, Training Loss: 42.8636, Validation Loss: 42.8345\n",
      "Epoch 470/6000, Training Loss: 42.8436, Validation Loss: 42.8145\n",
      "Epoch 471/6000, Training Loss: 42.8236, Validation Loss: 42.7945\n",
      "Epoch 472/6000, Training Loss: 42.8036, Validation Loss: 42.7746\n",
      "Epoch 473/6000, Training Loss: 42.7836, Validation Loss: 42.7546\n",
      "Epoch 474/6000, Training Loss: 42.7635, Validation Loss: 42.7346\n",
      "Epoch 475/6000, Training Loss: 42.7435, Validation Loss: 42.7147\n",
      "Epoch 476/6000, Training Loss: 42.7235, Validation Loss: 42.6947\n",
      "Epoch 477/6000, Training Loss: 42.7035, Validation Loss: 42.6747\n",
      "Epoch 478/6000, Training Loss: 42.6835, Validation Loss: 42.6548\n",
      "Epoch 479/6000, Training Loss: 42.6635, Validation Loss: 42.6348\n",
      "Epoch 480/6000, Training Loss: 42.6435, Validation Loss: 42.6148\n",
      "Epoch 481/6000, Training Loss: 42.6235, Validation Loss: 42.5949\n",
      "Epoch 482/6000, Training Loss: 42.6035, Validation Loss: 42.5749\n",
      "Epoch 483/6000, Training Loss: 42.5835, Validation Loss: 42.5549\n",
      "Epoch 484/6000, Training Loss: 42.5635, Validation Loss: 42.5350\n",
      "Epoch 485/6000, Training Loss: 42.5435, Validation Loss: 42.5150\n",
      "Epoch 486/6000, Training Loss: 42.5234, Validation Loss: 42.4950\n",
      "Epoch 487/6000, Training Loss: 42.5034, Validation Loss: 42.4751\n",
      "Epoch 488/6000, Training Loss: 42.4834, Validation Loss: 42.4551\n",
      "Epoch 489/6000, Training Loss: 42.4634, Validation Loss: 42.4352\n",
      "Epoch 490/6000, Training Loss: 42.4434, Validation Loss: 42.4152\n",
      "Epoch 491/6000, Training Loss: 42.4234, Validation Loss: 42.3952\n",
      "Epoch 492/6000, Training Loss: 42.4034, Validation Loss: 42.3753\n",
      "Epoch 493/6000, Training Loss: 42.3834, Validation Loss: 42.3553\n",
      "Epoch 494/6000, Training Loss: 42.3634, Validation Loss: 42.3353\n",
      "Epoch 495/6000, Training Loss: 42.3434, Validation Loss: 42.3154\n",
      "Epoch 496/6000, Training Loss: 42.3234, Validation Loss: 42.2954\n",
      "Epoch 497/6000, Training Loss: 42.3033, Validation Loss: 42.2754\n",
      "Epoch 498/6000, Training Loss: 42.2833, Validation Loss: 42.2555\n",
      "Epoch 499/6000, Training Loss: 42.2633, Validation Loss: 42.2355\n",
      "Epoch 500/6000, Training Loss: 42.2433, Validation Loss: 42.2155\n",
      "Epoch 501/6000, Training Loss: 42.2233, Validation Loss: 42.1956\n",
      "Epoch 502/6000, Training Loss: 42.2033, Validation Loss: 42.1756\n",
      "Epoch 503/6000, Training Loss: 42.1833, Validation Loss: 42.1556\n",
      "Epoch 504/6000, Training Loss: 42.1633, Validation Loss: 42.1357\n",
      "Epoch 505/6000, Training Loss: 42.1433, Validation Loss: 42.1157\n",
      "Epoch 506/6000, Training Loss: 42.1233, Validation Loss: 42.0957\n",
      "Epoch 507/6000, Training Loss: 42.1033, Validation Loss: 42.0758\n",
      "Epoch 508/6000, Training Loss: 42.0833, Validation Loss: 42.0558\n",
      "Epoch 509/6000, Training Loss: 42.0632, Validation Loss: 42.0358\n",
      "Epoch 510/6000, Training Loss: 42.0432, Validation Loss: 42.0159\n",
      "Epoch 511/6000, Training Loss: 42.0232, Validation Loss: 41.9959\n",
      "Epoch 512/6000, Training Loss: 42.0032, Validation Loss: 41.9759\n",
      "Epoch 513/6000, Training Loss: 41.9832, Validation Loss: 41.9560\n",
      "Epoch 514/6000, Training Loss: 41.9632, Validation Loss: 41.9360\n",
      "Epoch 515/6000, Training Loss: 41.9432, Validation Loss: 41.9160\n",
      "Epoch 516/6000, Training Loss: 41.9232, Validation Loss: 41.8961\n",
      "Epoch 517/6000, Training Loss: 41.9032, Validation Loss: 41.8761\n",
      "Epoch 518/6000, Training Loss: 41.8832, Validation Loss: 41.8562\n",
      "Epoch 519/6000, Training Loss: 41.8632, Validation Loss: 41.8362\n",
      "Epoch 520/6000, Training Loss: 41.8431, Validation Loss: 41.8162\n",
      "Epoch 521/6000, Training Loss: 41.8231, Validation Loss: 41.7963\n",
      "Epoch 522/6000, Training Loss: 41.8031, Validation Loss: 41.7763\n",
      "Epoch 523/6000, Training Loss: 41.7831, Validation Loss: 41.7563\n",
      "Epoch 524/6000, Training Loss: 41.7631, Validation Loss: 41.7364\n",
      "Epoch 525/6000, Training Loss: 41.7431, Validation Loss: 41.7164\n",
      "Epoch 526/6000, Training Loss: 41.7231, Validation Loss: 41.6964\n",
      "Epoch 527/6000, Training Loss: 41.7031, Validation Loss: 41.6765\n",
      "Epoch 528/6000, Training Loss: 41.6831, Validation Loss: 41.6565\n",
      "Epoch 529/6000, Training Loss: 41.6631, Validation Loss: 41.6365\n",
      "Epoch 530/6000, Training Loss: 41.6431, Validation Loss: 41.6166\n",
      "Epoch 531/6000, Training Loss: 41.6231, Validation Loss: 41.5966\n",
      "Epoch 532/6000, Training Loss: 41.6030, Validation Loss: 41.5766\n",
      "Epoch 533/6000, Training Loss: 41.5830, Validation Loss: 41.5567\n",
      "Epoch 534/6000, Training Loss: 41.5630, Validation Loss: 41.5367\n",
      "Epoch 535/6000, Training Loss: 41.5430, Validation Loss: 41.5167\n",
      "Epoch 536/6000, Training Loss: 41.5230, Validation Loss: 41.4968\n",
      "Epoch 537/6000, Training Loss: 41.5030, Validation Loss: 41.4768\n",
      "Epoch 538/6000, Training Loss: 41.4830, Validation Loss: 41.4568\n",
      "Epoch 539/6000, Training Loss: 41.4630, Validation Loss: 41.4369\n",
      "Epoch 540/6000, Training Loss: 41.4430, Validation Loss: 41.4169\n",
      "Epoch 541/6000, Training Loss: 41.4230, Validation Loss: 41.3969\n",
      "Epoch 542/6000, Training Loss: 41.4030, Validation Loss: 41.3770\n",
      "Epoch 543/6000, Training Loss: 41.3829, Validation Loss: 41.3570\n",
      "Epoch 544/6000, Training Loss: 41.3629, Validation Loss: 41.3371\n",
      "Epoch 545/6000, Training Loss: 41.3429, Validation Loss: 41.3171\n",
      "Epoch 546/6000, Training Loss: 41.3229, Validation Loss: 41.2971\n",
      "Epoch 547/6000, Training Loss: 41.3029, Validation Loss: 41.2772\n",
      "Epoch 548/6000, Training Loss: 41.2829, Validation Loss: 41.2572\n",
      "Epoch 549/6000, Training Loss: 41.2629, Validation Loss: 41.2372\n",
      "Epoch 550/6000, Training Loss: 41.2429, Validation Loss: 41.2173\n",
      "Epoch 551/6000, Training Loss: 41.2229, Validation Loss: 41.1973\n",
      "Epoch 552/6000, Training Loss: 41.2029, Validation Loss: 41.1773\n",
      "Epoch 553/6000, Training Loss: 41.1829, Validation Loss: 41.1574\n",
      "Epoch 554/6000, Training Loss: 41.1629, Validation Loss: 41.1374\n",
      "Epoch 555/6000, Training Loss: 41.1428, Validation Loss: 41.1174\n",
      "Epoch 556/6000, Training Loss: 41.1228, Validation Loss: 41.0975\n",
      "Epoch 557/6000, Training Loss: 41.1028, Validation Loss: 41.0775\n",
      "Epoch 558/6000, Training Loss: 41.0828, Validation Loss: 41.0575\n",
      "Epoch 559/6000, Training Loss: 41.0628, Validation Loss: 41.0376\n",
      "Epoch 560/6000, Training Loss: 41.0428, Validation Loss: 41.0176\n",
      "Epoch 561/6000, Training Loss: 41.0228, Validation Loss: 40.9976\n",
      "Epoch 562/6000, Training Loss: 41.0028, Validation Loss: 40.9777\n",
      "Epoch 563/6000, Training Loss: 40.9828, Validation Loss: 40.9577\n",
      "Epoch 564/6000, Training Loss: 40.9628, Validation Loss: 40.9377\n",
      "Epoch 565/6000, Training Loss: 40.9428, Validation Loss: 40.9178\n",
      "Epoch 566/6000, Training Loss: 40.9227, Validation Loss: 40.8978\n",
      "Epoch 567/6000, Training Loss: 40.9027, Validation Loss: 40.8778\n",
      "Epoch 568/6000, Training Loss: 40.8827, Validation Loss: 40.8579\n",
      "Epoch 569/6000, Training Loss: 40.8627, Validation Loss: 40.8379\n",
      "Epoch 570/6000, Training Loss: 40.8427, Validation Loss: 40.8179\n",
      "Epoch 571/6000, Training Loss: 40.8227, Validation Loss: 40.7980\n",
      "Epoch 572/6000, Training Loss: 40.8027, Validation Loss: 40.7780\n",
      "Epoch 573/6000, Training Loss: 40.7827, Validation Loss: 40.7581\n",
      "Epoch 574/6000, Training Loss: 40.7627, Validation Loss: 40.7381\n",
      "Epoch 575/6000, Training Loss: 40.7427, Validation Loss: 40.7181\n",
      "Epoch 576/6000, Training Loss: 40.7227, Validation Loss: 40.6982\n",
      "Epoch 577/6000, Training Loss: 40.7027, Validation Loss: 40.6782\n",
      "Epoch 578/6000, Training Loss: 40.6826, Validation Loss: 40.6582\n",
      "Epoch 579/6000, Training Loss: 40.6626, Validation Loss: 40.6383\n",
      "Epoch 580/6000, Training Loss: 40.6426, Validation Loss: 40.6183\n",
      "Epoch 581/6000, Training Loss: 40.6226, Validation Loss: 40.5983\n",
      "Epoch 582/6000, Training Loss: 40.6026, Validation Loss: 40.5784\n",
      "Epoch 583/6000, Training Loss: 40.5826, Validation Loss: 40.5584\n",
      "Epoch 584/6000, Training Loss: 40.5626, Validation Loss: 40.5384\n",
      "Epoch 585/6000, Training Loss: 40.5426, Validation Loss: 40.5185\n",
      "Epoch 586/6000, Training Loss: 40.5226, Validation Loss: 40.4985\n",
      "Epoch 587/6000, Training Loss: 40.5026, Validation Loss: 40.4785\n",
      "Epoch 588/6000, Training Loss: 40.4826, Validation Loss: 40.4586\n",
      "Epoch 589/6000, Training Loss: 40.4625, Validation Loss: 40.4386\n",
      "Epoch 590/6000, Training Loss: 40.4425, Validation Loss: 40.4186\n",
      "Epoch 591/6000, Training Loss: 40.4225, Validation Loss: 40.3987\n",
      "Epoch 592/6000, Training Loss: 40.4025, Validation Loss: 40.3787\n",
      "Epoch 593/6000, Training Loss: 40.3825, Validation Loss: 40.3587\n",
      "Epoch 594/6000, Training Loss: 40.3625, Validation Loss: 40.3388\n",
      "Epoch 595/6000, Training Loss: 40.3425, Validation Loss: 40.3188\n",
      "Epoch 596/6000, Training Loss: 40.3225, Validation Loss: 40.2989\n",
      "Epoch 597/6000, Training Loss: 40.3025, Validation Loss: 40.2789\n",
      "Epoch 598/6000, Training Loss: 40.2825, Validation Loss: 40.2589\n",
      "Epoch 599/6000, Training Loss: 40.2625, Validation Loss: 40.2390\n",
      "Epoch 600/6000, Training Loss: 40.2425, Validation Loss: 40.2190\n",
      "Epoch 601/6000, Training Loss: 40.2224, Validation Loss: 40.1990\n",
      "Epoch 602/6000, Training Loss: 40.2024, Validation Loss: 40.1791\n",
      "Epoch 603/6000, Training Loss: 40.1824, Validation Loss: 40.1591\n",
      "Epoch 604/6000, Training Loss: 40.1624, Validation Loss: 40.1391\n",
      "Epoch 605/6000, Training Loss: 40.1424, Validation Loss: 40.1192\n",
      "Epoch 606/6000, Training Loss: 40.1224, Validation Loss: 40.0992\n",
      "Epoch 607/6000, Training Loss: 40.1024, Validation Loss: 40.0792\n",
      "Epoch 608/6000, Training Loss: 40.0824, Validation Loss: 40.0593\n",
      "Epoch 609/6000, Training Loss: 40.0624, Validation Loss: 40.0393\n",
      "Epoch 610/6000, Training Loss: 40.0424, Validation Loss: 40.0193\n",
      "Epoch 611/6000, Training Loss: 40.0224, Validation Loss: 39.9994\n",
      "Epoch 612/6000, Training Loss: 40.0023, Validation Loss: 39.9794\n",
      "Epoch 613/6000, Training Loss: 39.9823, Validation Loss: 39.9594\n",
      "Epoch 614/6000, Training Loss: 39.9623, Validation Loss: 39.9395\n",
      "Epoch 615/6000, Training Loss: 39.9423, Validation Loss: 39.9195\n",
      "Epoch 616/6000, Training Loss: 39.9223, Validation Loss: 39.8995\n",
      "Epoch 617/6000, Training Loss: 39.9023, Validation Loss: 39.8796\n",
      "Epoch 618/6000, Training Loss: 39.8823, Validation Loss: 39.8596\n",
      "Epoch 619/6000, Training Loss: 39.8623, Validation Loss: 39.8396\n",
      "Epoch 620/6000, Training Loss: 39.8423, Validation Loss: 39.8197\n",
      "Epoch 621/6000, Training Loss: 39.8223, Validation Loss: 39.7997\n",
      "Epoch 622/6000, Training Loss: 39.8023, Validation Loss: 39.7798\n",
      "Epoch 623/6000, Training Loss: 39.7823, Validation Loss: 39.7598\n",
      "Epoch 624/6000, Training Loss: 39.7622, Validation Loss: 39.7398\n",
      "Epoch 625/6000, Training Loss: 39.7422, Validation Loss: 39.7199\n",
      "Epoch 626/6000, Training Loss: 39.7222, Validation Loss: 39.6999\n",
      "Epoch 627/6000, Training Loss: 39.7022, Validation Loss: 39.6799\n",
      "Epoch 628/6000, Training Loss: 39.6822, Validation Loss: 39.6600\n",
      "Epoch 629/6000, Training Loss: 39.6622, Validation Loss: 39.6400\n",
      "Epoch 630/6000, Training Loss: 39.6422, Validation Loss: 39.6200\n",
      "Epoch 631/6000, Training Loss: 39.6222, Validation Loss: 39.6001\n",
      "Epoch 632/6000, Training Loss: 39.6022, Validation Loss: 39.5801\n",
      "Epoch 633/6000, Training Loss: 39.5822, Validation Loss: 39.5601\n",
      "Epoch 634/6000, Training Loss: 39.5622, Validation Loss: 39.5402\n",
      "Epoch 635/6000, Training Loss: 39.5422, Validation Loss: 39.5202\n",
      "Epoch 636/6000, Training Loss: 39.5221, Validation Loss: 39.5002\n",
      "Epoch 637/6000, Training Loss: 39.5021, Validation Loss: 39.4803\n",
      "Epoch 638/6000, Training Loss: 39.4821, Validation Loss: 39.4603\n",
      "Epoch 639/6000, Training Loss: 39.4621, Validation Loss: 39.4403\n",
      "Epoch 640/6000, Training Loss: 39.4421, Validation Loss: 39.4204\n",
      "Epoch 641/6000, Training Loss: 39.4221, Validation Loss: 39.4004\n",
      "Epoch 642/6000, Training Loss: 39.4021, Validation Loss: 39.3804\n",
      "Epoch 643/6000, Training Loss: 39.3821, Validation Loss: 39.3605\n",
      "Epoch 644/6000, Training Loss: 39.3621, Validation Loss: 39.3405\n",
      "Epoch 645/6000, Training Loss: 39.3421, Validation Loss: 39.3205\n",
      "Epoch 646/6000, Training Loss: 39.3221, Validation Loss: 39.3006\n",
      "Epoch 647/6000, Training Loss: 39.3020, Validation Loss: 39.2806\n",
      "Epoch 648/6000, Training Loss: 39.2820, Validation Loss: 39.2606\n",
      "Epoch 649/6000, Training Loss: 39.2620, Validation Loss: 39.2407\n",
      "Epoch 650/6000, Training Loss: 39.2420, Validation Loss: 39.2207\n",
      "Epoch 651/6000, Training Loss: 39.2220, Validation Loss: 39.2008\n",
      "Epoch 652/6000, Training Loss: 39.2020, Validation Loss: 39.1808\n",
      "Epoch 653/6000, Training Loss: 39.1820, Validation Loss: 39.1608\n",
      "Epoch 654/6000, Training Loss: 39.1620, Validation Loss: 39.1409\n",
      "Epoch 655/6000, Training Loss: 39.1420, Validation Loss: 39.1209\n",
      "Epoch 656/6000, Training Loss: 39.1220, Validation Loss: 39.1009\n",
      "Epoch 657/6000, Training Loss: 39.1020, Validation Loss: 39.0810\n",
      "Epoch 658/6000, Training Loss: 39.0820, Validation Loss: 39.0610\n",
      "Epoch 659/6000, Training Loss: 39.0620, Validation Loss: 39.0410\n",
      "Epoch 660/6000, Training Loss: 39.0420, Validation Loss: 39.0211\n",
      "Epoch 661/6000, Training Loss: 39.0219, Validation Loss: 39.0011\n",
      "Epoch 662/6000, Training Loss: 39.0019, Validation Loss: 38.9811\n",
      "Epoch 663/6000, Training Loss: 38.9819, Validation Loss: 38.9612\n",
      "Epoch 664/6000, Training Loss: 38.9619, Validation Loss: 38.9412\n",
      "Epoch 665/6000, Training Loss: 38.9419, Validation Loss: 38.9212\n",
      "Epoch 666/6000, Training Loss: 38.9219, Validation Loss: 38.9013\n",
      "Epoch 667/6000, Training Loss: 38.9019, Validation Loss: 38.8813\n",
      "Epoch 668/6000, Training Loss: 38.8819, Validation Loss: 38.8613\n",
      "Epoch 669/6000, Training Loss: 38.8619, Validation Loss: 38.8414\n",
      "Epoch 670/6000, Training Loss: 38.8419, Validation Loss: 38.8214\n",
      "Epoch 671/6000, Training Loss: 38.8219, Validation Loss: 38.8014\n",
      "Epoch 672/6000, Training Loss: 38.8019, Validation Loss: 38.7815\n",
      "Epoch 673/6000, Training Loss: 38.7818, Validation Loss: 38.7615\n",
      "Epoch 674/6000, Training Loss: 38.7618, Validation Loss: 38.7415\n",
      "Epoch 675/6000, Training Loss: 38.7418, Validation Loss: 38.7216\n",
      "Epoch 676/6000, Training Loss: 38.7218, Validation Loss: 38.7016\n",
      "Epoch 677/6000, Training Loss: 38.7018, Validation Loss: 38.6816\n",
      "Epoch 678/6000, Training Loss: 38.6818, Validation Loss: 38.6617\n",
      "Epoch 679/6000, Training Loss: 38.6618, Validation Loss: 38.6417\n",
      "Epoch 680/6000, Training Loss: 38.6418, Validation Loss: 38.6217\n",
      "Epoch 681/6000, Training Loss: 38.6218, Validation Loss: 38.6018\n",
      "Epoch 682/6000, Training Loss: 38.6018, Validation Loss: 38.5818\n",
      "Epoch 683/6000, Training Loss: 38.5818, Validation Loss: 38.5618\n",
      "Epoch 684/6000, Training Loss: 38.5618, Validation Loss: 38.5419\n",
      "Epoch 685/6000, Training Loss: 38.5418, Validation Loss: 38.5219\n",
      "Epoch 686/6000, Training Loss: 38.5218, Validation Loss: 38.5019\n",
      "Epoch 687/6000, Training Loss: 38.5018, Validation Loss: 38.4819\n",
      "Epoch 688/6000, Training Loss: 38.4817, Validation Loss: 38.4620\n",
      "Epoch 689/6000, Training Loss: 38.4617, Validation Loss: 38.4420\n",
      "Epoch 690/6000, Training Loss: 38.4417, Validation Loss: 38.4220\n",
      "Epoch 691/6000, Training Loss: 38.4217, Validation Loss: 38.4021\n",
      "Epoch 692/6000, Training Loss: 38.4017, Validation Loss: 38.3821\n",
      "Epoch 693/6000, Training Loss: 38.3817, Validation Loss: 38.3621\n",
      "Epoch 694/6000, Training Loss: 38.3617, Validation Loss: 38.3422\n",
      "Epoch 695/6000, Training Loss: 38.3417, Validation Loss: 38.3222\n",
      "Epoch 696/6000, Training Loss: 38.3217, Validation Loss: 38.3022\n",
      "Epoch 697/6000, Training Loss: 38.3017, Validation Loss: 38.2823\n",
      "Epoch 698/6000, Training Loss: 38.2817, Validation Loss: 38.2623\n",
      "Epoch 699/6000, Training Loss: 38.2617, Validation Loss: 38.2423\n",
      "Epoch 700/6000, Training Loss: 38.2417, Validation Loss: 38.2224\n",
      "Epoch 701/6000, Training Loss: 38.2217, Validation Loss: 38.2024\n",
      "Epoch 702/6000, Training Loss: 38.2017, Validation Loss: 38.1824\n",
      "Epoch 703/6000, Training Loss: 38.1816, Validation Loss: 38.1625\n",
      "Epoch 704/6000, Training Loss: 38.1616, Validation Loss: 38.1425\n",
      "Epoch 705/6000, Training Loss: 38.1416, Validation Loss: 38.1225\n",
      "Epoch 706/6000, Training Loss: 38.1216, Validation Loss: 38.1025\n",
      "Epoch 707/6000, Training Loss: 38.1016, Validation Loss: 38.0826\n",
      "Epoch 708/6000, Training Loss: 38.0816, Validation Loss: 38.0626\n",
      "Epoch 709/6000, Training Loss: 38.0616, Validation Loss: 38.0426\n",
      "Epoch 710/6000, Training Loss: 38.0416, Validation Loss: 38.0227\n",
      "Epoch 711/6000, Training Loss: 38.0216, Validation Loss: 38.0027\n",
      "Epoch 712/6000, Training Loss: 38.0016, Validation Loss: 37.9827\n",
      "Epoch 713/6000, Training Loss: 37.9816, Validation Loss: 37.9628\n",
      "Epoch 714/6000, Training Loss: 37.9616, Validation Loss: 37.9428\n",
      "Epoch 715/6000, Training Loss: 37.9416, Validation Loss: 37.9228\n",
      "Epoch 716/6000, Training Loss: 37.9216, Validation Loss: 37.9029\n",
      "Epoch 717/6000, Training Loss: 37.9016, Validation Loss: 37.8829\n",
      "Epoch 718/6000, Training Loss: 37.8816, Validation Loss: 37.8629\n",
      "Epoch 719/6000, Training Loss: 37.8616, Validation Loss: 37.8429\n",
      "Epoch 720/6000, Training Loss: 37.8416, Validation Loss: 37.8230\n",
      "Epoch 721/6000, Training Loss: 37.8215, Validation Loss: 37.8030\n",
      "Epoch 722/6000, Training Loss: 37.8015, Validation Loss: 37.7830\n",
      "Epoch 723/6000, Training Loss: 37.7815, Validation Loss: 37.7631\n",
      "Epoch 724/6000, Training Loss: 37.7615, Validation Loss: 37.7431\n",
      "Epoch 725/6000, Training Loss: 37.7415, Validation Loss: 37.7231\n",
      "Epoch 726/6000, Training Loss: 37.7215, Validation Loss: 37.7032\n",
      "Epoch 727/6000, Training Loss: 37.7015, Validation Loss: 37.6832\n",
      "Epoch 728/6000, Training Loss: 37.6815, Validation Loss: 37.6632\n",
      "Epoch 729/6000, Training Loss: 37.6615, Validation Loss: 37.6433\n",
      "Epoch 730/6000, Training Loss: 37.6415, Validation Loss: 37.6233\n",
      "Epoch 731/6000, Training Loss: 37.6215, Validation Loss: 37.6033\n",
      "Epoch 732/6000, Training Loss: 37.6015, Validation Loss: 37.5833\n",
      "Epoch 733/6000, Training Loss: 37.5815, Validation Loss: 37.5634\n",
      "Epoch 734/6000, Training Loss: 37.5615, Validation Loss: 37.5434\n",
      "Epoch 735/6000, Training Loss: 37.5415, Validation Loss: 37.5234\n",
      "Epoch 736/6000, Training Loss: 37.5215, Validation Loss: 37.5035\n",
      "Epoch 737/6000, Training Loss: 37.5015, Validation Loss: 37.4835\n",
      "Epoch 738/6000, Training Loss: 37.4815, Validation Loss: 37.4635\n",
      "Epoch 739/6000, Training Loss: 37.4615, Validation Loss: 37.4435\n",
      "Epoch 740/6000, Training Loss: 37.4415, Validation Loss: 37.4236\n",
      "Epoch 741/6000, Training Loss: 37.4215, Validation Loss: 37.4036\n",
      "Epoch 742/6000, Training Loss: 37.4015, Validation Loss: 37.3836\n",
      "Epoch 743/6000, Training Loss: 37.3815, Validation Loss: 37.3637\n",
      "Epoch 744/6000, Training Loss: 37.3614, Validation Loss: 37.3437\n",
      "Epoch 745/6000, Training Loss: 37.3414, Validation Loss: 37.3237\n",
      "Epoch 746/6000, Training Loss: 37.3214, Validation Loss: 37.3037\n",
      "Epoch 747/6000, Training Loss: 37.3014, Validation Loss: 37.2838\n",
      "Epoch 748/6000, Training Loss: 37.2814, Validation Loss: 37.2638\n",
      "Epoch 749/6000, Training Loss: 37.2614, Validation Loss: 37.2438\n",
      "Epoch 750/6000, Training Loss: 37.2414, Validation Loss: 37.2239\n",
      "Epoch 751/6000, Training Loss: 37.2214, Validation Loss: 37.2039\n",
      "Epoch 752/6000, Training Loss: 37.2014, Validation Loss: 37.1839\n",
      "Epoch 753/6000, Training Loss: 37.1814, Validation Loss: 37.1639\n",
      "Epoch 754/6000, Training Loss: 37.1614, Validation Loss: 37.1440\n",
      "Epoch 755/6000, Training Loss: 37.1414, Validation Loss: 37.1240\n",
      "Epoch 756/6000, Training Loss: 37.1214, Validation Loss: 37.1040\n",
      "Epoch 757/6000, Training Loss: 37.1014, Validation Loss: 37.0841\n",
      "Epoch 758/6000, Training Loss: 37.0814, Validation Loss: 37.0641\n",
      "Epoch 759/6000, Training Loss: 37.0614, Validation Loss: 37.0441\n",
      "Epoch 760/6000, Training Loss: 37.0414, Validation Loss: 37.0241\n",
      "Epoch 761/6000, Training Loss: 37.0214, Validation Loss: 37.0042\n",
      "Epoch 762/6000, Training Loss: 37.0014, Validation Loss: 36.9842\n",
      "Epoch 763/6000, Training Loss: 36.9814, Validation Loss: 36.9642\n",
      "Epoch 764/6000, Training Loss: 36.9614, Validation Loss: 36.9443\n",
      "Epoch 765/6000, Training Loss: 36.9414, Validation Loss: 36.9243\n",
      "Epoch 766/6000, Training Loss: 36.9214, Validation Loss: 36.9043\n",
      "Epoch 767/6000, Training Loss: 36.9014, Validation Loss: 36.8843\n",
      "Epoch 768/6000, Training Loss: 36.8814, Validation Loss: 36.8644\n",
      "Epoch 769/6000, Training Loss: 36.8614, Validation Loss: 36.8444\n",
      "Epoch 770/6000, Training Loss: 36.8414, Validation Loss: 36.8244\n",
      "Epoch 771/6000, Training Loss: 36.8214, Validation Loss: 36.8045\n",
      "Epoch 772/6000, Training Loss: 36.8014, Validation Loss: 36.7845\n",
      "Epoch 773/6000, Training Loss: 36.7814, Validation Loss: 36.7645\n",
      "Epoch 774/6000, Training Loss: 36.7614, Validation Loss: 36.7445\n",
      "Epoch 775/6000, Training Loss: 36.7414, Validation Loss: 36.7246\n",
      "Epoch 776/6000, Training Loss: 36.7214, Validation Loss: 36.7046\n",
      "Epoch 777/6000, Training Loss: 36.7014, Validation Loss: 36.6846\n",
      "Epoch 778/6000, Training Loss: 36.6814, Validation Loss: 36.6646\n",
      "Epoch 779/6000, Training Loss: 36.6614, Validation Loss: 36.6447\n",
      "Epoch 780/6000, Training Loss: 36.6414, Validation Loss: 36.6247\n",
      "Epoch 781/6000, Training Loss: 36.6214, Validation Loss: 36.6047\n",
      "Epoch 782/6000, Training Loss: 36.6014, Validation Loss: 36.5847\n",
      "Epoch 783/6000, Training Loss: 36.5814, Validation Loss: 36.5648\n",
      "Epoch 784/6000, Training Loss: 36.5614, Validation Loss: 36.5448\n",
      "Epoch 785/6000, Training Loss: 36.5414, Validation Loss: 36.5248\n",
      "Epoch 786/6000, Training Loss: 36.5214, Validation Loss: 36.5049\n",
      "Epoch 787/6000, Training Loss: 36.5014, Validation Loss: 36.4849\n",
      "Epoch 788/6000, Training Loss: 36.4814, Validation Loss: 36.4649\n",
      "Epoch 789/6000, Training Loss: 36.4614, Validation Loss: 36.4449\n",
      "Epoch 790/6000, Training Loss: 36.4414, Validation Loss: 36.4250\n",
      "Epoch 791/6000, Training Loss: 36.4214, Validation Loss: 36.4050\n",
      "Epoch 792/6000, Training Loss: 36.4014, Validation Loss: 36.3850\n",
      "Epoch 793/6000, Training Loss: 36.3814, Validation Loss: 36.3650\n",
      "Epoch 794/6000, Training Loss: 36.3614, Validation Loss: 36.3451\n",
      "Epoch 795/6000, Training Loss: 36.3414, Validation Loss: 36.3251\n",
      "Epoch 796/6000, Training Loss: 36.3214, Validation Loss: 36.3051\n",
      "Epoch 797/6000, Training Loss: 36.3014, Validation Loss: 36.2851\n",
      "Epoch 798/6000, Training Loss: 36.2814, Validation Loss: 36.2652\n",
      "Epoch 799/6000, Training Loss: 36.2614, Validation Loss: 36.2452\n",
      "Epoch 800/6000, Training Loss: 36.2414, Validation Loss: 36.2252\n",
      "Epoch 801/6000, Training Loss: 36.2214, Validation Loss: 36.2052\n",
      "Epoch 802/6000, Training Loss: 36.2014, Validation Loss: 36.1852\n",
      "Epoch 803/6000, Training Loss: 36.1814, Validation Loss: 36.1653\n",
      "Epoch 804/6000, Training Loss: 36.1614, Validation Loss: 36.1453\n",
      "Epoch 805/6000, Training Loss: 36.1414, Validation Loss: 36.1253\n",
      "Epoch 806/6000, Training Loss: 36.1214, Validation Loss: 36.1053\n",
      "Epoch 807/6000, Training Loss: 36.1014, Validation Loss: 36.0854\n",
      "Epoch 808/6000, Training Loss: 36.0814, Validation Loss: 36.0654\n",
      "Epoch 809/6000, Training Loss: 36.0614, Validation Loss: 36.0454\n",
      "Epoch 810/6000, Training Loss: 36.0414, Validation Loss: 36.0254\n",
      "Epoch 811/6000, Training Loss: 36.0214, Validation Loss: 36.0055\n",
      "Epoch 812/6000, Training Loss: 36.0014, Validation Loss: 35.9855\n",
      "Epoch 813/6000, Training Loss: 35.9814, Validation Loss: 35.9655\n",
      "Epoch 814/6000, Training Loss: 35.9614, Validation Loss: 35.9455\n",
      "Epoch 815/6000, Training Loss: 35.9414, Validation Loss: 35.9255\n",
      "Epoch 816/6000, Training Loss: 35.9214, Validation Loss: 35.9056\n",
      "Epoch 817/6000, Training Loss: 35.9015, Validation Loss: 35.8856\n",
      "Epoch 818/6000, Training Loss: 35.8815, Validation Loss: 35.8656\n",
      "Epoch 819/6000, Training Loss: 35.8615, Validation Loss: 35.8456\n",
      "Epoch 820/6000, Training Loss: 35.8415, Validation Loss: 35.8256\n",
      "Epoch 821/6000, Training Loss: 35.8215, Validation Loss: 35.8056\n",
      "Epoch 822/6000, Training Loss: 35.8015, Validation Loss: 35.7857\n",
      "Epoch 823/6000, Training Loss: 35.7815, Validation Loss: 35.7657\n",
      "Epoch 824/6000, Training Loss: 35.7615, Validation Loss: 35.7457\n",
      "Epoch 825/6000, Training Loss: 35.7415, Validation Loss: 35.7257\n",
      "Epoch 826/6000, Training Loss: 35.7216, Validation Loss: 35.7057\n",
      "Epoch 827/6000, Training Loss: 35.7016, Validation Loss: 35.6857\n",
      "Epoch 828/6000, Training Loss: 35.6816, Validation Loss: 35.6657\n",
      "Epoch 829/6000, Training Loss: 35.6616, Validation Loss: 35.6458\n",
      "Epoch 830/6000, Training Loss: 35.6416, Validation Loss: 35.6258\n",
      "Epoch 831/6000, Training Loss: 35.6216, Validation Loss: 35.6058\n",
      "Epoch 832/6000, Training Loss: 35.6017, Validation Loss: 35.5858\n",
      "Epoch 833/6000, Training Loss: 35.5817, Validation Loss: 35.5658\n",
      "Epoch 834/6000, Training Loss: 35.5617, Validation Loss: 35.5458\n",
      "Epoch 835/6000, Training Loss: 35.5417, Validation Loss: 35.5258\n",
      "Epoch 836/6000, Training Loss: 35.5217, Validation Loss: 35.5058\n",
      "Epoch 837/6000, Training Loss: 35.5018, Validation Loss: 35.4858\n",
      "Epoch 838/6000, Training Loss: 35.4818, Validation Loss: 35.4658\n",
      "Epoch 839/6000, Training Loss: 35.4618, Validation Loss: 35.4458\n",
      "Epoch 840/6000, Training Loss: 35.4418, Validation Loss: 35.4258\n",
      "Epoch 841/6000, Training Loss: 35.4218, Validation Loss: 35.4058\n",
      "Epoch 842/6000, Training Loss: 35.4019, Validation Loss: 35.3858\n",
      "Epoch 843/6000, Training Loss: 35.3819, Validation Loss: 35.3658\n",
      "Epoch 844/6000, Training Loss: 35.3619, Validation Loss: 35.3458\n",
      "Epoch 845/6000, Training Loss: 35.3419, Validation Loss: 35.3258\n",
      "Epoch 846/6000, Training Loss: 35.3220, Validation Loss: 35.3058\n",
      "Epoch 847/6000, Training Loss: 35.3020, Validation Loss: 35.2858\n",
      "Epoch 848/6000, Training Loss: 35.2820, Validation Loss: 35.2658\n",
      "Epoch 849/6000, Training Loss: 35.2620, Validation Loss: 35.2458\n",
      "Epoch 850/6000, Training Loss: 35.2420, Validation Loss: 35.2258\n",
      "Epoch 851/6000, Training Loss: 35.2221, Validation Loss: 35.2058\n",
      "Epoch 852/6000, Training Loss: 35.2021, Validation Loss: 35.1858\n",
      "Epoch 853/6000, Training Loss: 35.1821, Validation Loss: 35.1658\n",
      "Epoch 854/6000, Training Loss: 35.1621, Validation Loss: 35.1458\n",
      "Epoch 855/6000, Training Loss: 35.1421, Validation Loss: 35.1258\n",
      "Epoch 856/6000, Training Loss: 35.1222, Validation Loss: 35.1057\n",
      "Epoch 857/6000, Training Loss: 35.1022, Validation Loss: 35.0857\n",
      "Epoch 858/6000, Training Loss: 35.0822, Validation Loss: 35.0657\n",
      "Epoch 859/6000, Training Loss: 35.0622, Validation Loss: 35.0457\n",
      "Epoch 860/6000, Training Loss: 35.0423, Validation Loss: 35.0257\n",
      "Epoch 861/6000, Training Loss: 35.0223, Validation Loss: 35.0057\n",
      "Epoch 862/6000, Training Loss: 35.0023, Validation Loss: 34.9857\n",
      "Epoch 863/6000, Training Loss: 34.9823, Validation Loss: 34.9657\n",
      "Epoch 864/6000, Training Loss: 34.9623, Validation Loss: 34.9456\n",
      "Epoch 865/6000, Training Loss: 34.9424, Validation Loss: 34.9256\n",
      "Epoch 866/6000, Training Loss: 34.9224, Validation Loss: 34.9056\n",
      "Epoch 867/6000, Training Loss: 34.9024, Validation Loss: 34.8856\n",
      "Epoch 868/6000, Training Loss: 34.8824, Validation Loss: 34.8656\n",
      "Epoch 869/6000, Training Loss: 34.8624, Validation Loss: 34.8456\n",
      "Epoch 870/6000, Training Loss: 34.8425, Validation Loss: 34.8256\n",
      "Epoch 871/6000, Training Loss: 34.8225, Validation Loss: 34.8056\n",
      "Epoch 872/6000, Training Loss: 34.8025, Validation Loss: 34.7855\n",
      "Epoch 873/6000, Training Loss: 34.7825, Validation Loss: 34.7655\n",
      "Epoch 874/6000, Training Loss: 34.7626, Validation Loss: 34.7455\n",
      "Epoch 875/6000, Training Loss: 34.7426, Validation Loss: 34.7255\n",
      "Epoch 876/6000, Training Loss: 34.7226, Validation Loss: 34.7055\n",
      "Epoch 877/6000, Training Loss: 34.7026, Validation Loss: 34.6855\n",
      "Epoch 878/6000, Training Loss: 34.6826, Validation Loss: 34.6655\n",
      "Epoch 879/6000, Training Loss: 34.6627, Validation Loss: 34.6455\n",
      "Epoch 880/6000, Training Loss: 34.6427, Validation Loss: 34.6254\n",
      "Epoch 881/6000, Training Loss: 34.6227, Validation Loss: 34.6054\n",
      "Epoch 882/6000, Training Loss: 34.6027, Validation Loss: 34.5854\n",
      "Epoch 883/6000, Training Loss: 34.5827, Validation Loss: 34.5654\n",
      "Epoch 884/6000, Training Loss: 34.5628, Validation Loss: 34.5454\n",
      "Epoch 885/6000, Training Loss: 34.5428, Validation Loss: 34.5254\n",
      "Epoch 886/6000, Training Loss: 34.5228, Validation Loss: 34.5054\n",
      "Epoch 887/6000, Training Loss: 34.5028, Validation Loss: 34.4853\n",
      "Epoch 888/6000, Training Loss: 34.4828, Validation Loss: 34.4653\n",
      "Epoch 889/6000, Training Loss: 34.4629, Validation Loss: 34.4453\n",
      "Epoch 890/6000, Training Loss: 34.4429, Validation Loss: 34.4253\n",
      "Epoch 891/6000, Training Loss: 34.4229, Validation Loss: 34.4053\n",
      "Epoch 892/6000, Training Loss: 34.4029, Validation Loss: 34.3853\n",
      "Epoch 893/6000, Training Loss: 34.3830, Validation Loss: 34.3653\n",
      "Epoch 894/6000, Training Loss: 34.3630, Validation Loss: 34.3453\n",
      "Epoch 895/6000, Training Loss: 34.3430, Validation Loss: 34.3252\n",
      "Epoch 896/6000, Training Loss: 34.3230, Validation Loss: 34.3052\n",
      "Epoch 897/6000, Training Loss: 34.3030, Validation Loss: 34.2852\n",
      "Epoch 898/6000, Training Loss: 34.2831, Validation Loss: 34.2652\n",
      "Epoch 899/6000, Training Loss: 34.2631, Validation Loss: 34.2452\n",
      "Epoch 900/6000, Training Loss: 34.2431, Validation Loss: 34.2252\n",
      "Epoch 901/6000, Training Loss: 34.2231, Validation Loss: 34.2052\n",
      "Epoch 902/6000, Training Loss: 34.2031, Validation Loss: 34.1851\n",
      "Epoch 903/6000, Training Loss: 34.1832, Validation Loss: 34.1651\n",
      "Epoch 904/6000, Training Loss: 34.1632, Validation Loss: 34.1451\n",
      "Epoch 905/6000, Training Loss: 34.1432, Validation Loss: 34.1251\n",
      "Epoch 906/6000, Training Loss: 34.1232, Validation Loss: 34.1051\n",
      "Epoch 907/6000, Training Loss: 34.1033, Validation Loss: 34.0851\n",
      "Epoch 908/6000, Training Loss: 34.0833, Validation Loss: 34.0651\n",
      "Epoch 909/6000, Training Loss: 34.0633, Validation Loss: 34.0450\n",
      "Epoch 910/6000, Training Loss: 34.0433, Validation Loss: 34.0250\n",
      "Epoch 911/6000, Training Loss: 34.0233, Validation Loss: 34.0050\n",
      "Epoch 912/6000, Training Loss: 34.0034, Validation Loss: 33.9850\n",
      "Epoch 913/6000, Training Loss: 33.9834, Validation Loss: 33.9650\n",
      "Epoch 914/6000, Training Loss: 33.9634, Validation Loss: 33.9450\n",
      "Epoch 915/6000, Training Loss: 33.9434, Validation Loss: 33.9250\n",
      "Epoch 916/6000, Training Loss: 33.9234, Validation Loss: 33.9049\n",
      "Epoch 917/6000, Training Loss: 33.9035, Validation Loss: 33.8849\n",
      "Epoch 918/6000, Training Loss: 33.8835, Validation Loss: 33.8649\n",
      "Epoch 919/6000, Training Loss: 33.8635, Validation Loss: 33.8449\n",
      "Epoch 920/6000, Training Loss: 33.8435, Validation Loss: 33.8249\n",
      "Epoch 921/6000, Training Loss: 33.8235, Validation Loss: 33.8049\n",
      "Epoch 922/6000, Training Loss: 33.8036, Validation Loss: 33.7849\n",
      "Epoch 923/6000, Training Loss: 33.7836, Validation Loss: 33.7649\n",
      "Epoch 924/6000, Training Loss: 33.7636, Validation Loss: 33.7448\n",
      "Epoch 925/6000, Training Loss: 33.7436, Validation Loss: 33.7248\n",
      "Epoch 926/6000, Training Loss: 33.7237, Validation Loss: 33.7048\n",
      "Epoch 927/6000, Training Loss: 33.7037, Validation Loss: 33.6848\n",
      "Epoch 928/6000, Training Loss: 33.6837, Validation Loss: 33.6648\n",
      "Epoch 929/6000, Training Loss: 33.6637, Validation Loss: 33.6448\n",
      "Epoch 930/6000, Training Loss: 33.6437, Validation Loss: 33.6248\n",
      "Epoch 931/6000, Training Loss: 33.6238, Validation Loss: 33.6047\n",
      "Epoch 932/6000, Training Loss: 33.6038, Validation Loss: 33.5847\n",
      "Epoch 933/6000, Training Loss: 33.5838, Validation Loss: 33.5647\n",
      "Epoch 934/6000, Training Loss: 33.5638, Validation Loss: 33.5447\n",
      "Epoch 935/6000, Training Loss: 33.5438, Validation Loss: 33.5247\n",
      "Epoch 936/6000, Training Loss: 33.5239, Validation Loss: 33.5047\n",
      "Epoch 937/6000, Training Loss: 33.5039, Validation Loss: 33.4847\n",
      "Epoch 938/6000, Training Loss: 33.4839, Validation Loss: 33.4646\n",
      "Epoch 939/6000, Training Loss: 33.4639, Validation Loss: 33.4446\n",
      "Epoch 940/6000, Training Loss: 33.4439, Validation Loss: 33.4246\n",
      "Epoch 941/6000, Training Loss: 33.4240, Validation Loss: 33.4046\n",
      "Epoch 942/6000, Training Loss: 33.4040, Validation Loss: 33.3846\n",
      "Epoch 943/6000, Training Loss: 33.3840, Validation Loss: 33.3646\n",
      "Epoch 944/6000, Training Loss: 33.3640, Validation Loss: 33.3446\n",
      "Epoch 945/6000, Training Loss: 33.3441, Validation Loss: 33.3245\n",
      "Epoch 946/6000, Training Loss: 33.3241, Validation Loss: 33.3045\n",
      "Epoch 947/6000, Training Loss: 33.3041, Validation Loss: 33.2845\n",
      "Epoch 948/6000, Training Loss: 33.2841, Validation Loss: 33.2645\n",
      "Epoch 949/6000, Training Loss: 33.2641, Validation Loss: 33.2445\n",
      "Epoch 950/6000, Training Loss: 33.2442, Validation Loss: 33.2245\n",
      "Epoch 951/6000, Training Loss: 33.2242, Validation Loss: 33.2045\n",
      "Epoch 952/6000, Training Loss: 33.2042, Validation Loss: 33.1844\n",
      "Epoch 953/6000, Training Loss: 33.1842, Validation Loss: 33.1644\n",
      "Epoch 954/6000, Training Loss: 33.1642, Validation Loss: 33.1444\n",
      "Epoch 955/6000, Training Loss: 33.1443, Validation Loss: 33.1244\n",
      "Epoch 956/6000, Training Loss: 33.1243, Validation Loss: 33.1044\n",
      "Epoch 957/6000, Training Loss: 33.1043, Validation Loss: 33.0844\n",
      "Epoch 958/6000, Training Loss: 33.0843, Validation Loss: 33.0643\n",
      "Epoch 959/6000, Training Loss: 33.0643, Validation Loss: 33.0443\n",
      "Epoch 960/6000, Training Loss: 33.0444, Validation Loss: 33.0243\n",
      "Epoch 961/6000, Training Loss: 33.0244, Validation Loss: 33.0043\n",
      "Epoch 962/6000, Training Loss: 33.0044, Validation Loss: 32.9843\n",
      "Epoch 963/6000, Training Loss: 32.9844, Validation Loss: 32.9643\n",
      "Epoch 964/6000, Training Loss: 32.9644, Validation Loss: 32.9443\n",
      "Epoch 965/6000, Training Loss: 32.9445, Validation Loss: 32.9243\n",
      "Epoch 966/6000, Training Loss: 32.9245, Validation Loss: 32.9042\n",
      "Epoch 967/6000, Training Loss: 32.9045, Validation Loss: 32.8842\n",
      "Epoch 968/6000, Training Loss: 32.8845, Validation Loss: 32.8642\n",
      "Epoch 969/6000, Training Loss: 32.8645, Validation Loss: 32.8442\n",
      "Epoch 970/6000, Training Loss: 32.8446, Validation Loss: 32.8242\n",
      "Epoch 971/6000, Training Loss: 32.8246, Validation Loss: 32.8042\n",
      "Epoch 972/6000, Training Loss: 32.8046, Validation Loss: 32.7841\n",
      "Epoch 973/6000, Training Loss: 32.7846, Validation Loss: 32.7641\n",
      "Epoch 974/6000, Training Loss: 32.7646, Validation Loss: 32.7441\n",
      "Epoch 975/6000, Training Loss: 32.7447, Validation Loss: 32.7241\n",
      "Epoch 976/6000, Training Loss: 32.7247, Validation Loss: 32.7041\n",
      "Epoch 977/6000, Training Loss: 32.7047, Validation Loss: 32.6841\n",
      "Epoch 978/6000, Training Loss: 32.6847, Validation Loss: 32.6641\n",
      "Epoch 979/6000, Training Loss: 32.6647, Validation Loss: 32.6440\n",
      "Epoch 980/6000, Training Loss: 32.6448, Validation Loss: 32.6240\n",
      "Epoch 981/6000, Training Loss: 32.6248, Validation Loss: 32.6040\n",
      "Epoch 982/6000, Training Loss: 32.6048, Validation Loss: 32.5840\n",
      "Epoch 983/6000, Training Loss: 32.5848, Validation Loss: 32.5640\n",
      "Epoch 984/6000, Training Loss: 32.5648, Validation Loss: 32.5440\n",
      "Epoch 985/6000, Training Loss: 32.5449, Validation Loss: 32.5240\n",
      "Epoch 986/6000, Training Loss: 32.5249, Validation Loss: 32.5039\n",
      "Epoch 987/6000, Training Loss: 32.5049, Validation Loss: 32.4839\n",
      "Epoch 988/6000, Training Loss: 32.4849, Validation Loss: 32.4639\n",
      "Epoch 989/6000, Training Loss: 32.4650, Validation Loss: 32.4439\n",
      "Epoch 990/6000, Training Loss: 32.4450, Validation Loss: 32.4239\n",
      "Epoch 991/6000, Training Loss: 32.4250, Validation Loss: 32.4039\n",
      "Epoch 992/6000, Training Loss: 32.4050, Validation Loss: 32.3838\n",
      "Epoch 993/6000, Training Loss: 32.3850, Validation Loss: 32.3638\n",
      "Epoch 994/6000, Training Loss: 32.3651, Validation Loss: 32.3438\n",
      "Epoch 995/6000, Training Loss: 32.3451, Validation Loss: 32.3238\n",
      "Epoch 996/6000, Training Loss: 32.3251, Validation Loss: 32.3038\n",
      "Epoch 997/6000, Training Loss: 32.3051, Validation Loss: 32.2838\n",
      "Epoch 998/6000, Training Loss: 32.2851, Validation Loss: 32.2637\n",
      "Epoch 999/6000, Training Loss: 32.2652, Validation Loss: 32.2437\n",
      "Epoch 1000/6000, Training Loss: 32.2452, Validation Loss: 32.2237\n",
      "Epoch 1001/6000, Training Loss: 32.2252, Validation Loss: 32.2037\n",
      "Epoch 1002/6000, Training Loss: 32.2052, Validation Loss: 32.1837\n",
      "Epoch 1003/6000, Training Loss: 32.1852, Validation Loss: 32.1637\n",
      "Epoch 1004/6000, Training Loss: 32.1653, Validation Loss: 32.1437\n",
      "Epoch 1005/6000, Training Loss: 32.1453, Validation Loss: 32.1236\n",
      "Epoch 1006/6000, Training Loss: 32.1253, Validation Loss: 32.1036\n",
      "Epoch 1007/6000, Training Loss: 32.1053, Validation Loss: 32.0836\n",
      "Epoch 1008/6000, Training Loss: 32.0853, Validation Loss: 32.0636\n",
      "Epoch 1009/6000, Training Loss: 32.0654, Validation Loss: 32.0436\n",
      "Epoch 1010/6000, Training Loss: 32.0454, Validation Loss: 32.0236\n",
      "Epoch 1011/6000, Training Loss: 32.0254, Validation Loss: 32.0035\n",
      "Epoch 1012/6000, Training Loss: 32.0054, Validation Loss: 31.9835\n",
      "Epoch 1013/6000, Training Loss: 31.9854, Validation Loss: 31.9635\n",
      "Epoch 1014/6000, Training Loss: 31.9655, Validation Loss: 31.9435\n",
      "Epoch 1015/6000, Training Loss: 31.9455, Validation Loss: 31.9235\n",
      "Epoch 1016/6000, Training Loss: 31.9255, Validation Loss: 31.9035\n",
      "Epoch 1017/6000, Training Loss: 31.9055, Validation Loss: 31.8835\n",
      "Epoch 1018/6000, Training Loss: 31.8855, Validation Loss: 31.8634\n",
      "Epoch 1019/6000, Training Loss: 31.8656, Validation Loss: 31.8434\n",
      "Epoch 1020/6000, Training Loss: 31.8456, Validation Loss: 31.8234\n",
      "Epoch 1021/6000, Training Loss: 31.8256, Validation Loss: 31.8034\n",
      "Epoch 1022/6000, Training Loss: 31.8056, Validation Loss: 31.7834\n",
      "Epoch 1023/6000, Training Loss: 31.7856, Validation Loss: 31.7634\n",
      "Epoch 1024/6000, Training Loss: 31.7657, Validation Loss: 31.7433\n",
      "Epoch 1025/6000, Training Loss: 31.7457, Validation Loss: 31.7233\n",
      "Epoch 1026/6000, Training Loss: 31.7257, Validation Loss: 31.7033\n",
      "Epoch 1027/6000, Training Loss: 31.7057, Validation Loss: 31.6833\n",
      "Epoch 1028/6000, Training Loss: 31.6857, Validation Loss: 31.6633\n",
      "Epoch 1029/6000, Training Loss: 31.6658, Validation Loss: 31.6433\n",
      "Epoch 1030/6000, Training Loss: 31.6458, Validation Loss: 31.6232\n",
      "Epoch 1031/6000, Training Loss: 31.6258, Validation Loss: 31.6032\n",
      "Epoch 1032/6000, Training Loss: 31.6058, Validation Loss: 31.5832\n",
      "Epoch 1033/6000, Training Loss: 31.5858, Validation Loss: 31.5632\n",
      "Epoch 1034/6000, Training Loss: 31.5659, Validation Loss: 31.5432\n",
      "Epoch 1035/6000, Training Loss: 31.5459, Validation Loss: 31.5231\n",
      "Epoch 1036/6000, Training Loss: 31.5259, Validation Loss: 31.5031\n",
      "Epoch 1037/6000, Training Loss: 31.5059, Validation Loss: 31.4831\n",
      "Epoch 1038/6000, Training Loss: 31.4859, Validation Loss: 31.4631\n",
      "Epoch 1039/6000, Training Loss: 31.4660, Validation Loss: 31.4431\n",
      "Epoch 1040/6000, Training Loss: 31.4460, Validation Loss: 31.4231\n",
      "Epoch 1041/6000, Training Loss: 31.4260, Validation Loss: 31.4030\n",
      "Epoch 1042/6000, Training Loss: 31.4060, Validation Loss: 31.3830\n",
      "Epoch 1043/6000, Training Loss: 31.3860, Validation Loss: 31.3630\n",
      "Epoch 1044/6000, Training Loss: 31.3661, Validation Loss: 31.3430\n",
      "Epoch 1045/6000, Training Loss: 31.3461, Validation Loss: 31.3230\n",
      "Epoch 1046/6000, Training Loss: 31.3261, Validation Loss: 31.3030\n",
      "Epoch 1047/6000, Training Loss: 31.3061, Validation Loss: 31.2829\n",
      "Epoch 1048/6000, Training Loss: 31.2861, Validation Loss: 31.2629\n",
      "Epoch 1049/6000, Training Loss: 31.2662, Validation Loss: 31.2429\n",
      "Epoch 1050/6000, Training Loss: 31.2462, Validation Loss: 31.2229\n",
      "Epoch 1051/6000, Training Loss: 31.2262, Validation Loss: 31.2029\n",
      "Epoch 1052/6000, Training Loss: 31.2062, Validation Loss: 31.1829\n",
      "Epoch 1053/6000, Training Loss: 31.1862, Validation Loss: 31.1628\n",
      "Epoch 1054/6000, Training Loss: 31.1663, Validation Loss: 31.1428\n",
      "Epoch 1055/6000, Training Loss: 31.1463, Validation Loss: 31.1228\n",
      "Epoch 1056/6000, Training Loss: 31.1263, Validation Loss: 31.1028\n",
      "Epoch 1057/6000, Training Loss: 31.1063, Validation Loss: 31.0828\n",
      "Epoch 1058/6000, Training Loss: 31.0863, Validation Loss: 31.0628\n",
      "Epoch 1059/6000, Training Loss: 31.0664, Validation Loss: 31.0427\n",
      "Epoch 1060/6000, Training Loss: 31.0464, Validation Loss: 31.0227\n",
      "Epoch 1061/6000, Training Loss: 31.0264, Validation Loss: 31.0027\n",
      "Epoch 1062/6000, Training Loss: 31.0064, Validation Loss: 30.9827\n",
      "Epoch 1063/6000, Training Loss: 30.9864, Validation Loss: 30.9627\n",
      "Epoch 1064/6000, Training Loss: 30.9665, Validation Loss: 30.9427\n",
      "Epoch 1065/6000, Training Loss: 30.9465, Validation Loss: 30.9226\n",
      "Epoch 1066/6000, Training Loss: 30.9265, Validation Loss: 30.9026\n",
      "Epoch 1067/6000, Training Loss: 30.9065, Validation Loss: 30.8826\n",
      "Epoch 1068/6000, Training Loss: 30.8865, Validation Loss: 30.8626\n",
      "Epoch 1069/6000, Training Loss: 30.8666, Validation Loss: 30.8426\n",
      "Epoch 1070/6000, Training Loss: 30.8466, Validation Loss: 30.8225\n",
      "Epoch 1071/6000, Training Loss: 30.8266, Validation Loss: 30.8025\n",
      "Epoch 1072/6000, Training Loss: 30.8066, Validation Loss: 30.7825\n",
      "Epoch 1073/6000, Training Loss: 30.7866, Validation Loss: 30.7625\n",
      "Epoch 1074/6000, Training Loss: 30.7667, Validation Loss: 30.7425\n",
      "Epoch 1075/6000, Training Loss: 30.7467, Validation Loss: 30.7225\n",
      "Epoch 1076/6000, Training Loss: 30.7267, Validation Loss: 30.7024\n",
      "Epoch 1077/6000, Training Loss: 30.7067, Validation Loss: 30.6824\n",
      "Epoch 1078/6000, Training Loss: 30.6867, Validation Loss: 30.6624\n",
      "Epoch 1079/6000, Training Loss: 30.6668, Validation Loss: 30.6424\n",
      "Epoch 1080/6000, Training Loss: 30.6468, Validation Loss: 30.6224\n",
      "Epoch 1081/6000, Training Loss: 30.6268, Validation Loss: 30.6024\n",
      "Epoch 1082/6000, Training Loss: 30.6068, Validation Loss: 30.5823\n",
      "Epoch 1083/6000, Training Loss: 30.5868, Validation Loss: 30.5623\n",
      "Epoch 1084/6000, Training Loss: 30.5669, Validation Loss: 30.5423\n",
      "Epoch 1085/6000, Training Loss: 30.5469, Validation Loss: 30.5223\n",
      "Epoch 1086/6000, Training Loss: 30.5269, Validation Loss: 30.5023\n",
      "Epoch 1087/6000, Training Loss: 30.5069, Validation Loss: 30.4823\n",
      "Epoch 1088/6000, Training Loss: 30.4869, Validation Loss: 30.4622\n",
      "Epoch 1089/6000, Training Loss: 30.4670, Validation Loss: 30.4422\n",
      "Epoch 1090/6000, Training Loss: 30.4470, Validation Loss: 30.4222\n",
      "Epoch 1091/6000, Training Loss: 30.4270, Validation Loss: 30.4022\n",
      "Epoch 1092/6000, Training Loss: 30.4070, Validation Loss: 30.3822\n",
      "Epoch 1093/6000, Training Loss: 30.3870, Validation Loss: 30.3621\n",
      "Epoch 1094/6000, Training Loss: 30.3671, Validation Loss: 30.3421\n",
      "Epoch 1095/6000, Training Loss: 30.3471, Validation Loss: 30.3221\n",
      "Epoch 1096/6000, Training Loss: 30.3271, Validation Loss: 30.3021\n",
      "Epoch 1097/6000, Training Loss: 30.3071, Validation Loss: 30.2821\n",
      "Epoch 1098/6000, Training Loss: 30.2871, Validation Loss: 30.2621\n",
      "Epoch 1099/6000, Training Loss: 30.2672, Validation Loss: 30.2420\n",
      "Epoch 1100/6000, Training Loss: 30.2472, Validation Loss: 30.2220\n",
      "Epoch 1101/6000, Training Loss: 30.2272, Validation Loss: 30.2020\n",
      "Epoch 1102/6000, Training Loss: 30.2072, Validation Loss: 30.1820\n",
      "Epoch 1103/6000, Training Loss: 30.1872, Validation Loss: 30.1620\n",
      "Epoch 1104/6000, Training Loss: 30.1673, Validation Loss: 30.1419\n",
      "Epoch 1105/6000, Training Loss: 30.1473, Validation Loss: 30.1219\n",
      "Epoch 1106/6000, Training Loss: 30.1273, Validation Loss: 30.1019\n",
      "Epoch 1107/6000, Training Loss: 30.1073, Validation Loss: 30.0819\n",
      "Epoch 1108/6000, Training Loss: 30.0873, Validation Loss: 30.0619\n",
      "Epoch 1109/6000, Training Loss: 30.0674, Validation Loss: 30.0419\n",
      "Epoch 1110/6000, Training Loss: 30.0474, Validation Loss: 30.0218\n",
      "Epoch 1111/6000, Training Loss: 30.0274, Validation Loss: 30.0018\n",
      "Epoch 1112/6000, Training Loss: 30.0074, Validation Loss: 29.9818\n",
      "Epoch 1113/6000, Training Loss: 29.9874, Validation Loss: 29.9618\n",
      "Epoch 1114/6000, Training Loss: 29.9675, Validation Loss: 29.9418\n",
      "Epoch 1115/6000, Training Loss: 29.9475, Validation Loss: 29.9217\n",
      "Epoch 1116/6000, Training Loss: 29.9275, Validation Loss: 29.9017\n",
      "Epoch 1117/6000, Training Loss: 29.9075, Validation Loss: 29.8817\n",
      "Epoch 1118/6000, Training Loss: 29.8875, Validation Loss: 29.8617\n",
      "Epoch 1119/6000, Training Loss: 29.8676, Validation Loss: 29.8417\n",
      "Epoch 1120/6000, Training Loss: 29.8476, Validation Loss: 29.8217\n",
      "Epoch 1121/6000, Training Loss: 29.8276, Validation Loss: 29.8016\n",
      "Epoch 1122/6000, Training Loss: 29.8076, Validation Loss: 29.7816\n",
      "Epoch 1123/6000, Training Loss: 29.7876, Validation Loss: 29.7616\n",
      "Epoch 1124/6000, Training Loss: 29.7677, Validation Loss: 29.7416\n",
      "Epoch 1125/6000, Training Loss: 29.7477, Validation Loss: 29.7216\n",
      "Epoch 1126/6000, Training Loss: 29.7277, Validation Loss: 29.7015\n",
      "Epoch 1127/6000, Training Loss: 29.7077, Validation Loss: 29.6815\n",
      "Epoch 1128/6000, Training Loss: 29.6877, Validation Loss: 29.6615\n",
      "Epoch 1129/6000, Training Loss: 29.6678, Validation Loss: 29.6415\n",
      "Epoch 1130/6000, Training Loss: 29.6478, Validation Loss: 29.6215\n",
      "Epoch 1131/6000, Training Loss: 29.6278, Validation Loss: 29.6014\n",
      "Epoch 1132/6000, Training Loss: 29.6078, Validation Loss: 29.5814\n",
      "Epoch 1133/6000, Training Loss: 29.5878, Validation Loss: 29.5614\n",
      "Epoch 1134/6000, Training Loss: 29.5679, Validation Loss: 29.5414\n",
      "Epoch 1135/6000, Training Loss: 29.5479, Validation Loss: 29.5214\n",
      "Epoch 1136/6000, Training Loss: 29.5279, Validation Loss: 29.5014\n",
      "Epoch 1137/6000, Training Loss: 29.5079, Validation Loss: 29.4813\n",
      "Epoch 1138/6000, Training Loss: 29.4880, Validation Loss: 29.4613\n",
      "Epoch 1139/6000, Training Loss: 29.4680, Validation Loss: 29.4413\n",
      "Epoch 1140/6000, Training Loss: 29.4480, Validation Loss: 29.4213\n",
      "Epoch 1141/6000, Training Loss: 29.4280, Validation Loss: 29.4013\n",
      "Epoch 1142/6000, Training Loss: 29.4080, Validation Loss: 29.3812\n",
      "Epoch 1143/6000, Training Loss: 29.3880, Validation Loss: 29.3612\n",
      "Epoch 1144/6000, Training Loss: 29.3681, Validation Loss: 29.3412\n",
      "Epoch 1145/6000, Training Loss: 29.3481, Validation Loss: 29.3212\n",
      "Epoch 1146/6000, Training Loss: 29.3281, Validation Loss: 29.3012\n",
      "Epoch 1147/6000, Training Loss: 29.3081, Validation Loss: 29.2811\n",
      "Epoch 1148/6000, Training Loss: 29.2881, Validation Loss: 29.2611\n",
      "Epoch 1149/6000, Training Loss: 29.2682, Validation Loss: 29.2411\n",
      "Epoch 1150/6000, Training Loss: 29.2482, Validation Loss: 29.2211\n",
      "Epoch 1151/6000, Training Loss: 29.2282, Validation Loss: 29.2011\n",
      "Epoch 1152/6000, Training Loss: 29.2082, Validation Loss: 29.1810\n",
      "Epoch 1153/6000, Training Loss: 29.1882, Validation Loss: 29.1610\n",
      "Epoch 1154/6000, Training Loss: 29.1683, Validation Loss: 29.1410\n",
      "Epoch 1155/6000, Training Loss: 29.1483, Validation Loss: 29.1210\n",
      "Epoch 1156/6000, Training Loss: 29.1283, Validation Loss: 29.1010\n",
      "Epoch 1157/6000, Training Loss: 29.1083, Validation Loss: 29.0809\n",
      "Epoch 1158/6000, Training Loss: 29.0884, Validation Loss: 29.0609\n",
      "Epoch 1159/6000, Training Loss: 29.0684, Validation Loss: 29.0409\n",
      "Epoch 1160/6000, Training Loss: 29.0484, Validation Loss: 29.0209\n",
      "Epoch 1161/6000, Training Loss: 29.0284, Validation Loss: 29.0009\n",
      "Epoch 1162/6000, Training Loss: 29.0084, Validation Loss: 28.9809\n",
      "Epoch 1163/6000, Training Loss: 28.9885, Validation Loss: 28.9608\n",
      "Epoch 1164/6000, Training Loss: 28.9685, Validation Loss: 28.9408\n",
      "Epoch 1165/6000, Training Loss: 28.9485, Validation Loss: 28.9208\n",
      "Epoch 1166/6000, Training Loss: 28.9285, Validation Loss: 28.9008\n",
      "Epoch 1167/6000, Training Loss: 28.9085, Validation Loss: 28.8808\n",
      "Epoch 1168/6000, Training Loss: 28.8886, Validation Loss: 28.8607\n",
      "Epoch 1169/6000, Training Loss: 28.8686, Validation Loss: 28.8407\n",
      "Epoch 1170/6000, Training Loss: 28.8486, Validation Loss: 28.8207\n",
      "Epoch 1171/6000, Training Loss: 28.8286, Validation Loss: 28.8007\n",
      "Epoch 1172/6000, Training Loss: 28.8086, Validation Loss: 28.7807\n",
      "Epoch 1173/6000, Training Loss: 28.7887, Validation Loss: 28.7606\n",
      "Epoch 1174/6000, Training Loss: 28.7687, Validation Loss: 28.7406\n",
      "Epoch 1175/6000, Training Loss: 28.7487, Validation Loss: 28.7206\n",
      "Epoch 1176/6000, Training Loss: 28.7287, Validation Loss: 28.7006\n",
      "Epoch 1177/6000, Training Loss: 28.7087, Validation Loss: 28.6806\n",
      "Epoch 1178/6000, Training Loss: 28.6888, Validation Loss: 28.6605\n",
      "Epoch 1179/6000, Training Loss: 28.6688, Validation Loss: 28.6405\n",
      "Epoch 1180/6000, Training Loss: 28.6488, Validation Loss: 28.6205\n",
      "Epoch 1181/6000, Training Loss: 28.6288, Validation Loss: 28.6005\n",
      "Epoch 1182/6000, Training Loss: 28.6088, Validation Loss: 28.5805\n",
      "Epoch 1183/6000, Training Loss: 28.5889, Validation Loss: 28.5604\n",
      "Epoch 1184/6000, Training Loss: 28.5689, Validation Loss: 28.5404\n",
      "Epoch 1185/6000, Training Loss: 28.5489, Validation Loss: 28.5204\n",
      "Epoch 1186/6000, Training Loss: 28.5289, Validation Loss: 28.5004\n",
      "Epoch 1187/6000, Training Loss: 28.5089, Validation Loss: 28.4804\n",
      "Epoch 1188/6000, Training Loss: 28.4890, Validation Loss: 28.4604\n",
      "Epoch 1189/6000, Training Loss: 28.4690, Validation Loss: 28.4403\n",
      "Epoch 1190/6000, Training Loss: 28.4490, Validation Loss: 28.4203\n",
      "Epoch 1191/6000, Training Loss: 28.4290, Validation Loss: 28.4003\n",
      "Epoch 1192/6000, Training Loss: 28.4090, Validation Loss: 28.3803\n",
      "Epoch 1193/6000, Training Loss: 28.3891, Validation Loss: 28.3603\n",
      "Epoch 1194/6000, Training Loss: 28.3691, Validation Loss: 28.3402\n",
      "Epoch 1195/6000, Training Loss: 28.3491, Validation Loss: 28.3202\n",
      "Epoch 1196/6000, Training Loss: 28.3291, Validation Loss: 28.3002\n",
      "Epoch 1197/6000, Training Loss: 28.3091, Validation Loss: 28.2802\n",
      "Epoch 1198/6000, Training Loss: 28.2892, Validation Loss: 28.2601\n",
      "Epoch 1199/6000, Training Loss: 28.2692, Validation Loss: 28.2401\n",
      "Epoch 1200/6000, Training Loss: 28.2492, Validation Loss: 28.2201\n",
      "Epoch 1201/6000, Training Loss: 28.2292, Validation Loss: 28.2001\n",
      "Epoch 1202/6000, Training Loss: 28.2092, Validation Loss: 28.1801\n",
      "Epoch 1203/6000, Training Loss: 28.1893, Validation Loss: 28.1600\n",
      "Epoch 1204/6000, Training Loss: 28.1693, Validation Loss: 28.1400\n",
      "Epoch 1205/6000, Training Loss: 28.1493, Validation Loss: 28.1200\n",
      "Epoch 1206/6000, Training Loss: 28.1293, Validation Loss: 28.1000\n",
      "Epoch 1207/6000, Training Loss: 28.1093, Validation Loss: 28.0800\n",
      "Epoch 1208/6000, Training Loss: 28.0894, Validation Loss: 28.0599\n",
      "Epoch 1209/6000, Training Loss: 28.0694, Validation Loss: 28.0399\n",
      "Epoch 1210/6000, Training Loss: 28.0494, Validation Loss: 28.0199\n",
      "Epoch 1211/6000, Training Loss: 28.0294, Validation Loss: 27.9999\n",
      "Epoch 1212/6000, Training Loss: 28.0095, Validation Loss: 27.9799\n",
      "Epoch 1213/6000, Training Loss: 27.9895, Validation Loss: 27.9598\n",
      "Epoch 1214/6000, Training Loss: 27.9695, Validation Loss: 27.9398\n",
      "Epoch 1215/6000, Training Loss: 27.9495, Validation Loss: 27.9198\n",
      "Epoch 1216/6000, Training Loss: 27.9295, Validation Loss: 27.8998\n",
      "Epoch 1217/6000, Training Loss: 27.9096, Validation Loss: 27.8798\n",
      "Epoch 1218/6000, Training Loss: 27.8896, Validation Loss: 27.8597\n",
      "Epoch 1219/6000, Training Loss: 27.8696, Validation Loss: 27.8397\n",
      "Epoch 1220/6000, Training Loss: 27.8496, Validation Loss: 27.8197\n",
      "Epoch 1221/6000, Training Loss: 27.8296, Validation Loss: 27.7997\n",
      "Epoch 1222/6000, Training Loss: 27.8097, Validation Loss: 27.7796\n",
      "Epoch 1223/6000, Training Loss: 27.7897, Validation Loss: 27.7596\n",
      "Epoch 1224/6000, Training Loss: 27.7697, Validation Loss: 27.7396\n",
      "Epoch 1225/6000, Training Loss: 27.7497, Validation Loss: 27.7196\n",
      "Epoch 1226/6000, Training Loss: 27.7297, Validation Loss: 27.6996\n",
      "Epoch 1227/6000, Training Loss: 27.7098, Validation Loss: 27.6795\n",
      "Epoch 1228/6000, Training Loss: 27.6898, Validation Loss: 27.6595\n",
      "Epoch 1229/6000, Training Loss: 27.6698, Validation Loss: 27.6395\n",
      "Epoch 1230/6000, Training Loss: 27.6498, Validation Loss: 27.6195\n",
      "Epoch 1231/6000, Training Loss: 27.6299, Validation Loss: 27.5994\n",
      "Epoch 1232/6000, Training Loss: 27.6099, Validation Loss: 27.5794\n",
      "Epoch 1233/6000, Training Loss: 27.5899, Validation Loss: 27.5594\n",
      "Epoch 1234/6000, Training Loss: 27.5699, Validation Loss: 27.5394\n",
      "Epoch 1235/6000, Training Loss: 27.5499, Validation Loss: 27.5194\n",
      "Epoch 1236/6000, Training Loss: 27.5300, Validation Loss: 27.4993\n",
      "Epoch 1237/6000, Training Loss: 27.5100, Validation Loss: 27.4793\n",
      "Epoch 1238/6000, Training Loss: 27.4900, Validation Loss: 27.4593\n",
      "Epoch 1239/6000, Training Loss: 27.4700, Validation Loss: 27.4393\n",
      "Epoch 1240/6000, Training Loss: 27.4500, Validation Loss: 27.4192\n",
      "Epoch 1241/6000, Training Loss: 27.4301, Validation Loss: 27.3992\n",
      "Epoch 1242/6000, Training Loss: 27.4101, Validation Loss: 27.3792\n",
      "Epoch 1243/6000, Training Loss: 27.3901, Validation Loss: 27.3592\n",
      "Epoch 1244/6000, Training Loss: 27.3701, Validation Loss: 27.3391\n",
      "Epoch 1245/6000, Training Loss: 27.3501, Validation Loss: 27.3191\n",
      "Epoch 1246/6000, Training Loss: 27.3302, Validation Loss: 27.2991\n",
      "Epoch 1247/6000, Training Loss: 27.3102, Validation Loss: 27.2791\n",
      "Epoch 1248/6000, Training Loss: 27.2902, Validation Loss: 27.2591\n",
      "Epoch 1249/6000, Training Loss: 27.2702, Validation Loss: 27.2390\n",
      "Epoch 1250/6000, Training Loss: 27.2503, Validation Loss: 27.2190\n",
      "Epoch 1251/6000, Training Loss: 27.2303, Validation Loss: 27.1990\n",
      "Epoch 1252/6000, Training Loss: 27.2103, Validation Loss: 27.1790\n",
      "Epoch 1253/6000, Training Loss: 27.1903, Validation Loss: 27.1589\n",
      "Epoch 1254/6000, Training Loss: 27.1703, Validation Loss: 27.1389\n",
      "Epoch 1255/6000, Training Loss: 27.1504, Validation Loss: 27.1189\n",
      "Epoch 1256/6000, Training Loss: 27.1304, Validation Loss: 27.0989\n",
      "Epoch 1257/6000, Training Loss: 27.1104, Validation Loss: 27.0788\n",
      "Epoch 1258/6000, Training Loss: 27.0904, Validation Loss: 27.0588\n",
      "Epoch 1259/6000, Training Loss: 27.0705, Validation Loss: 27.0388\n",
      "Epoch 1260/6000, Training Loss: 27.0505, Validation Loss: 27.0188\n",
      "Epoch 1261/6000, Training Loss: 27.0305, Validation Loss: 26.9987\n",
      "Epoch 1262/6000, Training Loss: 27.0105, Validation Loss: 26.9787\n",
      "Epoch 1263/6000, Training Loss: 26.9905, Validation Loss: 26.9587\n",
      "Epoch 1264/6000, Training Loss: 26.9706, Validation Loss: 26.9387\n",
      "Epoch 1265/6000, Training Loss: 26.9506, Validation Loss: 26.9186\n",
      "Epoch 1266/6000, Training Loss: 26.9306, Validation Loss: 26.8986\n",
      "Epoch 1267/6000, Training Loss: 26.9106, Validation Loss: 26.8786\n",
      "Epoch 1268/6000, Training Loss: 26.8906, Validation Loss: 26.8585\n",
      "Epoch 1269/6000, Training Loss: 26.8707, Validation Loss: 26.8385\n",
      "Epoch 1270/6000, Training Loss: 26.8507, Validation Loss: 26.8185\n",
      "Epoch 1271/6000, Training Loss: 26.8307, Validation Loss: 26.7985\n",
      "Epoch 1272/6000, Training Loss: 26.8107, Validation Loss: 26.7784\n",
      "Epoch 1273/6000, Training Loss: 26.7908, Validation Loss: 26.7584\n",
      "Epoch 1274/6000, Training Loss: 26.7708, Validation Loss: 26.7384\n",
      "Epoch 1275/6000, Training Loss: 26.7508, Validation Loss: 26.7184\n",
      "Epoch 1276/6000, Training Loss: 26.7308, Validation Loss: 26.6983\n",
      "Epoch 1277/6000, Training Loss: 26.7109, Validation Loss: 26.6783\n",
      "Epoch 1278/6000, Training Loss: 26.6909, Validation Loss: 26.6583\n",
      "Epoch 1279/6000, Training Loss: 26.6709, Validation Loss: 26.6382\n",
      "Epoch 1280/6000, Training Loss: 26.6509, Validation Loss: 26.6182\n",
      "Epoch 1281/6000, Training Loss: 26.6309, Validation Loss: 26.5982\n",
      "Epoch 1282/6000, Training Loss: 26.6110, Validation Loss: 26.5782\n",
      "Epoch 1283/6000, Training Loss: 26.5910, Validation Loss: 26.5581\n",
      "Epoch 1284/6000, Training Loss: 26.5710, Validation Loss: 26.5381\n",
      "Epoch 1285/6000, Training Loss: 26.5510, Validation Loss: 26.5181\n",
      "Epoch 1286/6000, Training Loss: 26.5311, Validation Loss: 26.4980\n",
      "Epoch 1287/6000, Training Loss: 26.5111, Validation Loss: 26.4780\n",
      "Epoch 1288/6000, Training Loss: 26.4911, Validation Loss: 26.4580\n",
      "Epoch 1289/6000, Training Loss: 26.4711, Validation Loss: 26.4380\n",
      "Epoch 1290/6000, Training Loss: 26.4511, Validation Loss: 26.4179\n",
      "Epoch 1291/6000, Training Loss: 26.4312, Validation Loss: 26.3979\n",
      "Epoch 1292/6000, Training Loss: 26.4112, Validation Loss: 26.3779\n",
      "Epoch 1293/6000, Training Loss: 26.3912, Validation Loss: 26.3578\n",
      "Epoch 1294/6000, Training Loss: 26.3712, Validation Loss: 26.3378\n",
      "Epoch 1295/6000, Training Loss: 26.3513, Validation Loss: 26.3178\n",
      "Epoch 1296/6000, Training Loss: 26.3313, Validation Loss: 26.2977\n",
      "Epoch 1297/6000, Training Loss: 26.3113, Validation Loss: 26.2777\n",
      "Epoch 1298/6000, Training Loss: 26.2913, Validation Loss: 26.2577\n",
      "Epoch 1299/6000, Training Loss: 26.2714, Validation Loss: 26.2377\n",
      "Epoch 1300/6000, Training Loss: 26.2514, Validation Loss: 26.2176\n",
      "Epoch 1301/6000, Training Loss: 26.2314, Validation Loss: 26.1976\n",
      "Epoch 1302/6000, Training Loss: 26.2114, Validation Loss: 26.1776\n",
      "Epoch 1303/6000, Training Loss: 26.1915, Validation Loss: 26.1576\n",
      "Epoch 1304/6000, Training Loss: 26.1715, Validation Loss: 26.1375\n",
      "Epoch 1305/6000, Training Loss: 26.1515, Validation Loss: 26.1175\n",
      "Epoch 1306/6000, Training Loss: 26.1315, Validation Loss: 26.0975\n",
      "Epoch 1307/6000, Training Loss: 26.1116, Validation Loss: 26.0774\n",
      "Epoch 1308/6000, Training Loss: 26.0916, Validation Loss: 26.0574\n",
      "Epoch 1309/6000, Training Loss: 26.0716, Validation Loss: 26.0374\n",
      "Epoch 1310/6000, Training Loss: 26.0517, Validation Loss: 26.0174\n",
      "Epoch 1311/6000, Training Loss: 26.0317, Validation Loss: 25.9973\n",
      "Epoch 1312/6000, Training Loss: 26.0117, Validation Loss: 25.9773\n",
      "Epoch 1313/6000, Training Loss: 25.9917, Validation Loss: 25.9573\n",
      "Epoch 1314/6000, Training Loss: 25.9718, Validation Loss: 25.9373\n",
      "Epoch 1315/6000, Training Loss: 25.9518, Validation Loss: 25.9172\n",
      "Epoch 1316/6000, Training Loss: 25.9318, Validation Loss: 25.8972\n",
      "Epoch 1317/6000, Training Loss: 25.9119, Validation Loss: 25.8772\n",
      "Epoch 1318/6000, Training Loss: 25.8919, Validation Loss: 25.8571\n",
      "Epoch 1319/6000, Training Loss: 25.8719, Validation Loss: 25.8371\n",
      "Epoch 1320/6000, Training Loss: 25.8520, Validation Loss: 25.8171\n",
      "Epoch 1321/6000, Training Loss: 25.8320, Validation Loss: 25.7971\n",
      "Epoch 1322/6000, Training Loss: 25.8120, Validation Loss: 25.7770\n",
      "Epoch 1323/6000, Training Loss: 25.7921, Validation Loss: 25.7570\n",
      "Epoch 1324/6000, Training Loss: 25.7721, Validation Loss: 25.7370\n",
      "Epoch 1325/6000, Training Loss: 25.7522, Validation Loss: 25.7170\n",
      "Epoch 1326/6000, Training Loss: 25.7322, Validation Loss: 25.6970\n",
      "Epoch 1327/6000, Training Loss: 25.7122, Validation Loss: 25.6769\n",
      "Epoch 1328/6000, Training Loss: 25.6923, Validation Loss: 25.6569\n",
      "Epoch 1329/6000, Training Loss: 25.6723, Validation Loss: 25.6369\n",
      "Epoch 1330/6000, Training Loss: 25.6524, Validation Loss: 25.6169\n",
      "Epoch 1331/6000, Training Loss: 25.6325, Validation Loss: 25.5970\n",
      "Epoch 1332/6000, Training Loss: 25.6126, Validation Loss: 25.5770\n",
      "Epoch 1333/6000, Training Loss: 25.5926, Validation Loss: 25.5570\n",
      "Epoch 1334/6000, Training Loss: 25.5727, Validation Loss: 25.5370\n",
      "Epoch 1335/6000, Training Loss: 25.5528, Validation Loss: 25.5170\n",
      "Epoch 1336/6000, Training Loss: 25.5329, Validation Loss: 25.4970\n",
      "Epoch 1337/6000, Training Loss: 25.5130, Validation Loss: 25.4770\n",
      "Epoch 1338/6000, Training Loss: 25.4931, Validation Loss: 25.4570\n",
      "Epoch 1339/6000, Training Loss: 25.4731, Validation Loss: 25.4370\n",
      "Epoch 1340/6000, Training Loss: 25.4532, Validation Loss: 25.4170\n",
      "Epoch 1341/6000, Training Loss: 25.4333, Validation Loss: 25.3970\n",
      "Epoch 1342/6000, Training Loss: 25.4134, Validation Loss: 25.3770\n",
      "Epoch 1343/6000, Training Loss: 25.3934, Validation Loss: 25.3570\n",
      "Epoch 1344/6000, Training Loss: 25.3735, Validation Loss: 25.3370\n",
      "Epoch 1345/6000, Training Loss: 25.3536, Validation Loss: 25.3170\n",
      "Epoch 1346/6000, Training Loss: 25.3337, Validation Loss: 25.2970\n",
      "Epoch 1347/6000, Training Loss: 25.3138, Validation Loss: 25.2770\n",
      "Epoch 1348/6000, Training Loss: 25.2938, Validation Loss: 25.2570\n",
      "Epoch 1349/6000, Training Loss: 25.2739, Validation Loss: 25.2370\n",
      "Epoch 1350/6000, Training Loss: 25.2540, Validation Loss: 25.2171\n",
      "Epoch 1351/6000, Training Loss: 25.2341, Validation Loss: 25.1971\n",
      "Epoch 1352/6000, Training Loss: 25.2142, Validation Loss: 25.1771\n",
      "Epoch 1353/6000, Training Loss: 25.1943, Validation Loss: 25.1571\n",
      "Epoch 1354/6000, Training Loss: 25.1744, Validation Loss: 25.1371\n",
      "Epoch 1355/6000, Training Loss: 25.1545, Validation Loss: 25.1171\n",
      "Epoch 1356/6000, Training Loss: 25.1346, Validation Loss: 25.0971\n",
      "Epoch 1357/6000, Training Loss: 25.1147, Validation Loss: 25.0771\n",
      "Epoch 1358/6000, Training Loss: 25.0948, Validation Loss: 25.0571\n",
      "Epoch 1359/6000, Training Loss: 25.0749, Validation Loss: 25.0371\n",
      "Epoch 1360/6000, Training Loss: 25.0549, Validation Loss: 25.0171\n",
      "Epoch 1361/6000, Training Loss: 25.0350, Validation Loss: 24.9971\n",
      "Epoch 1362/6000, Training Loss: 25.0151, Validation Loss: 24.9772\n",
      "Epoch 1363/6000, Training Loss: 24.9952, Validation Loss: 24.9572\n",
      "Epoch 1364/6000, Training Loss: 24.9753, Validation Loss: 24.9372\n",
      "Epoch 1365/6000, Training Loss: 24.9554, Validation Loss: 24.9172\n",
      "Epoch 1366/6000, Training Loss: 24.9355, Validation Loss: 24.8972\n",
      "Epoch 1367/6000, Training Loss: 24.9156, Validation Loss: 24.8772\n",
      "Epoch 1368/6000, Training Loss: 24.8957, Validation Loss: 24.8573\n",
      "Epoch 1369/6000, Training Loss: 24.8758, Validation Loss: 24.8373\n",
      "Epoch 1370/6000, Training Loss: 24.8559, Validation Loss: 24.8173\n",
      "Epoch 1371/6000, Training Loss: 24.8360, Validation Loss: 24.7973\n",
      "Epoch 1372/6000, Training Loss: 24.8161, Validation Loss: 24.7774\n",
      "Epoch 1373/6000, Training Loss: 24.7962, Validation Loss: 24.7574\n",
      "Epoch 1374/6000, Training Loss: 24.7763, Validation Loss: 24.7374\n",
      "Epoch 1375/6000, Training Loss: 24.7565, Validation Loss: 24.7175\n",
      "Epoch 1376/6000, Training Loss: 24.7366, Validation Loss: 24.6975\n",
      "Epoch 1377/6000, Training Loss: 24.7167, Validation Loss: 24.6775\n",
      "Epoch 1378/6000, Training Loss: 24.6968, Validation Loss: 24.6576\n",
      "Epoch 1379/6000, Training Loss: 24.6769, Validation Loss: 24.6376\n",
      "Epoch 1380/6000, Training Loss: 24.6570, Validation Loss: 24.6176\n",
      "Epoch 1381/6000, Training Loss: 24.6371, Validation Loss: 24.5977\n",
      "Epoch 1382/6000, Training Loss: 24.6172, Validation Loss: 24.5777\n",
      "Epoch 1383/6000, Training Loss: 24.5973, Validation Loss: 24.5577\n",
      "Epoch 1384/6000, Training Loss: 24.5774, Validation Loss: 24.5378\n",
      "Epoch 1385/6000, Training Loss: 24.5576, Validation Loss: 24.5178\n",
      "Epoch 1386/6000, Training Loss: 24.5377, Validation Loss: 24.4978\n",
      "Epoch 1387/6000, Training Loss: 24.5178, Validation Loss: 24.4779\n",
      "Epoch 1388/6000, Training Loss: 24.4979, Validation Loss: 24.4579\n",
      "Epoch 1389/6000, Training Loss: 24.4780, Validation Loss: 24.4380\n",
      "Epoch 1390/6000, Training Loss: 24.4581, Validation Loss: 24.4180\n",
      "Epoch 1391/6000, Training Loss: 24.4383, Validation Loss: 24.3980\n",
      "Epoch 1392/6000, Training Loss: 24.4184, Validation Loss: 24.3781\n",
      "Epoch 1393/6000, Training Loss: 24.3985, Validation Loss: 24.3581\n",
      "Epoch 1394/6000, Training Loss: 24.3786, Validation Loss: 24.3382\n",
      "Epoch 1395/6000, Training Loss: 24.3588, Validation Loss: 24.3182\n",
      "Epoch 1396/6000, Training Loss: 24.3389, Validation Loss: 24.2983\n",
      "Epoch 1397/6000, Training Loss: 24.3190, Validation Loss: 24.2783\n",
      "Epoch 1398/6000, Training Loss: 24.2991, Validation Loss: 24.2584\n",
      "Epoch 1399/6000, Training Loss: 24.2793, Validation Loss: 24.2384\n",
      "Epoch 1400/6000, Training Loss: 24.2594, Validation Loss: 24.2185\n",
      "Epoch 1401/6000, Training Loss: 24.2395, Validation Loss: 24.1985\n",
      "Epoch 1402/6000, Training Loss: 24.2197, Validation Loss: 24.1786\n",
      "Epoch 1403/6000, Training Loss: 24.1998, Validation Loss: 24.1586\n",
      "Epoch 1404/6000, Training Loss: 24.1799, Validation Loss: 24.1387\n",
      "Epoch 1405/6000, Training Loss: 24.1601, Validation Loss: 24.1187\n",
      "Epoch 1406/6000, Training Loss: 24.1402, Validation Loss: 24.0988\n",
      "Epoch 1407/6000, Training Loss: 24.1203, Validation Loss: 24.0789\n",
      "Epoch 1408/6000, Training Loss: 24.1005, Validation Loss: 24.0589\n",
      "Epoch 1409/6000, Training Loss: 24.0806, Validation Loss: 24.0390\n",
      "Epoch 1410/6000, Training Loss: 24.0607, Validation Loss: 24.0191\n",
      "Epoch 1411/6000, Training Loss: 24.0409, Validation Loss: 23.9991\n",
      "Epoch 1412/6000, Training Loss: 24.0210, Validation Loss: 23.9792\n",
      "Epoch 1413/6000, Training Loss: 24.0012, Validation Loss: 23.9593\n",
      "Epoch 1414/6000, Training Loss: 23.9813, Validation Loss: 23.9393\n",
      "Epoch 1415/6000, Training Loss: 23.9615, Validation Loss: 23.9194\n",
      "Epoch 1416/6000, Training Loss: 23.9416, Validation Loss: 23.8995\n",
      "Epoch 1417/6000, Training Loss: 23.9218, Validation Loss: 23.8796\n",
      "Epoch 1418/6000, Training Loss: 23.9019, Validation Loss: 23.8596\n",
      "Epoch 1419/6000, Training Loss: 23.8821, Validation Loss: 23.8397\n",
      "Epoch 1420/6000, Training Loss: 23.8622, Validation Loss: 23.8198\n",
      "Epoch 1421/6000, Training Loss: 23.8424, Validation Loss: 23.7999\n",
      "Epoch 1422/6000, Training Loss: 23.8225, Validation Loss: 23.7799\n",
      "Epoch 1423/6000, Training Loss: 23.8027, Validation Loss: 23.7600\n",
      "Epoch 1424/6000, Training Loss: 23.7828, Validation Loss: 23.7401\n",
      "Epoch 1425/6000, Training Loss: 23.7630, Validation Loss: 23.7202\n",
      "Epoch 1426/6000, Training Loss: 23.7431, Validation Loss: 23.7003\n",
      "Epoch 1427/6000, Training Loss: 23.7233, Validation Loss: 23.6804\n",
      "Epoch 1428/6000, Training Loss: 23.7035, Validation Loss: 23.6605\n",
      "Epoch 1429/6000, Training Loss: 23.6836, Validation Loss: 23.6405\n",
      "Epoch 1430/6000, Training Loss: 23.6638, Validation Loss: 23.6206\n",
      "Epoch 1431/6000, Training Loss: 23.6439, Validation Loss: 23.6007\n",
      "Epoch 1432/6000, Training Loss: 23.6241, Validation Loss: 23.5808\n",
      "Epoch 1433/6000, Training Loss: 23.6043, Validation Loss: 23.5609\n",
      "Epoch 1434/6000, Training Loss: 23.5844, Validation Loss: 23.5410\n",
      "Epoch 1435/6000, Training Loss: 23.5646, Validation Loss: 23.5211\n",
      "Epoch 1436/6000, Training Loss: 23.5448, Validation Loss: 23.5012\n",
      "Epoch 1437/6000, Training Loss: 23.5250, Validation Loss: 23.4813\n",
      "Epoch 1438/6000, Training Loss: 23.5051, Validation Loss: 23.4614\n",
      "Epoch 1439/6000, Training Loss: 23.4853, Validation Loss: 23.4415\n",
      "Epoch 1440/6000, Training Loss: 23.4655, Validation Loss: 23.4216\n",
      "Epoch 1441/6000, Training Loss: 23.4457, Validation Loss: 23.4017\n",
      "Epoch 1442/6000, Training Loss: 23.4258, Validation Loss: 23.3818\n",
      "Epoch 1443/6000, Training Loss: 23.4060, Validation Loss: 23.3619\n",
      "Epoch 1444/6000, Training Loss: 23.3862, Validation Loss: 23.3420\n",
      "Epoch 1445/6000, Training Loss: 23.3664, Validation Loss: 23.3221\n",
      "Epoch 1446/6000, Training Loss: 23.3466, Validation Loss: 23.3022\n",
      "Epoch 1447/6000, Training Loss: 23.3267, Validation Loss: 23.2824\n",
      "Epoch 1448/6000, Training Loss: 23.3069, Validation Loss: 23.2625\n",
      "Epoch 1449/6000, Training Loss: 23.2871, Validation Loss: 23.2426\n",
      "Epoch 1450/6000, Training Loss: 23.2673, Validation Loss: 23.2227\n",
      "Epoch 1451/6000, Training Loss: 23.2475, Validation Loss: 23.2028\n",
      "Epoch 1452/6000, Training Loss: 23.2277, Validation Loss: 23.1829\n",
      "Epoch 1453/6000, Training Loss: 23.2079, Validation Loss: 23.1631\n",
      "Epoch 1454/6000, Training Loss: 23.1881, Validation Loss: 23.1432\n",
      "Epoch 1455/6000, Training Loss: 23.1682, Validation Loss: 23.1233\n",
      "Epoch 1456/6000, Training Loss: 23.1484, Validation Loss: 23.1034\n",
      "Epoch 1457/6000, Training Loss: 23.1286, Validation Loss: 23.0836\n",
      "Epoch 1458/6000, Training Loss: 23.1088, Validation Loss: 23.0637\n",
      "Epoch 1459/6000, Training Loss: 23.0890, Validation Loss: 23.0438\n",
      "Epoch 1460/6000, Training Loss: 23.0692, Validation Loss: 23.0240\n",
      "Epoch 1461/6000, Training Loss: 23.0494, Validation Loss: 23.0041\n",
      "Epoch 1462/6000, Training Loss: 23.0296, Validation Loss: 22.9842\n",
      "Epoch 1463/6000, Training Loss: 23.0098, Validation Loss: 22.9644\n",
      "Epoch 1464/6000, Training Loss: 22.9900, Validation Loss: 22.9445\n",
      "Epoch 1465/6000, Training Loss: 22.9703, Validation Loss: 22.9247\n",
      "Epoch 1466/6000, Training Loss: 22.9505, Validation Loss: 22.9048\n",
      "Epoch 1467/6000, Training Loss: 22.9307, Validation Loss: 22.8849\n",
      "Epoch 1468/6000, Training Loss: 22.9109, Validation Loss: 22.8651\n",
      "Epoch 1469/6000, Training Loss: 22.8911, Validation Loss: 22.8452\n",
      "Epoch 1470/6000, Training Loss: 22.8713, Validation Loss: 22.8254\n",
      "Epoch 1471/6000, Training Loss: 22.8515, Validation Loss: 22.8055\n",
      "Epoch 1472/6000, Training Loss: 22.8317, Validation Loss: 22.7857\n",
      "Epoch 1473/6000, Training Loss: 22.8119, Validation Loss: 22.7658\n",
      "Epoch 1474/6000, Training Loss: 22.7922, Validation Loss: 22.7460\n",
      "Epoch 1475/6000, Training Loss: 22.7724, Validation Loss: 22.7261\n",
      "Epoch 1476/6000, Training Loss: 22.7526, Validation Loss: 22.7063\n",
      "Epoch 1477/6000, Training Loss: 22.7328, Validation Loss: 22.6865\n",
      "Epoch 1478/6000, Training Loss: 22.7130, Validation Loss: 22.6666\n",
      "Epoch 1479/6000, Training Loss: 22.6933, Validation Loss: 22.6468\n",
      "Epoch 1480/6000, Training Loss: 22.6735, Validation Loss: 22.6270\n",
      "Epoch 1481/6000, Training Loss: 22.6537, Validation Loss: 22.6071\n",
      "Epoch 1482/6000, Training Loss: 22.6339, Validation Loss: 22.5873\n",
      "Epoch 1483/6000, Training Loss: 22.6142, Validation Loss: 22.5675\n",
      "Epoch 1484/6000, Training Loss: 22.5944, Validation Loss: 22.5476\n",
      "Epoch 1485/6000, Training Loss: 22.5746, Validation Loss: 22.5278\n",
      "Epoch 1486/6000, Training Loss: 22.5548, Validation Loss: 22.5080\n",
      "Epoch 1487/6000, Training Loss: 22.5351, Validation Loss: 22.4881\n",
      "Epoch 1488/6000, Training Loss: 22.5153, Validation Loss: 22.4683\n",
      "Epoch 1489/6000, Training Loss: 22.4955, Validation Loss: 22.4485\n",
      "Epoch 1490/6000, Training Loss: 22.4758, Validation Loss: 22.4287\n",
      "Epoch 1491/6000, Training Loss: 22.4560, Validation Loss: 22.4088\n",
      "Epoch 1492/6000, Training Loss: 22.4362, Validation Loss: 22.3890\n",
      "Epoch 1493/6000, Training Loss: 22.4165, Validation Loss: 22.3692\n",
      "Epoch 1494/6000, Training Loss: 22.3967, Validation Loss: 22.3494\n",
      "Epoch 1495/6000, Training Loss: 22.3770, Validation Loss: 22.3296\n",
      "Epoch 1496/6000, Training Loss: 22.3572, Validation Loss: 22.3098\n",
      "Epoch 1497/6000, Training Loss: 22.3375, Validation Loss: 22.2900\n",
      "Epoch 1498/6000, Training Loss: 22.3177, Validation Loss: 22.2702\n",
      "Epoch 1499/6000, Training Loss: 22.2980, Validation Loss: 22.2504\n",
      "Epoch 1500/6000, Training Loss: 22.2783, Validation Loss: 22.2307\n",
      "Epoch 1501/6000, Training Loss: 22.2587, Validation Loss: 22.2110\n",
      "Epoch 1502/6000, Training Loss: 22.2390, Validation Loss: 22.1913\n",
      "Epoch 1503/6000, Training Loss: 22.2195, Validation Loss: 22.1717\n",
      "Epoch 1504/6000, Training Loss: 22.1999, Validation Loss: 22.1522\n",
      "Epoch 1505/6000, Training Loss: 22.1805, Validation Loss: 22.1327\n",
      "Epoch 1506/6000, Training Loss: 22.1610, Validation Loss: 22.1133\n",
      "Epoch 1507/6000, Training Loss: 22.1417, Validation Loss: 22.0939\n",
      "Epoch 1508/6000, Training Loss: 22.1224, Validation Loss: 22.0745\n",
      "Epoch 1509/6000, Training Loss: 22.1031, Validation Loss: 22.0552\n",
      "Epoch 1510/6000, Training Loss: 22.0838, Validation Loss: 22.0359\n",
      "Epoch 1511/6000, Training Loss: 22.0645, Validation Loss: 22.0166\n",
      "Epoch 1512/6000, Training Loss: 22.0453, Validation Loss: 21.9974\n",
      "Epoch 1513/6000, Training Loss: 22.0261, Validation Loss: 21.9782\n",
      "Epoch 1514/6000, Training Loss: 22.0069, Validation Loss: 21.9590\n",
      "Epoch 1515/6000, Training Loss: 21.9877, Validation Loss: 21.9398\n",
      "Epoch 1516/6000, Training Loss: 21.9685, Validation Loss: 21.9207\n",
      "Epoch 1517/6000, Training Loss: 21.9494, Validation Loss: 21.9016\n",
      "Epoch 1518/6000, Training Loss: 21.9303, Validation Loss: 21.8825\n",
      "Epoch 1519/6000, Training Loss: 21.9113, Validation Loss: 21.8635\n",
      "Epoch 1520/6000, Training Loss: 21.8923, Validation Loss: 21.8446\n",
      "Epoch 1521/6000, Training Loss: 21.8733, Validation Loss: 21.8257\n",
      "Epoch 1522/6000, Training Loss: 21.8544, Validation Loss: 21.8068\n",
      "Epoch 1523/6000, Training Loss: 21.8356, Validation Loss: 21.7880\n",
      "Epoch 1524/6000, Training Loss: 21.8168, Validation Loss: 21.7692\n",
      "Epoch 1525/6000, Training Loss: 21.7980, Validation Loss: 21.7505\n",
      "Epoch 1526/6000, Training Loss: 21.7793, Validation Loss: 21.7319\n",
      "Epoch 1527/6000, Training Loss: 21.7607, Validation Loss: 21.7132\n",
      "Epoch 1528/6000, Training Loss: 21.7420, Validation Loss: 21.6946\n",
      "Epoch 1529/6000, Training Loss: 21.7234, Validation Loss: 21.6761\n",
      "Epoch 1530/6000, Training Loss: 21.7048, Validation Loss: 21.6576\n",
      "Epoch 1531/6000, Training Loss: 21.6863, Validation Loss: 21.6390\n",
      "Epoch 1532/6000, Training Loss: 21.6678, Validation Loss: 21.6206\n",
      "Epoch 1533/6000, Training Loss: 21.6493, Validation Loss: 21.6021\n",
      "Epoch 1534/6000, Training Loss: 21.6308, Validation Loss: 21.5836\n",
      "Epoch 1535/6000, Training Loss: 21.6123, Validation Loss: 21.5652\n",
      "Epoch 1536/6000, Training Loss: 21.5938, Validation Loss: 21.5468\n",
      "Epoch 1537/6000, Training Loss: 21.5754, Validation Loss: 21.5284\n",
      "Epoch 1538/6000, Training Loss: 21.5570, Validation Loss: 21.5100\n",
      "Epoch 1539/6000, Training Loss: 21.5386, Validation Loss: 21.4917\n",
      "Epoch 1540/6000, Training Loss: 21.5202, Validation Loss: 21.4734\n",
      "Epoch 1541/6000, Training Loss: 21.5019, Validation Loss: 21.4551\n",
      "Epoch 1542/6000, Training Loss: 21.4835, Validation Loss: 21.4368\n",
      "Epoch 1543/6000, Training Loss: 21.4653, Validation Loss: 21.4186\n",
      "Epoch 1544/6000, Training Loss: 21.4470, Validation Loss: 21.4004\n",
      "Epoch 1545/6000, Training Loss: 21.4288, Validation Loss: 21.3822\n",
      "Epoch 1546/6000, Training Loss: 21.4106, Validation Loss: 21.3640\n",
      "Epoch 1547/6000, Training Loss: 21.3924, Validation Loss: 21.3459\n",
      "Epoch 1548/6000, Training Loss: 21.3743, Validation Loss: 21.3279\n",
      "Epoch 1549/6000, Training Loss: 21.3563, Validation Loss: 21.3099\n",
      "Epoch 1550/6000, Training Loss: 21.3383, Validation Loss: 21.2920\n",
      "Epoch 1551/6000, Training Loss: 21.3204, Validation Loss: 21.2741\n",
      "Epoch 1552/6000, Training Loss: 21.3025, Validation Loss: 21.2563\n",
      "Epoch 1553/6000, Training Loss: 21.2847, Validation Loss: 21.2385\n",
      "Epoch 1554/6000, Training Loss: 21.2668, Validation Loss: 21.2207\n",
      "Epoch 1555/6000, Training Loss: 21.2491, Validation Loss: 21.2030\n",
      "Epoch 1556/6000, Training Loss: 21.2313, Validation Loss: 21.1853\n",
      "Epoch 1557/6000, Training Loss: 21.2136, Validation Loss: 21.1676\n",
      "Epoch 1558/6000, Training Loss: 21.1958, Validation Loss: 21.1499\n",
      "Epoch 1559/6000, Training Loss: 21.1781, Validation Loss: 21.1323\n",
      "Epoch 1560/6000, Training Loss: 21.1605, Validation Loss: 21.1147\n",
      "Epoch 1561/6000, Training Loss: 21.1428, Validation Loss: 21.0971\n",
      "Epoch 1562/6000, Training Loss: 21.1251, Validation Loss: 21.0795\n",
      "Epoch 1563/6000, Training Loss: 21.1075, Validation Loss: 21.0619\n",
      "Epoch 1564/6000, Training Loss: 21.0899, Validation Loss: 21.0443\n",
      "Epoch 1565/6000, Training Loss: 21.0723, Validation Loss: 21.0268\n",
      "Epoch 1566/6000, Training Loss: 21.0547, Validation Loss: 21.0093\n",
      "Epoch 1567/6000, Training Loss: 21.0371, Validation Loss: 20.9918\n",
      "Epoch 1568/6000, Training Loss: 21.0196, Validation Loss: 20.9743\n",
      "Epoch 1569/6000, Training Loss: 21.0021, Validation Loss: 20.9569\n",
      "Epoch 1570/6000, Training Loss: 20.9846, Validation Loss: 20.9395\n",
      "Epoch 1571/6000, Training Loss: 20.9672, Validation Loss: 20.9221\n",
      "Epoch 1572/6000, Training Loss: 20.9498, Validation Loss: 20.9048\n",
      "Epoch 1573/6000, Training Loss: 20.9324, Validation Loss: 20.8875\n",
      "Epoch 1574/6000, Training Loss: 20.9151, Validation Loss: 20.8703\n",
      "Epoch 1575/6000, Training Loss: 20.8979, Validation Loss: 20.8531\n",
      "Epoch 1576/6000, Training Loss: 20.8806, Validation Loss: 20.8359\n",
      "Epoch 1577/6000, Training Loss: 20.8635, Validation Loss: 20.8188\n",
      "Epoch 1578/6000, Training Loss: 20.8463, Validation Loss: 20.8018\n",
      "Epoch 1579/6000, Training Loss: 20.8293, Validation Loss: 20.7847\n",
      "Epoch 1580/6000, Training Loss: 20.8122, Validation Loss: 20.7677\n",
      "Epoch 1581/6000, Training Loss: 20.7952, Validation Loss: 20.7508\n",
      "Epoch 1582/6000, Training Loss: 20.7782, Validation Loss: 20.7339\n",
      "Epoch 1583/6000, Training Loss: 20.7612, Validation Loss: 20.7170\n",
      "Epoch 1584/6000, Training Loss: 20.7443, Validation Loss: 20.7001\n",
      "Epoch 1585/6000, Training Loss: 20.7274, Validation Loss: 20.6832\n",
      "Epoch 1586/6000, Training Loss: 20.7104, Validation Loss: 20.6664\n",
      "Epoch 1587/6000, Training Loss: 20.6935, Validation Loss: 20.6495\n",
      "Epoch 1588/6000, Training Loss: 20.6767, Validation Loss: 20.6327\n",
      "Epoch 1589/6000, Training Loss: 20.6598, Validation Loss: 20.6159\n",
      "Epoch 1590/6000, Training Loss: 20.6429, Validation Loss: 20.5992\n",
      "Epoch 1591/6000, Training Loss: 20.6261, Validation Loss: 20.5824\n",
      "Epoch 1592/6000, Training Loss: 20.6093, Validation Loss: 20.5656\n",
      "Epoch 1593/6000, Training Loss: 20.5925, Validation Loss: 20.5489\n",
      "Epoch 1594/6000, Training Loss: 20.5757, Validation Loss: 20.5321\n",
      "Epoch 1595/6000, Training Loss: 20.5589, Validation Loss: 20.5154\n",
      "Epoch 1596/6000, Training Loss: 20.5422, Validation Loss: 20.4988\n",
      "Epoch 1597/6000, Training Loss: 20.5255, Validation Loss: 20.4822\n",
      "Epoch 1598/6000, Training Loss: 20.5089, Validation Loss: 20.4656\n",
      "Epoch 1599/6000, Training Loss: 20.4922, Validation Loss: 20.4490\n",
      "Epoch 1600/6000, Training Loss: 20.4757, Validation Loss: 20.4326\n",
      "Epoch 1601/6000, Training Loss: 20.4591, Validation Loss: 20.4161\n",
      "Epoch 1602/6000, Training Loss: 20.4426, Validation Loss: 20.3997\n",
      "Epoch 1603/6000, Training Loss: 20.4262, Validation Loss: 20.3834\n",
      "Epoch 1604/6000, Training Loss: 20.4098, Validation Loss: 20.3670\n",
      "Epoch 1605/6000, Training Loss: 20.3934, Validation Loss: 20.3507\n",
      "Epoch 1606/6000, Training Loss: 20.3771, Validation Loss: 20.3345\n",
      "Epoch 1607/6000, Training Loss: 20.3608, Validation Loss: 20.3183\n",
      "Epoch 1608/6000, Training Loss: 20.3446, Validation Loss: 20.3021\n",
      "Epoch 1609/6000, Training Loss: 20.3283, Validation Loss: 20.2860\n",
      "Epoch 1610/6000, Training Loss: 20.3121, Validation Loss: 20.2698\n",
      "Epoch 1611/6000, Training Loss: 20.2960, Validation Loss: 20.2537\n",
      "Epoch 1612/6000, Training Loss: 20.2798, Validation Loss: 20.2376\n",
      "Epoch 1613/6000, Training Loss: 20.2636, Validation Loss: 20.2216\n",
      "Epoch 1614/6000, Training Loss: 20.2475, Validation Loss: 20.2055\n",
      "Epoch 1615/6000, Training Loss: 20.2314, Validation Loss: 20.1895\n",
      "Epoch 1616/6000, Training Loss: 20.2153, Validation Loss: 20.1735\n",
      "Epoch 1617/6000, Training Loss: 20.1992, Validation Loss: 20.1575\n",
      "Epoch 1618/6000, Training Loss: 20.1831, Validation Loss: 20.1415\n",
      "Epoch 1619/6000, Training Loss: 20.1670, Validation Loss: 20.1255\n",
      "Epoch 1620/6000, Training Loss: 20.1510, Validation Loss: 20.1096\n",
      "Epoch 1621/6000, Training Loss: 20.1350, Validation Loss: 20.0936\n",
      "Epoch 1622/6000, Training Loss: 20.1190, Validation Loss: 20.0777\n",
      "Epoch 1623/6000, Training Loss: 20.1030, Validation Loss: 20.0618\n",
      "Epoch 1624/6000, Training Loss: 20.0871, Validation Loss: 20.0459\n",
      "Epoch 1625/6000, Training Loss: 20.0711, Validation Loss: 20.0301\n",
      "Epoch 1626/6000, Training Loss: 20.0553, Validation Loss: 20.0143\n",
      "Epoch 1627/6000, Training Loss: 20.0394, Validation Loss: 19.9985\n",
      "Epoch 1628/6000, Training Loss: 20.0236, Validation Loss: 19.9828\n",
      "Epoch 1629/6000, Training Loss: 20.0079, Validation Loss: 19.9671\n",
      "Epoch 1630/6000, Training Loss: 19.9921, Validation Loss: 19.9515\n",
      "Epoch 1631/6000, Training Loss: 19.9764, Validation Loss: 19.9359\n",
      "Epoch 1632/6000, Training Loss: 19.9608, Validation Loss: 19.9204\n",
      "Epoch 1633/6000, Training Loss: 19.9452, Validation Loss: 19.9048\n",
      "Epoch 1634/6000, Training Loss: 19.9296, Validation Loss: 19.8893\n",
      "Epoch 1635/6000, Training Loss: 19.9141, Validation Loss: 19.8739\n",
      "Epoch 1636/6000, Training Loss: 19.8985, Validation Loss: 19.8584\n",
      "Epoch 1637/6000, Training Loss: 19.8830, Validation Loss: 19.8430\n",
      "Epoch 1638/6000, Training Loss: 19.8676, Validation Loss: 19.8276\n",
      "Epoch 1639/6000, Training Loss: 19.8521, Validation Loss: 19.8122\n",
      "Epoch 1640/6000, Training Loss: 19.8367, Validation Loss: 19.7968\n",
      "Epoch 1641/6000, Training Loss: 19.8213, Validation Loss: 19.7815\n",
      "Epoch 1642/6000, Training Loss: 19.8059, Validation Loss: 19.7661\n",
      "Epoch 1643/6000, Training Loss: 19.7905, Validation Loss: 19.7508\n",
      "Epoch 1644/6000, Training Loss: 19.7751, Validation Loss: 19.7355\n",
      "Epoch 1645/6000, Training Loss: 19.7597, Validation Loss: 19.7202\n",
      "Epoch 1646/6000, Training Loss: 19.7444, Validation Loss: 19.7050\n",
      "Epoch 1647/6000, Training Loss: 19.7291, Validation Loss: 19.6897\n",
      "Epoch 1648/6000, Training Loss: 19.7138, Validation Loss: 19.6745\n",
      "Epoch 1649/6000, Training Loss: 19.6985, Validation Loss: 19.6592\n",
      "Epoch 1650/6000, Training Loss: 19.6832, Validation Loss: 19.6440\n",
      "Epoch 1651/6000, Training Loss: 19.6680, Validation Loss: 19.6288\n",
      "Epoch 1652/6000, Training Loss: 19.6528, Validation Loss: 19.6136\n",
      "Epoch 1653/6000, Training Loss: 19.6376, Validation Loss: 19.5984\n",
      "Epoch 1654/6000, Training Loss: 19.6224, Validation Loss: 19.5833\n",
      "Epoch 1655/6000, Training Loss: 19.6073, Validation Loss: 19.5683\n",
      "Epoch 1656/6000, Training Loss: 19.5922, Validation Loss: 19.5533\n",
      "Epoch 1657/6000, Training Loss: 19.5772, Validation Loss: 19.5383\n",
      "Epoch 1658/6000, Training Loss: 19.5621, Validation Loss: 19.5234\n",
      "Epoch 1659/6000, Training Loss: 19.5472, Validation Loss: 19.5085\n",
      "Epoch 1660/6000, Training Loss: 19.5322, Validation Loss: 19.4936\n",
      "Epoch 1661/6000, Training Loss: 19.5173, Validation Loss: 19.4788\n",
      "Epoch 1662/6000, Training Loss: 19.5025, Validation Loss: 19.4640\n",
      "Epoch 1663/6000, Training Loss: 19.4877, Validation Loss: 19.4493\n",
      "Epoch 1664/6000, Training Loss: 19.4728, Validation Loss: 19.4345\n",
      "Epoch 1665/6000, Training Loss: 19.4581, Validation Loss: 19.4198\n",
      "Epoch 1666/6000, Training Loss: 19.4433, Validation Loss: 19.4051\n",
      "Epoch 1667/6000, Training Loss: 19.4286, Validation Loss: 19.3904\n",
      "Epoch 1668/6000, Training Loss: 19.4138, Validation Loss: 19.3758\n",
      "Epoch 1669/6000, Training Loss: 19.3991, Validation Loss: 19.3611\n",
      "Epoch 1670/6000, Training Loss: 19.3844, Validation Loss: 19.3465\n",
      "Epoch 1671/6000, Training Loss: 19.3697, Validation Loss: 19.3318\n",
      "Epoch 1672/6000, Training Loss: 19.3551, Validation Loss: 19.3172\n",
      "Epoch 1673/6000, Training Loss: 19.3404, Validation Loss: 19.3026\n",
      "Epoch 1674/6000, Training Loss: 19.3258, Validation Loss: 19.2881\n",
      "Epoch 1675/6000, Training Loss: 19.3111, Validation Loss: 19.2735\n",
      "Epoch 1676/6000, Training Loss: 19.2965, Validation Loss: 19.2589\n",
      "Epoch 1677/6000, Training Loss: 19.2820, Validation Loss: 19.2444\n",
      "Epoch 1678/6000, Training Loss: 19.2674, Validation Loss: 19.2299\n",
      "Epoch 1679/6000, Training Loss: 19.2528, Validation Loss: 19.2154\n",
      "Epoch 1680/6000, Training Loss: 19.2383, Validation Loss: 19.2009\n",
      "Epoch 1681/6000, Training Loss: 19.2238, Validation Loss: 19.1864\n",
      "Epoch 1682/6000, Training Loss: 19.2093, Validation Loss: 19.1719\n",
      "Epoch 1683/6000, Training Loss: 19.1948, Validation Loss: 19.1575\n",
      "Epoch 1684/6000, Training Loss: 19.1804, Validation Loss: 19.1432\n",
      "Epoch 1685/6000, Training Loss: 19.1660, Validation Loss: 19.1288\n",
      "Epoch 1686/6000, Training Loss: 19.1517, Validation Loss: 19.1146\n",
      "Epoch 1687/6000, Training Loss: 19.1374, Validation Loss: 19.1003\n",
      "Epoch 1688/6000, Training Loss: 19.1231, Validation Loss: 19.0862\n",
      "Epoch 1689/6000, Training Loss: 19.1089, Validation Loss: 19.0720\n",
      "Epoch 1690/6000, Training Loss: 19.0947, Validation Loss: 19.0579\n",
      "Epoch 1691/6000, Training Loss: 19.0805, Validation Loss: 19.0438\n",
      "Epoch 1692/6000, Training Loss: 19.0664, Validation Loss: 19.0297\n",
      "Epoch 1693/6000, Training Loss: 19.0523, Validation Loss: 19.0157\n",
      "Epoch 1694/6000, Training Loss: 19.0382, Validation Loss: 19.0016\n",
      "Epoch 1695/6000, Training Loss: 19.0241, Validation Loss: 18.9876\n",
      "Epoch 1696/6000, Training Loss: 19.0101, Validation Loss: 18.9736\n",
      "Epoch 1697/6000, Training Loss: 18.9960, Validation Loss: 18.9596\n",
      "Epoch 1698/6000, Training Loss: 18.9820, Validation Loss: 18.9456\n",
      "Epoch 1699/6000, Training Loss: 18.9680, Validation Loss: 18.9317\n",
      "Epoch 1700/6000, Training Loss: 18.9540, Validation Loss: 18.9177\n",
      "Epoch 1701/6000, Training Loss: 18.9400, Validation Loss: 18.9037\n",
      "Epoch 1702/6000, Training Loss: 18.9260, Validation Loss: 18.8898\n",
      "Epoch 1703/6000, Training Loss: 18.9121, Validation Loss: 18.8759\n",
      "Epoch 1704/6000, Training Loss: 18.8981, Validation Loss: 18.8620\n",
      "Epoch 1705/6000, Training Loss: 18.8842, Validation Loss: 18.8481\n",
      "Epoch 1706/6000, Training Loss: 18.8703, Validation Loss: 18.8342\n",
      "Epoch 1707/6000, Training Loss: 18.8564, Validation Loss: 18.8204\n",
      "Epoch 1708/6000, Training Loss: 18.8425, Validation Loss: 18.8066\n",
      "Epoch 1709/6000, Training Loss: 18.8287, Validation Loss: 18.7927\n",
      "Epoch 1710/6000, Training Loss: 18.8148, Validation Loss: 18.7789\n",
      "Epoch 1711/6000, Training Loss: 18.8010, Validation Loss: 18.7652\n",
      "Epoch 1712/6000, Training Loss: 18.7872, Validation Loss: 18.7514\n",
      "Epoch 1713/6000, Training Loss: 18.7735, Validation Loss: 18.7377\n",
      "Epoch 1714/6000, Training Loss: 18.7597, Validation Loss: 18.7240\n",
      "Epoch 1715/6000, Training Loss: 18.7460, Validation Loss: 18.7104\n",
      "Epoch 1716/6000, Training Loss: 18.7324, Validation Loss: 18.6968\n",
      "Epoch 1717/6000, Training Loss: 18.7188, Validation Loss: 18.6833\n",
      "Epoch 1718/6000, Training Loss: 18.7052, Validation Loss: 18.6698\n",
      "Epoch 1719/6000, Training Loss: 18.6917, Validation Loss: 18.6564\n",
      "Epoch 1720/6000, Training Loss: 18.6782, Validation Loss: 18.6429\n",
      "Epoch 1721/6000, Training Loss: 18.6647, Validation Loss: 18.6295\n",
      "Epoch 1722/6000, Training Loss: 18.6512, Validation Loss: 18.6162\n",
      "Epoch 1723/6000, Training Loss: 18.6378, Validation Loss: 18.6028\n",
      "Epoch 1724/6000, Training Loss: 18.6244, Validation Loss: 18.5894\n",
      "Epoch 1725/6000, Training Loss: 18.6110, Validation Loss: 18.5761\n",
      "Epoch 1726/6000, Training Loss: 18.5976, Validation Loss: 18.5628\n",
      "Epoch 1727/6000, Training Loss: 18.5843, Validation Loss: 18.5494\n",
      "Epoch 1728/6000, Training Loss: 18.5709, Validation Loss: 18.5361\n",
      "Epoch 1729/6000, Training Loss: 18.5576, Validation Loss: 18.5228\n",
      "Epoch 1730/6000, Training Loss: 18.5443, Validation Loss: 18.5096\n",
      "Epoch 1731/6000, Training Loss: 18.5309, Validation Loss: 18.4963\n",
      "Epoch 1732/6000, Training Loss: 18.5176, Validation Loss: 18.4830\n",
      "Epoch 1733/6000, Training Loss: 18.5043, Validation Loss: 18.4698\n",
      "Epoch 1734/6000, Training Loss: 18.4911, Validation Loss: 18.4565\n",
      "Epoch 1735/6000, Training Loss: 18.4778, Validation Loss: 18.4433\n",
      "Epoch 1736/6000, Training Loss: 18.4646, Validation Loss: 18.4301\n",
      "Epoch 1737/6000, Training Loss: 18.4514, Validation Loss: 18.4170\n",
      "Epoch 1738/6000, Training Loss: 18.4382, Validation Loss: 18.4038\n",
      "Epoch 1739/6000, Training Loss: 18.4250, Validation Loss: 18.3907\n",
      "Epoch 1740/6000, Training Loss: 18.4118, Validation Loss: 18.3775\n",
      "Epoch 1741/6000, Training Loss: 18.3987, Validation Loss: 18.3644\n",
      "Epoch 1742/6000, Training Loss: 18.3856, Validation Loss: 18.3513\n",
      "Epoch 1743/6000, Training Loss: 18.3725, Validation Loss: 18.3383\n",
      "Epoch 1744/6000, Training Loss: 18.3594, Validation Loss: 18.3253\n",
      "Epoch 1745/6000, Training Loss: 18.3464, Validation Loss: 18.3123\n",
      "Epoch 1746/6000, Training Loss: 18.3334, Validation Loss: 18.2994\n",
      "Epoch 1747/6000, Training Loss: 18.3204, Validation Loss: 18.2865\n",
      "Epoch 1748/6000, Training Loss: 18.3075, Validation Loss: 18.2737\n",
      "Epoch 1749/6000, Training Loss: 18.2946, Validation Loss: 18.2609\n",
      "Epoch 1750/6000, Training Loss: 18.2818, Validation Loss: 18.2482\n",
      "Epoch 1751/6000, Training Loss: 18.2690, Validation Loss: 18.2354\n",
      "Epoch 1752/6000, Training Loss: 18.2562, Validation Loss: 18.2227\n",
      "Epoch 1753/6000, Training Loss: 18.2434, Validation Loss: 18.2100\n",
      "Epoch 1754/6000, Training Loss: 18.2307, Validation Loss: 18.1973\n",
      "Epoch 1755/6000, Training Loss: 18.2179, Validation Loss: 18.1846\n",
      "Epoch 1756/6000, Training Loss: 18.2052, Validation Loss: 18.1720\n",
      "Epoch 1757/6000, Training Loss: 18.1925, Validation Loss: 18.1593\n",
      "Epoch 1758/6000, Training Loss: 18.1798, Validation Loss: 18.1467\n",
      "Epoch 1759/6000, Training Loss: 18.1671, Validation Loss: 18.1340\n",
      "Epoch 1760/6000, Training Loss: 18.1545, Validation Loss: 18.1214\n",
      "Epoch 1761/6000, Training Loss: 18.1418, Validation Loss: 18.1088\n",
      "Epoch 1762/6000, Training Loss: 18.1291, Validation Loss: 18.0962\n",
      "Epoch 1763/6000, Training Loss: 18.1165, Validation Loss: 18.0836\n",
      "Epoch 1764/6000, Training Loss: 18.1038, Validation Loss: 18.0710\n",
      "Epoch 1765/6000, Training Loss: 18.0912, Validation Loss: 18.0584\n",
      "Epoch 1766/6000, Training Loss: 18.0786, Validation Loss: 18.0458\n",
      "Epoch 1767/6000, Training Loss: 18.0660, Validation Loss: 18.0333\n",
      "Epoch 1768/6000, Training Loss: 18.0535, Validation Loss: 18.0208\n",
      "Epoch 1769/6000, Training Loss: 18.0409, Validation Loss: 18.0083\n",
      "Epoch 1770/6000, Training Loss: 18.0284, Validation Loss: 17.9958\n",
      "Epoch 1771/6000, Training Loss: 18.0159, Validation Loss: 17.9833\n",
      "Epoch 1772/6000, Training Loss: 18.0034, Validation Loss: 17.9709\n",
      "Epoch 1773/6000, Training Loss: 17.9909, Validation Loss: 17.9585\n",
      "Epoch 1774/6000, Training Loss: 17.9785, Validation Loss: 17.9461\n",
      "Epoch 1775/6000, Training Loss: 17.9661, Validation Loss: 17.9338\n",
      "Epoch 1776/6000, Training Loss: 17.9537, Validation Loss: 17.9215\n",
      "Epoch 1777/6000, Training Loss: 17.9414, Validation Loss: 17.9092\n",
      "Epoch 1778/6000, Training Loss: 17.9291, Validation Loss: 17.8970\n",
      "Epoch 1779/6000, Training Loss: 17.9168, Validation Loss: 17.8848\n",
      "Epoch 1780/6000, Training Loss: 17.9046, Validation Loss: 17.8726\n",
      "Epoch 1781/6000, Training Loss: 17.8923, Validation Loss: 17.8605\n",
      "Epoch 1782/6000, Training Loss: 17.8802, Validation Loss: 17.8484\n",
      "Epoch 1783/6000, Training Loss: 17.8680, Validation Loss: 17.8363\n",
      "Epoch 1784/6000, Training Loss: 17.8559, Validation Loss: 17.8242\n",
      "Epoch 1785/6000, Training Loss: 17.8438, Validation Loss: 17.8122\n",
      "Epoch 1786/6000, Training Loss: 17.8317, Validation Loss: 17.8001\n",
      "Epoch 1787/6000, Training Loss: 17.8196, Validation Loss: 17.7881\n",
      "Epoch 1788/6000, Training Loss: 17.8075, Validation Loss: 17.7760\n",
      "Epoch 1789/6000, Training Loss: 17.7955, Validation Loss: 17.7640\n",
      "Epoch 1790/6000, Training Loss: 17.7834, Validation Loss: 17.7520\n",
      "Epoch 1791/6000, Training Loss: 17.7714, Validation Loss: 17.7400\n",
      "Epoch 1792/6000, Training Loss: 17.7593, Validation Loss: 17.7280\n",
      "Epoch 1793/6000, Training Loss: 17.7473, Validation Loss: 17.7161\n",
      "Epoch 1794/6000, Training Loss: 17.7353, Validation Loss: 17.7041\n",
      "Epoch 1795/6000, Training Loss: 17.7233, Validation Loss: 17.6922\n",
      "Epoch 1796/6000, Training Loss: 17.7113, Validation Loss: 17.6802\n",
      "Epoch 1797/6000, Training Loss: 17.6993, Validation Loss: 17.6683\n",
      "Epoch 1798/6000, Training Loss: 17.6874, Validation Loss: 17.6564\n",
      "Epoch 1799/6000, Training Loss: 17.6755, Validation Loss: 17.6445\n",
      "Epoch 1800/6000, Training Loss: 17.6636, Validation Loss: 17.6326\n",
      "Epoch 1801/6000, Training Loss: 17.6517, Validation Loss: 17.6208\n",
      "Epoch 1802/6000, Training Loss: 17.6398, Validation Loss: 17.6089\n",
      "Epoch 1803/6000, Training Loss: 17.6279, Validation Loss: 17.5971\n",
      "Epoch 1804/6000, Training Loss: 17.6161, Validation Loss: 17.5853\n",
      "Epoch 1805/6000, Training Loss: 17.6043, Validation Loss: 17.5736\n",
      "Epoch 1806/6000, Training Loss: 17.5925, Validation Loss: 17.5618\n",
      "Epoch 1807/6000, Training Loss: 17.5807, Validation Loss: 17.5501\n",
      "Epoch 1808/6000, Training Loss: 17.5690, Validation Loss: 17.5385\n",
      "Epoch 1809/6000, Training Loss: 17.5573, Validation Loss: 17.5269\n",
      "Epoch 1810/6000, Training Loss: 17.5456, Validation Loss: 17.5153\n",
      "Epoch 1811/6000, Training Loss: 17.5340, Validation Loss: 17.5038\n",
      "Epoch 1812/6000, Training Loss: 17.5223, Validation Loss: 17.4922\n",
      "Epoch 1813/6000, Training Loss: 17.5108, Validation Loss: 17.4808\n",
      "Epoch 1814/6000, Training Loss: 17.4992, Validation Loss: 17.4693\n",
      "Epoch 1815/6000, Training Loss: 17.4877, Validation Loss: 17.4579\n",
      "Epoch 1816/6000, Training Loss: 17.4762, Validation Loss: 17.4464\n",
      "Epoch 1817/6000, Training Loss: 17.4647, Validation Loss: 17.4350\n",
      "Epoch 1818/6000, Training Loss: 17.4532, Validation Loss: 17.4236\n",
      "Epoch 1819/6000, Training Loss: 17.4417, Validation Loss: 17.4122\n",
      "Epoch 1820/6000, Training Loss: 17.4303, Validation Loss: 17.4008\n",
      "Epoch 1821/6000, Training Loss: 17.4188, Validation Loss: 17.3894\n",
      "Epoch 1822/6000, Training Loss: 17.4074, Validation Loss: 17.3780\n",
      "Epoch 1823/6000, Training Loss: 17.3959, Validation Loss: 17.3667\n",
      "Epoch 1824/6000, Training Loss: 17.3845, Validation Loss: 17.3553\n",
      "Epoch 1825/6000, Training Loss: 17.3731, Validation Loss: 17.3439\n",
      "Epoch 1826/6000, Training Loss: 17.3617, Validation Loss: 17.3326\n",
      "Epoch 1827/6000, Training Loss: 17.3503, Validation Loss: 17.3213\n",
      "Epoch 1828/6000, Training Loss: 17.3389, Validation Loss: 17.3100\n",
      "Epoch 1829/6000, Training Loss: 17.3275, Validation Loss: 17.2987\n",
      "Epoch 1830/6000, Training Loss: 17.3162, Validation Loss: 17.2874\n",
      "Epoch 1831/6000, Training Loss: 17.3048, Validation Loss: 17.2761\n",
      "Epoch 1832/6000, Training Loss: 17.2935, Validation Loss: 17.2648\n",
      "Epoch 1833/6000, Training Loss: 17.2822, Validation Loss: 17.2535\n",
      "Epoch 1834/6000, Training Loss: 17.2709, Validation Loss: 17.2423\n",
      "Epoch 1835/6000, Training Loss: 17.2596, Validation Loss: 17.2311\n",
      "Epoch 1836/6000, Training Loss: 17.2484, Validation Loss: 17.2199\n",
      "Epoch 1837/6000, Training Loss: 17.2372, Validation Loss: 17.2088\n",
      "Epoch 1838/6000, Training Loss: 17.2260, Validation Loss: 17.1977\n",
      "Epoch 1839/6000, Training Loss: 17.2148, Validation Loss: 17.1866\n",
      "Epoch 1840/6000, Training Loss: 17.2037, Validation Loss: 17.1756\n",
      "Epoch 1841/6000, Training Loss: 17.1926, Validation Loss: 17.1646\n",
      "Epoch 1842/6000, Training Loss: 17.1815, Validation Loss: 17.1536\n",
      "Epoch 1843/6000, Training Loss: 17.1705, Validation Loss: 17.1427\n",
      "Epoch 1844/6000, Training Loss: 17.1594, Validation Loss: 17.1318\n",
      "Epoch 1845/6000, Training Loss: 17.1484, Validation Loss: 17.1209\n",
      "Epoch 1846/6000, Training Loss: 17.1375, Validation Loss: 17.1100\n",
      "Epoch 1847/6000, Training Loss: 17.1265, Validation Loss: 17.0992\n",
      "Epoch 1848/6000, Training Loss: 17.1156, Validation Loss: 17.0884\n",
      "Epoch 1849/6000, Training Loss: 17.1047, Validation Loss: 17.0775\n",
      "Epoch 1850/6000, Training Loss: 17.0938, Validation Loss: 17.0667\n",
      "Epoch 1851/6000, Training Loss: 17.0829, Validation Loss: 17.0559\n",
      "Epoch 1852/6000, Training Loss: 17.0721, Validation Loss: 17.0451\n",
      "Epoch 1853/6000, Training Loss: 17.0612, Validation Loss: 17.0344\n",
      "Epoch 1854/6000, Training Loss: 17.0504, Validation Loss: 17.0236\n",
      "Epoch 1855/6000, Training Loss: 17.0395, Validation Loss: 17.0128\n",
      "Epoch 1856/6000, Training Loss: 17.0287, Validation Loss: 17.0021\n",
      "Epoch 1857/6000, Training Loss: 17.0179, Validation Loss: 16.9913\n",
      "Epoch 1858/6000, Training Loss: 17.0070, Validation Loss: 16.9806\n",
      "Epoch 1859/6000, Training Loss: 16.9963, Validation Loss: 16.9699\n",
      "Epoch 1860/6000, Training Loss: 16.9855, Validation Loss: 16.9592\n",
      "Epoch 1861/6000, Training Loss: 16.9747, Validation Loss: 16.9485\n",
      "Epoch 1862/6000, Training Loss: 16.9639, Validation Loss: 16.9377\n",
      "Epoch 1863/6000, Training Loss: 16.9532, Validation Loss: 16.9270\n",
      "Epoch 1864/6000, Training Loss: 16.9425, Validation Loss: 16.9163\n",
      "Epoch 1865/6000, Training Loss: 16.9317, Validation Loss: 16.9057\n",
      "Epoch 1866/6000, Training Loss: 16.9210, Validation Loss: 16.8950\n",
      "Epoch 1867/6000, Training Loss: 16.9103, Validation Loss: 16.8843\n",
      "Epoch 1868/6000, Training Loss: 16.8996, Validation Loss: 16.8737\n",
      "Epoch 1869/6000, Training Loss: 16.8890, Validation Loss: 16.8631\n",
      "Epoch 1870/6000, Training Loss: 16.8783, Validation Loss: 16.8525\n",
      "Epoch 1871/6000, Training Loss: 16.8677, Validation Loss: 16.8420\n",
      "Epoch 1872/6000, Training Loss: 16.8572, Validation Loss: 16.8315\n",
      "Epoch 1873/6000, Training Loss: 16.8466, Validation Loss: 16.8211\n",
      "Epoch 1874/6000, Training Loss: 16.8361, Validation Loss: 16.8107\n",
      "Epoch 1875/6000, Training Loss: 16.8256, Validation Loss: 16.8003\n",
      "Epoch 1876/6000, Training Loss: 16.8152, Validation Loss: 16.7899\n",
      "Epoch 1877/6000, Training Loss: 16.8048, Validation Loss: 16.7796\n",
      "Epoch 1878/6000, Training Loss: 16.7943, Validation Loss: 16.7693\n",
      "Epoch 1879/6000, Training Loss: 16.7840, Validation Loss: 16.7590\n",
      "Epoch 1880/6000, Training Loss: 16.7736, Validation Loss: 16.7487\n",
      "Epoch 1881/6000, Training Loss: 16.7633, Validation Loss: 16.7384\n",
      "Epoch 1882/6000, Training Loss: 16.7529, Validation Loss: 16.7282\n",
      "Epoch 1883/6000, Training Loss: 16.7426, Validation Loss: 16.7180\n",
      "Epoch 1884/6000, Training Loss: 16.7323, Validation Loss: 16.7077\n",
      "Epoch 1885/6000, Training Loss: 16.7220, Validation Loss: 16.6975\n",
      "Epoch 1886/6000, Training Loss: 16.7118, Validation Loss: 16.6873\n",
      "Epoch 1887/6000, Training Loss: 16.7015, Validation Loss: 16.6771\n",
      "Epoch 1888/6000, Training Loss: 16.6912, Validation Loss: 16.6669\n",
      "Epoch 1889/6000, Training Loss: 16.6810, Validation Loss: 16.6567\n",
      "Epoch 1890/6000, Training Loss: 16.6707, Validation Loss: 16.6466\n",
      "Epoch 1891/6000, Training Loss: 16.6605, Validation Loss: 16.6364\n",
      "Epoch 1892/6000, Training Loss: 16.6503, Validation Loss: 16.6263\n",
      "Epoch 1893/6000, Training Loss: 16.6400, Validation Loss: 16.6161\n",
      "Epoch 1894/6000, Training Loss: 16.6299, Validation Loss: 16.6060\n",
      "Epoch 1895/6000, Training Loss: 16.6197, Validation Loss: 16.5959\n",
      "Epoch 1896/6000, Training Loss: 16.6095, Validation Loss: 16.5858\n",
      "Epoch 1897/6000, Training Loss: 16.5993, Validation Loss: 16.5757\n",
      "Epoch 1898/6000, Training Loss: 16.5892, Validation Loss: 16.5656\n",
      "Epoch 1899/6000, Training Loss: 16.5791, Validation Loss: 16.5555\n",
      "Epoch 1900/6000, Training Loss: 16.5689, Validation Loss: 16.5454\n",
      "Epoch 1901/6000, Training Loss: 16.5588, Validation Loss: 16.5353\n",
      "Epoch 1902/6000, Training Loss: 16.5487, Validation Loss: 16.5253\n",
      "Epoch 1903/6000, Training Loss: 16.5386, Validation Loss: 16.5153\n",
      "Epoch 1904/6000, Training Loss: 16.5286, Validation Loss: 16.5053\n",
      "Epoch 1905/6000, Training Loss: 16.5186, Validation Loss: 16.4954\n",
      "Epoch 1906/6000, Training Loss: 16.5086, Validation Loss: 16.4855\n",
      "Epoch 1907/6000, Training Loss: 16.4986, Validation Loss: 16.4756\n",
      "Epoch 1908/6000, Training Loss: 16.4887, Validation Loss: 16.4658\n",
      "Epoch 1909/6000, Training Loss: 16.4788, Validation Loss: 16.4560\n",
      "Epoch 1910/6000, Training Loss: 16.4689, Validation Loss: 16.4462\n",
      "Epoch 1911/6000, Training Loss: 16.4591, Validation Loss: 16.4364\n",
      "Epoch 1912/6000, Training Loss: 16.4492, Validation Loss: 16.4267\n",
      "Epoch 1913/6000, Training Loss: 16.4394, Validation Loss: 16.4170\n",
      "Epoch 1914/6000, Training Loss: 16.4296, Validation Loss: 16.4073\n",
      "Epoch 1915/6000, Training Loss: 16.4199, Validation Loss: 16.3976\n",
      "Epoch 1916/6000, Training Loss: 16.4101, Validation Loss: 16.3879\n",
      "Epoch 1917/6000, Training Loss: 16.4004, Validation Loss: 16.3782\n",
      "Epoch 1918/6000, Training Loss: 16.3906, Validation Loss: 16.3686\n",
      "Epoch 1919/6000, Training Loss: 16.3809, Validation Loss: 16.3589\n",
      "Epoch 1920/6000, Training Loss: 16.3712, Validation Loss: 16.3492\n",
      "Epoch 1921/6000, Training Loss: 16.3614, Validation Loss: 16.3396\n",
      "Epoch 1922/6000, Training Loss: 16.3517, Validation Loss: 16.3300\n",
      "Epoch 1923/6000, Training Loss: 16.3420, Validation Loss: 16.3203\n",
      "Epoch 1924/6000, Training Loss: 16.3323, Validation Loss: 16.3107\n",
      "Epoch 1925/6000, Training Loss: 16.3227, Validation Loss: 16.3011\n",
      "Epoch 1926/6000, Training Loss: 16.3130, Validation Loss: 16.2915\n",
      "Epoch 1927/6000, Training Loss: 16.3033, Validation Loss: 16.2819\n",
      "Epoch 1928/6000, Training Loss: 16.2937, Validation Loss: 16.2723\n",
      "Epoch 1929/6000, Training Loss: 16.2841, Validation Loss: 16.2628\n",
      "Epoch 1930/6000, Training Loss: 16.2745, Validation Loss: 16.2532\n",
      "Epoch 1931/6000, Training Loss: 16.2649, Validation Loss: 16.2437\n",
      "Epoch 1932/6000, Training Loss: 16.2553, Validation Loss: 16.2341\n",
      "Epoch 1933/6000, Training Loss: 16.2457, Validation Loss: 16.2246\n",
      "Epoch 1934/6000, Training Loss: 16.2361, Validation Loss: 16.2151\n",
      "Epoch 1935/6000, Training Loss: 16.2266, Validation Loss: 16.2056\n",
      "Epoch 1936/6000, Training Loss: 16.2170, Validation Loss: 16.1961\n",
      "Epoch 1937/6000, Training Loss: 16.2075, Validation Loss: 16.1866\n",
      "Epoch 1938/6000, Training Loss: 16.1980, Validation Loss: 16.1772\n",
      "Epoch 1939/6000, Training Loss: 16.1885, Validation Loss: 16.1677\n",
      "Epoch 1940/6000, Training Loss: 16.1790, Validation Loss: 16.1584\n",
      "Epoch 1941/6000, Training Loss: 16.1696, Validation Loss: 16.1490\n",
      "Epoch 1942/6000, Training Loss: 16.1602, Validation Loss: 16.1397\n",
      "Epoch 1943/6000, Training Loss: 16.1508, Validation Loss: 16.1304\n",
      "Epoch 1944/6000, Training Loss: 16.1415, Validation Loss: 16.1212\n",
      "Epoch 1945/6000, Training Loss: 16.1322, Validation Loss: 16.1119\n",
      "Epoch 1946/6000, Training Loss: 16.1229, Validation Loss: 16.1027\n",
      "Epoch 1947/6000, Training Loss: 16.1136, Validation Loss: 16.0935\n",
      "Epoch 1948/6000, Training Loss: 16.1043, Validation Loss: 16.0843\n",
      "Epoch 1949/6000, Training Loss: 16.0951, Validation Loss: 16.0751\n",
      "Epoch 1950/6000, Training Loss: 16.0858, Validation Loss: 16.0660\n",
      "Epoch 1951/6000, Training Loss: 16.0766, Validation Loss: 16.0568\n",
      "Epoch 1952/6000, Training Loss: 16.0674, Validation Loss: 16.0477\n",
      "Epoch 1953/6000, Training Loss: 16.0582, Validation Loss: 16.0386\n",
      "Epoch 1954/6000, Training Loss: 16.0491, Validation Loss: 16.0295\n",
      "Epoch 1955/6000, Training Loss: 16.0399, Validation Loss: 16.0203\n",
      "Epoch 1956/6000, Training Loss: 16.0307, Validation Loss: 16.0112\n",
      "Epoch 1957/6000, Training Loss: 16.0215, Validation Loss: 16.0021\n",
      "Epoch 1958/6000, Training Loss: 16.0124, Validation Loss: 15.9930\n",
      "Epoch 1959/6000, Training Loss: 16.0032, Validation Loss: 15.9839\n",
      "Epoch 1960/6000, Training Loss: 15.9941, Validation Loss: 15.9749\n",
      "Epoch 1961/6000, Training Loss: 15.9850, Validation Loss: 15.9658\n",
      "Epoch 1962/6000, Training Loss: 15.9759, Validation Loss: 15.9567\n",
      "Epoch 1963/6000, Training Loss: 15.9668, Validation Loss: 15.9477\n",
      "Epoch 1964/6000, Training Loss: 15.9577, Validation Loss: 15.9387\n",
      "Epoch 1965/6000, Training Loss: 15.9486, Validation Loss: 15.9296\n",
      "Epoch 1966/6000, Training Loss: 15.9395, Validation Loss: 15.9206\n",
      "Epoch 1967/6000, Training Loss: 15.9305, Validation Loss: 15.9116\n",
      "Epoch 1968/6000, Training Loss: 15.9214, Validation Loss: 15.9026\n",
      "Epoch 1969/6000, Training Loss: 15.9124, Validation Loss: 15.8936\n",
      "Epoch 1970/6000, Training Loss: 15.9034, Validation Loss: 15.8846\n",
      "Epoch 1971/6000, Training Loss: 15.8943, Validation Loss: 15.8757\n",
      "Epoch 1972/6000, Training Loss: 15.8853, Validation Loss: 15.8667\n",
      "Epoch 1973/6000, Training Loss: 15.8764, Validation Loss: 15.8578\n",
      "Epoch 1974/6000, Training Loss: 15.8674, Validation Loss: 15.8489\n",
      "Epoch 1975/6000, Training Loss: 15.8585, Validation Loss: 15.8400\n",
      "Epoch 1976/6000, Training Loss: 15.8496, Validation Loss: 15.8312\n",
      "Epoch 1977/6000, Training Loss: 15.8407, Validation Loss: 15.8224\n",
      "Epoch 1978/6000, Training Loss: 15.8319, Validation Loss: 15.8136\n",
      "Epoch 1979/6000, Training Loss: 15.8231, Validation Loss: 15.8049\n",
      "Epoch 1980/6000, Training Loss: 15.8143, Validation Loss: 15.7962\n",
      "Epoch 1981/6000, Training Loss: 15.8055, Validation Loss: 15.7875\n",
      "Epoch 1982/6000, Training Loss: 15.7968, Validation Loss: 15.7788\n",
      "Epoch 1983/6000, Training Loss: 15.7880, Validation Loss: 15.7701\n",
      "Epoch 1984/6000, Training Loss: 15.7793, Validation Loss: 15.7615\n",
      "Epoch 1985/6000, Training Loss: 15.7706, Validation Loss: 15.7529\n",
      "Epoch 1986/6000, Training Loss: 15.7619, Validation Loss: 15.7442\n",
      "Epoch 1987/6000, Training Loss: 15.7532, Validation Loss: 15.7356\n",
      "Epoch 1988/6000, Training Loss: 15.7446, Validation Loss: 15.7270\n",
      "Epoch 1989/6000, Training Loss: 15.7359, Validation Loss: 15.7184\n",
      "Epoch 1990/6000, Training Loss: 15.7273, Validation Loss: 15.7098\n",
      "Epoch 1991/6000, Training Loss: 15.7186, Validation Loss: 15.7013\n",
      "Epoch 1992/6000, Training Loss: 15.7100, Validation Loss: 15.6927\n",
      "Epoch 1993/6000, Training Loss: 15.7013, Validation Loss: 15.6841\n",
      "Epoch 1994/6000, Training Loss: 15.6927, Validation Loss: 15.6755\n",
      "Epoch 1995/6000, Training Loss: 15.6841, Validation Loss: 15.6670\n",
      "Epoch 1996/6000, Training Loss: 15.6755, Validation Loss: 15.6584\n",
      "Epoch 1997/6000, Training Loss: 15.6669, Validation Loss: 15.6499\n",
      "Epoch 1998/6000, Training Loss: 15.6583, Validation Loss: 15.6414\n",
      "Epoch 1999/6000, Training Loss: 15.6497, Validation Loss: 15.6328\n",
      "Epoch 2000/6000, Training Loss: 15.6411, Validation Loss: 15.6243\n",
      "Epoch 2001/6000, Training Loss: 15.6326, Validation Loss: 15.6158\n",
      "Epoch 2002/6000, Training Loss: 15.6240, Validation Loss: 15.6073\n",
      "Epoch 2003/6000, Training Loss: 15.6155, Validation Loss: 15.5988\n",
      "Epoch 2004/6000, Training Loss: 15.6070, Validation Loss: 15.5903\n",
      "Epoch 2005/6000, Training Loss: 15.5985, Validation Loss: 15.5818\n",
      "Epoch 2006/6000, Training Loss: 15.5899, Validation Loss: 15.5733\n",
      "Epoch 2007/6000, Training Loss: 15.5814, Validation Loss: 15.5649\n",
      "Epoch 2008/6000, Training Loss: 15.5729, Validation Loss: 15.5564\n",
      "Epoch 2009/6000, Training Loss: 15.5645, Validation Loss: 15.5480\n",
      "Epoch 2010/6000, Training Loss: 15.5560, Validation Loss: 15.5396\n",
      "Epoch 2011/6000, Training Loss: 15.5476, Validation Loss: 15.5312\n",
      "Epoch 2012/6000, Training Loss: 15.5392, Validation Loss: 15.5229\n",
      "Epoch 2013/6000, Training Loss: 15.5309, Validation Loss: 15.5146\n",
      "Epoch 2014/6000, Training Loss: 15.5225, Validation Loss: 15.5063\n",
      "Epoch 2015/6000, Training Loss: 15.5142, Validation Loss: 15.4980\n",
      "Epoch 2016/6000, Training Loss: 15.5059, Validation Loss: 15.4898\n",
      "Epoch 2017/6000, Training Loss: 15.4977, Validation Loss: 15.4816\n",
      "Epoch 2018/6000, Training Loss: 15.4894, Validation Loss: 15.4734\n",
      "Epoch 2019/6000, Training Loss: 15.4812, Validation Loss: 15.4652\n",
      "Epoch 2020/6000, Training Loss: 15.4730, Validation Loss: 15.4571\n",
      "Epoch 2021/6000, Training Loss: 15.4648, Validation Loss: 15.4489\n",
      "Epoch 2022/6000, Training Loss: 15.4566, Validation Loss: 15.4408\n",
      "Epoch 2023/6000, Training Loss: 15.4484, Validation Loss: 15.4327\n",
      "Epoch 2024/6000, Training Loss: 15.4403, Validation Loss: 15.4246\n",
      "Epoch 2025/6000, Training Loss: 15.4321, Validation Loss: 15.4165\n",
      "Epoch 2026/6000, Training Loss: 15.4239, Validation Loss: 15.4084\n",
      "Epoch 2027/6000, Training Loss: 15.4158, Validation Loss: 15.4003\n",
      "Epoch 2028/6000, Training Loss: 15.4077, Validation Loss: 15.3922\n",
      "Epoch 2029/6000, Training Loss: 15.3995, Validation Loss: 15.3841\n",
      "Epoch 2030/6000, Training Loss: 15.3914, Validation Loss: 15.3761\n",
      "Epoch 2031/6000, Training Loss: 15.3833, Validation Loss: 15.3680\n",
      "Epoch 2032/6000, Training Loss: 15.3751, Validation Loss: 15.3599\n",
      "Epoch 2033/6000, Training Loss: 15.3670, Validation Loss: 15.3519\n",
      "Epoch 2034/6000, Training Loss: 15.3589, Validation Loss: 15.3439\n",
      "Epoch 2035/6000, Training Loss: 15.3509, Validation Loss: 15.3358\n",
      "Epoch 2036/6000, Training Loss: 15.3428, Validation Loss: 15.3278\n",
      "Epoch 2037/6000, Training Loss: 15.3347, Validation Loss: 15.3198\n",
      "Epoch 2038/6000, Training Loss: 15.3267, Validation Loss: 15.3118\n",
      "Epoch 2039/6000, Training Loss: 15.3187, Validation Loss: 15.3038\n",
      "Epoch 2040/6000, Training Loss: 15.3106, Validation Loss: 15.2959\n",
      "Epoch 2041/6000, Training Loss: 15.3026, Validation Loss: 15.2879\n",
      "Epoch 2042/6000, Training Loss: 15.2946, Validation Loss: 15.2799\n",
      "Epoch 2043/6000, Training Loss: 15.2866, Validation Loss: 15.2719\n",
      "Epoch 2044/6000, Training Loss: 15.2786, Validation Loss: 15.2640\n",
      "Epoch 2045/6000, Training Loss: 15.2706, Validation Loss: 15.2561\n",
      "Epoch 2046/6000, Training Loss: 15.2627, Validation Loss: 15.2481\n",
      "Epoch 2047/6000, Training Loss: 15.2547, Validation Loss: 15.2402\n",
      "Epoch 2048/6000, Training Loss: 15.2468, Validation Loss: 15.2323\n",
      "Epoch 2049/6000, Training Loss: 15.2389, Validation Loss: 15.2245\n",
      "Epoch 2050/6000, Training Loss: 15.2310, Validation Loss: 15.2167\n",
      "Epoch 2051/6000, Training Loss: 15.2231, Validation Loss: 15.2089\n",
      "Epoch 2052/6000, Training Loss: 15.2153, Validation Loss: 15.2011\n",
      "Epoch 2053/6000, Training Loss: 15.2075, Validation Loss: 15.1934\n",
      "Epoch 2054/6000, Training Loss: 15.1998, Validation Loss: 15.1857\n",
      "Epoch 2055/6000, Training Loss: 15.1920, Validation Loss: 15.1780\n",
      "Epoch 2056/6000, Training Loss: 15.1843, Validation Loss: 15.1703\n",
      "Epoch 2057/6000, Training Loss: 15.1765, Validation Loss: 15.1627\n",
      "Epoch 2058/6000, Training Loss: 15.1688, Validation Loss: 15.1550\n",
      "Epoch 2059/6000, Training Loss: 15.1611, Validation Loss: 15.1474\n",
      "Epoch 2060/6000, Training Loss: 15.1535, Validation Loss: 15.1398\n",
      "Epoch 2061/6000, Training Loss: 15.1458, Validation Loss: 15.1322\n",
      "Epoch 2062/6000, Training Loss: 15.1381, Validation Loss: 15.1245\n",
      "Epoch 2063/6000, Training Loss: 15.1304, Validation Loss: 15.1169\n",
      "Epoch 2064/6000, Training Loss: 15.1228, Validation Loss: 15.1094\n",
      "Epoch 2065/6000, Training Loss: 15.1151, Validation Loss: 15.1018\n",
      "Epoch 2066/6000, Training Loss: 15.1075, Validation Loss: 15.0942\n",
      "Epoch 2067/6000, Training Loss: 15.0998, Validation Loss: 15.0866\n",
      "Epoch 2068/6000, Training Loss: 15.0922, Validation Loss: 15.0790\n",
      "Epoch 2069/6000, Training Loss: 15.0846, Validation Loss: 15.0715\n",
      "Epoch 2070/6000, Training Loss: 15.0769, Validation Loss: 15.0639\n",
      "Epoch 2071/6000, Training Loss: 15.0693, Validation Loss: 15.0564\n",
      "Epoch 2072/6000, Training Loss: 15.0617, Validation Loss: 15.0489\n",
      "Epoch 2073/6000, Training Loss: 15.0542, Validation Loss: 15.0413\n",
      "Epoch 2074/6000, Training Loss: 15.0466, Validation Loss: 15.0338\n",
      "Epoch 2075/6000, Training Loss: 15.0390, Validation Loss: 15.0263\n",
      "Epoch 2076/6000, Training Loss: 15.0315, Validation Loss: 15.0188\n",
      "Epoch 2077/6000, Training Loss: 15.0240, Validation Loss: 15.0113\n",
      "Epoch 2078/6000, Training Loss: 15.0164, Validation Loss: 15.0038\n",
      "Epoch 2079/6000, Training Loss: 15.0089, Validation Loss: 14.9964\n",
      "Epoch 2080/6000, Training Loss: 15.0014, Validation Loss: 14.9889\n",
      "Epoch 2081/6000, Training Loss: 14.9939, Validation Loss: 14.9814\n",
      "Epoch 2082/6000, Training Loss: 14.9864, Validation Loss: 14.9739\n",
      "Epoch 2083/6000, Training Loss: 14.9789, Validation Loss: 14.9665\n",
      "Epoch 2084/6000, Training Loss: 14.9714, Validation Loss: 14.9590\n",
      "Epoch 2085/6000, Training Loss: 14.9640, Validation Loss: 14.9516\n",
      "Epoch 2086/6000, Training Loss: 14.9565, Validation Loss: 14.9442\n",
      "Epoch 2087/6000, Training Loss: 14.9491, Validation Loss: 14.9369\n",
      "Epoch 2088/6000, Training Loss: 14.9417, Validation Loss: 14.9295\n",
      "Epoch 2089/6000, Training Loss: 14.9344, Validation Loss: 14.9222\n",
      "Epoch 2090/6000, Training Loss: 14.9271, Validation Loss: 14.9149\n",
      "Epoch 2091/6000, Training Loss: 14.9197, Validation Loss: 14.9077\n",
      "Epoch 2092/6000, Training Loss: 14.9125, Validation Loss: 14.9004\n",
      "Epoch 2093/6000, Training Loss: 14.9052, Validation Loss: 14.8932\n",
      "Epoch 2094/6000, Training Loss: 14.8979, Validation Loss: 14.8860\n",
      "Epoch 2095/6000, Training Loss: 14.8907, Validation Loss: 14.8788\n",
      "Epoch 2096/6000, Training Loss: 14.8835, Validation Loss: 14.8717\n",
      "Epoch 2097/6000, Training Loss: 14.8763, Validation Loss: 14.8645\n",
      "Epoch 2098/6000, Training Loss: 14.8691, Validation Loss: 14.8574\n",
      "Epoch 2099/6000, Training Loss: 14.8619, Validation Loss: 14.8502\n",
      "Epoch 2100/6000, Training Loss: 14.8547, Validation Loss: 14.8431\n",
      "Epoch 2101/6000, Training Loss: 14.8475, Validation Loss: 14.8359\n",
      "Epoch 2102/6000, Training Loss: 14.8404, Validation Loss: 14.8288\n",
      "Epoch 2103/6000, Training Loss: 14.8332, Validation Loss: 14.8217\n",
      "Epoch 2104/6000, Training Loss: 14.8260, Validation Loss: 14.8145\n",
      "Epoch 2105/6000, Training Loss: 14.8189, Validation Loss: 14.8074\n",
      "Epoch 2106/6000, Training Loss: 14.8117, Validation Loss: 14.8003\n",
      "Epoch 2107/6000, Training Loss: 14.8046, Validation Loss: 14.7932\n",
      "Epoch 2108/6000, Training Loss: 14.7974, Validation Loss: 14.7861\n",
      "Epoch 2109/6000, Training Loss: 14.7903, Validation Loss: 14.7790\n",
      "Epoch 2110/6000, Training Loss: 14.7832, Validation Loss: 14.7719\n",
      "Epoch 2111/6000, Training Loss: 14.7761, Validation Loss: 14.7649\n",
      "Epoch 2112/6000, Training Loss: 14.7690, Validation Loss: 14.7578\n",
      "Epoch 2113/6000, Training Loss: 14.7619, Validation Loss: 14.7508\n",
      "Epoch 2114/6000, Training Loss: 14.7548, Validation Loss: 14.7438\n",
      "Epoch 2115/6000, Training Loss: 14.7477, Validation Loss: 14.7367\n",
      "Epoch 2116/6000, Training Loss: 14.7407, Validation Loss: 14.7297\n",
      "Epoch 2117/6000, Training Loss: 14.7336, Validation Loss: 14.7227\n",
      "Epoch 2118/6000, Training Loss: 14.7266, Validation Loss: 14.7157\n",
      "Epoch 2119/6000, Training Loss: 14.7196, Validation Loss: 14.7088\n",
      "Epoch 2120/6000, Training Loss: 14.7125, Validation Loss: 14.7018\n",
      "Epoch 2121/6000, Training Loss: 14.7055, Validation Loss: 14.6948\n",
      "Epoch 2122/6000, Training Loss: 14.6985, Validation Loss: 14.6879\n",
      "Epoch 2123/6000, Training Loss: 14.6915, Validation Loss: 14.6809\n",
      "Epoch 2124/6000, Training Loss: 14.6845, Validation Loss: 14.6740\n",
      "Epoch 2125/6000, Training Loss: 14.6776, Validation Loss: 14.6671\n",
      "Epoch 2126/6000, Training Loss: 14.6706, Validation Loss: 14.6602\n",
      "Epoch 2127/6000, Training Loss: 14.6637, Validation Loss: 14.6534\n",
      "Epoch 2128/6000, Training Loss: 14.6568, Validation Loss: 14.6465\n",
      "Epoch 2129/6000, Training Loss: 14.6500, Validation Loss: 14.6397\n",
      "Epoch 2130/6000, Training Loss: 14.6432, Validation Loss: 14.6330\n",
      "Epoch 2131/6000, Training Loss: 14.6363, Validation Loss: 14.6262\n",
      "Epoch 2132/6000, Training Loss: 14.6295, Validation Loss: 14.6194\n",
      "Epoch 2133/6000, Training Loss: 14.6228, Validation Loss: 14.6127\n",
      "Epoch 2134/6000, Training Loss: 14.6160, Validation Loss: 14.6060\n",
      "Epoch 2135/6000, Training Loss: 14.6092, Validation Loss: 14.5993\n",
      "Epoch 2136/6000, Training Loss: 14.6025, Validation Loss: 14.5926\n",
      "Epoch 2137/6000, Training Loss: 14.5958, Validation Loss: 14.5859\n",
      "Epoch 2138/6000, Training Loss: 14.5891, Validation Loss: 14.5792\n",
      "Epoch 2139/6000, Training Loss: 14.5823, Validation Loss: 14.5725\n",
      "Epoch 2140/6000, Training Loss: 14.5756, Validation Loss: 14.5659\n",
      "Epoch 2141/6000, Training Loss: 14.5689, Validation Loss: 14.5592\n",
      "Epoch 2142/6000, Training Loss: 14.5622, Validation Loss: 14.5526\n",
      "Epoch 2143/6000, Training Loss: 14.5555, Validation Loss: 14.5459\n",
      "Epoch 2144/6000, Training Loss: 14.5488, Validation Loss: 14.5392\n",
      "Epoch 2145/6000, Training Loss: 14.5421, Validation Loss: 14.5326\n",
      "Epoch 2146/6000, Training Loss: 14.5355, Validation Loss: 14.5260\n",
      "Epoch 2147/6000, Training Loss: 14.5288, Validation Loss: 14.5193\n",
      "Epoch 2148/6000, Training Loss: 14.5221, Validation Loss: 14.5127\n",
      "Epoch 2149/6000, Training Loss: 14.5154, Validation Loss: 14.5061\n",
      "Epoch 2150/6000, Training Loss: 14.5088, Validation Loss: 14.4995\n",
      "Epoch 2151/6000, Training Loss: 14.5022, Validation Loss: 14.4929\n",
      "Epoch 2152/6000, Training Loss: 14.4955, Validation Loss: 14.4863\n",
      "Epoch 2153/6000, Training Loss: 14.4889, Validation Loss: 14.4798\n",
      "Epoch 2154/6000, Training Loss: 14.4823, Validation Loss: 14.4732\n",
      "Epoch 2155/6000, Training Loss: 14.4757, Validation Loss: 14.4667\n",
      "Epoch 2156/6000, Training Loss: 14.4691, Validation Loss: 14.4601\n",
      "Epoch 2157/6000, Training Loss: 14.4625, Validation Loss: 14.4536\n",
      "Epoch 2158/6000, Training Loss: 14.4560, Validation Loss: 14.4471\n",
      "Epoch 2159/6000, Training Loss: 14.4494, Validation Loss: 14.4406\n",
      "Epoch 2160/6000, Training Loss: 14.4429, Validation Loss: 14.4341\n",
      "Epoch 2161/6000, Training Loss: 14.4363, Validation Loss: 14.4276\n",
      "Epoch 2162/6000, Training Loss: 14.4298, Validation Loss: 14.4211\n",
      "Epoch 2163/6000, Training Loss: 14.4232, Validation Loss: 14.4146\n",
      "Epoch 2164/6000, Training Loss: 14.4167, Validation Loss: 14.4081\n",
      "Epoch 2165/6000, Training Loss: 14.4102, Validation Loss: 14.4017\n",
      "Epoch 2166/6000, Training Loss: 14.4037, Validation Loss: 14.3952\n",
      "Epoch 2167/6000, Training Loss: 14.3973, Validation Loss: 14.3888\n",
      "Epoch 2168/6000, Training Loss: 14.3908, Validation Loss: 14.3825\n",
      "Epoch 2169/6000, Training Loss: 14.3844, Validation Loss: 14.3761\n",
      "Epoch 2170/6000, Training Loss: 14.3781, Validation Loss: 14.3698\n",
      "Epoch 2171/6000, Training Loss: 14.3717, Validation Loss: 14.3635\n",
      "Epoch 2172/6000, Training Loss: 14.3654, Validation Loss: 14.3572\n",
      "Epoch 2173/6000, Training Loss: 14.3590, Validation Loss: 14.3509\n",
      "Epoch 2174/6000, Training Loss: 14.3527, Validation Loss: 14.3446\n",
      "Epoch 2175/6000, Training Loss: 14.3464, Validation Loss: 14.3384\n",
      "Epoch 2176/6000, Training Loss: 14.3402, Validation Loss: 14.3321\n",
      "Epoch 2177/6000, Training Loss: 14.3339, Validation Loss: 14.3259\n",
      "Epoch 2178/6000, Training Loss: 14.3276, Validation Loss: 14.3197\n",
      "Epoch 2179/6000, Training Loss: 14.3214, Validation Loss: 14.3135\n",
      "Epoch 2180/6000, Training Loss: 14.3151, Validation Loss: 14.3072\n",
      "Epoch 2181/6000, Training Loss: 14.3088, Validation Loss: 14.3010\n",
      "Epoch 2182/6000, Training Loss: 14.3026, Validation Loss: 14.2948\n",
      "Epoch 2183/6000, Training Loss: 14.2964, Validation Loss: 14.2886\n",
      "Epoch 2184/6000, Training Loss: 14.2901, Validation Loss: 14.2824\n",
      "Epoch 2185/6000, Training Loss: 14.2839, Validation Loss: 14.2762\n",
      "Epoch 2186/6000, Training Loss: 14.2777, Validation Loss: 14.2700\n",
      "Epoch 2187/6000, Training Loss: 14.2714, Validation Loss: 14.2638\n",
      "Epoch 2188/6000, Training Loss: 14.2652, Validation Loss: 14.2577\n",
      "Epoch 2189/6000, Training Loss: 14.2590, Validation Loss: 14.2515\n",
      "Epoch 2190/6000, Training Loss: 14.2528, Validation Loss: 14.2453\n",
      "Epoch 2191/6000, Training Loss: 14.2466, Validation Loss: 14.2392\n",
      "Epoch 2192/6000, Training Loss: 14.2404, Validation Loss: 14.2331\n",
      "Epoch 2193/6000, Training Loss: 14.2342, Validation Loss: 14.2269\n",
      "Epoch 2194/6000, Training Loss: 14.2280, Validation Loss: 14.2208\n",
      "Epoch 2195/6000, Training Loss: 14.2219, Validation Loss: 14.2147\n",
      "Epoch 2196/6000, Training Loss: 14.2157, Validation Loss: 14.2086\n",
      "Epoch 2197/6000, Training Loss: 14.2096, Validation Loss: 14.2026\n",
      "Epoch 2198/6000, Training Loss: 14.2035, Validation Loss: 14.1965\n",
      "Epoch 2199/6000, Training Loss: 14.1974, Validation Loss: 14.1904\n",
      "Epoch 2200/6000, Training Loss: 14.1912, Validation Loss: 14.1844\n",
      "Epoch 2201/6000, Training Loss: 14.1851, Validation Loss: 14.1783\n",
      "Epoch 2202/6000, Training Loss: 14.1790, Validation Loss: 14.1723\n",
      "Epoch 2203/6000, Training Loss: 14.1729, Validation Loss: 14.1662\n",
      "Epoch 2204/6000, Training Loss: 14.1669, Validation Loss: 14.1602\n",
      "Epoch 2205/6000, Training Loss: 14.1608, Validation Loss: 14.1542\n",
      "Epoch 2206/6000, Training Loss: 14.1547, Validation Loss: 14.1481\n",
      "Epoch 2207/6000, Training Loss: 14.1487, Validation Loss: 14.1422\n",
      "Epoch 2208/6000, Training Loss: 14.1427, Validation Loss: 14.1362\n",
      "Epoch 2209/6000, Training Loss: 14.1367, Validation Loss: 14.1302\n",
      "Epoch 2210/6000, Training Loss: 14.1307, Validation Loss: 14.1243\n",
      "Epoch 2211/6000, Training Loss: 14.1248, Validation Loss: 14.1184\n",
      "Epoch 2212/6000, Training Loss: 14.1189, Validation Loss: 14.1125\n",
      "Epoch 2213/6000, Training Loss: 14.1130, Validation Loss: 14.1067\n",
      "Epoch 2214/6000, Training Loss: 14.1071, Validation Loss: 14.1008\n",
      "Epoch 2215/6000, Training Loss: 14.1012, Validation Loss: 14.0950\n",
      "Epoch 2216/6000, Training Loss: 14.0954, Validation Loss: 14.0891\n",
      "Epoch 2217/6000, Training Loss: 14.0895, Validation Loss: 14.0833\n",
      "Epoch 2218/6000, Training Loss: 14.0837, Validation Loss: 14.0775\n",
      "Epoch 2219/6000, Training Loss: 14.0778, Validation Loss: 14.0717\n",
      "Epoch 2220/6000, Training Loss: 14.0720, Validation Loss: 14.0659\n",
      "Epoch 2221/6000, Training Loss: 14.0662, Validation Loss: 14.0601\n",
      "Epoch 2222/6000, Training Loss: 14.0604, Validation Loss: 14.0543\n",
      "Epoch 2223/6000, Training Loss: 14.0545, Validation Loss: 14.0485\n",
      "Epoch 2224/6000, Training Loss: 14.0487, Validation Loss: 14.0427\n",
      "Epoch 2225/6000, Training Loss: 14.0429, Validation Loss: 14.0369\n",
      "Epoch 2226/6000, Training Loss: 14.0371, Validation Loss: 14.0312\n",
      "Epoch 2227/6000, Training Loss: 14.0313, Validation Loss: 14.0254\n",
      "Epoch 2228/6000, Training Loss: 14.0255, Validation Loss: 14.0196\n",
      "Epoch 2229/6000, Training Loss: 14.0197, Validation Loss: 14.0139\n",
      "Epoch 2230/6000, Training Loss: 14.0140, Validation Loss: 14.0082\n",
      "Epoch 2231/6000, Training Loss: 14.0082, Validation Loss: 14.0024\n",
      "Epoch 2232/6000, Training Loss: 14.0024, Validation Loss: 13.9967\n",
      "Epoch 2233/6000, Training Loss: 13.9967, Validation Loss: 13.9910\n",
      "Epoch 2234/6000, Training Loss: 13.9909, Validation Loss: 13.9853\n",
      "Epoch 2235/6000, Training Loss: 13.9852, Validation Loss: 13.9797\n",
      "Epoch 2236/6000, Training Loss: 13.9795, Validation Loss: 13.9740\n",
      "Epoch 2237/6000, Training Loss: 13.9738, Validation Loss: 13.9683\n",
      "Epoch 2238/6000, Training Loss: 13.9681, Validation Loss: 13.9627\n",
      "Epoch 2239/6000, Training Loss: 13.9624, Validation Loss: 13.9571\n",
      "Epoch 2240/6000, Training Loss: 13.9567, Validation Loss: 13.9514\n",
      "Epoch 2241/6000, Training Loss: 13.9510, Validation Loss: 13.9458\n",
      "Epoch 2242/6000, Training Loss: 13.9454, Validation Loss: 13.9402\n",
      "Epoch 2243/6000, Training Loss: 13.9397, Validation Loss: 13.9345\n",
      "Epoch 2244/6000, Training Loss: 13.9340, Validation Loss: 13.9289\n",
      "Epoch 2245/6000, Training Loss: 13.9284, Validation Loss: 13.9233\n",
      "Epoch 2246/6000, Training Loss: 13.9227, Validation Loss: 13.9176\n",
      "Epoch 2247/6000, Training Loss: 13.9171, Validation Loss: 13.9120\n",
      "Epoch 2248/6000, Training Loss: 13.9115, Validation Loss: 13.9064\n",
      "Epoch 2249/6000, Training Loss: 13.9059, Validation Loss: 13.9008\n",
      "Epoch 2250/6000, Training Loss: 13.9003, Validation Loss: 13.8953\n",
      "Epoch 2251/6000, Training Loss: 13.8947, Validation Loss: 13.8897\n",
      "Epoch 2252/6000, Training Loss: 13.8892, Validation Loss: 13.8842\n",
      "Epoch 2253/6000, Training Loss: 13.8837, Validation Loss: 13.8787\n",
      "Epoch 2254/6000, Training Loss: 13.8782, Validation Loss: 13.8732\n",
      "Epoch 2255/6000, Training Loss: 13.8727, Validation Loss: 13.8677\n",
      "Epoch 2256/6000, Training Loss: 13.8672, Validation Loss: 13.8623\n",
      "Epoch 2257/6000, Training Loss: 13.8618, Validation Loss: 13.8568\n",
      "Epoch 2258/6000, Training Loss: 13.8564, Validation Loss: 13.8514\n",
      "Epoch 2259/6000, Training Loss: 13.8509, Validation Loss: 13.8460\n",
      "Epoch 2260/6000, Training Loss: 13.8455, Validation Loss: 13.8406\n",
      "Epoch 2261/6000, Training Loss: 13.8401, Validation Loss: 13.8352\n",
      "Epoch 2262/6000, Training Loss: 13.8347, Validation Loss: 13.8298\n",
      "Epoch 2263/6000, Training Loss: 13.8293, Validation Loss: 13.8244\n",
      "Epoch 2264/6000, Training Loss: 13.8239, Validation Loss: 13.8191\n",
      "Epoch 2265/6000, Training Loss: 13.8185, Validation Loss: 13.8137\n",
      "Epoch 2266/6000, Training Loss: 13.8131, Validation Loss: 13.8083\n",
      "Epoch 2267/6000, Training Loss: 13.8078, Validation Loss: 13.8030\n",
      "Epoch 2268/6000, Training Loss: 13.8024, Validation Loss: 13.7976\n",
      "Epoch 2269/6000, Training Loss: 13.7970, Validation Loss: 13.7923\n",
      "Epoch 2270/6000, Training Loss: 13.7916, Validation Loss: 13.7869\n",
      "Epoch 2271/6000, Training Loss: 13.7863, Validation Loss: 13.7816\n",
      "Epoch 2272/6000, Training Loss: 13.7809, Validation Loss: 13.7762\n",
      "Epoch 2273/6000, Training Loss: 13.7756, Validation Loss: 13.7709\n",
      "Epoch 2274/6000, Training Loss: 13.7702, Validation Loss: 13.7656\n",
      "Epoch 2275/6000, Training Loss: 13.7649, Validation Loss: 13.7603\n",
      "Epoch 2276/6000, Training Loss: 13.7596, Validation Loss: 13.7550\n",
      "Epoch 2277/6000, Training Loss: 13.7543, Validation Loss: 13.7498\n",
      "Epoch 2278/6000, Training Loss: 13.7490, Validation Loss: 13.7445\n",
      "Epoch 2279/6000, Training Loss: 13.7437, Validation Loss: 13.7393\n",
      "Epoch 2280/6000, Training Loss: 13.7384, Validation Loss: 13.7340\n",
      "Epoch 2281/6000, Training Loss: 13.7331, Validation Loss: 13.7288\n",
      "Epoch 2282/6000, Training Loss: 13.7279, Validation Loss: 13.7236\n",
      "Epoch 2283/6000, Training Loss: 13.7226, Validation Loss: 13.7184\n",
      "Epoch 2284/6000, Training Loss: 13.7173, Validation Loss: 13.7131\n",
      "Epoch 2285/6000, Training Loss: 13.7121, Validation Loss: 13.7079\n",
      "Epoch 2286/6000, Training Loss: 13.7069, Validation Loss: 13.7027\n",
      "Epoch 2287/6000, Training Loss: 13.7016, Validation Loss: 13.6975\n",
      "Epoch 2288/6000, Training Loss: 13.6964, Validation Loss: 13.6923\n",
      "Epoch 2289/6000, Training Loss: 13.6912, Validation Loss: 13.6871\n",
      "Epoch 2290/6000, Training Loss: 13.6859, Validation Loss: 13.6819\n",
      "Epoch 2291/6000, Training Loss: 13.6807, Validation Loss: 13.6767\n",
      "Epoch 2292/6000, Training Loss: 13.6756, Validation Loss: 13.6715\n",
      "Epoch 2293/6000, Training Loss: 13.6704, Validation Loss: 13.6663\n",
      "Epoch 2294/6000, Training Loss: 13.6652, Validation Loss: 13.6611\n",
      "Epoch 2295/6000, Training Loss: 13.6601, Validation Loss: 13.6560\n",
      "Epoch 2296/6000, Training Loss: 13.6550, Validation Loss: 13.6509\n",
      "Epoch 2297/6000, Training Loss: 13.6499, Validation Loss: 13.6458\n",
      "Epoch 2298/6000, Training Loss: 13.6448, Validation Loss: 13.6408\n",
      "Epoch 2299/6000, Training Loss: 13.6398, Validation Loss: 13.6357\n",
      "Epoch 2300/6000, Training Loss: 13.6347, Validation Loss: 13.6307\n",
      "Epoch 2301/6000, Training Loss: 13.6297, Validation Loss: 13.6256\n",
      "Epoch 2302/6000, Training Loss: 13.6247, Validation Loss: 13.6206\n",
      "Epoch 2303/6000, Training Loss: 13.6197, Validation Loss: 13.6156\n",
      "Epoch 2304/6000, Training Loss: 13.6147, Validation Loss: 13.6107\n",
      "Epoch 2305/6000, Training Loss: 13.6097, Validation Loss: 13.6057\n",
      "Epoch 2306/6000, Training Loss: 13.6047, Validation Loss: 13.6007\n",
      "Epoch 2307/6000, Training Loss: 13.5997, Validation Loss: 13.5957\n",
      "Epoch 2308/6000, Training Loss: 13.5948, Validation Loss: 13.5908\n",
      "Epoch 2309/6000, Training Loss: 13.5898, Validation Loss: 13.5858\n",
      "Epoch 2310/6000, Training Loss: 13.5848, Validation Loss: 13.5809\n",
      "Epoch 2311/6000, Training Loss: 13.5799, Validation Loss: 13.5759\n",
      "Epoch 2312/6000, Training Loss: 13.5749, Validation Loss: 13.5710\n",
      "Epoch 2313/6000, Training Loss: 13.5699, Validation Loss: 13.5660\n",
      "Epoch 2314/6000, Training Loss: 13.5650, Validation Loss: 13.5611\n",
      "Epoch 2315/6000, Training Loss: 13.5600, Validation Loss: 13.5562\n",
      "Epoch 2316/6000, Training Loss: 13.5551, Validation Loss: 13.5513\n",
      "Epoch 2317/6000, Training Loss: 13.5502, Validation Loss: 13.5463\n",
      "Epoch 2318/6000, Training Loss: 13.5452, Validation Loss: 13.5414\n",
      "Epoch 2319/6000, Training Loss: 13.5403, Validation Loss: 13.5366\n",
      "Epoch 2320/6000, Training Loss: 13.5354, Validation Loss: 13.5317\n",
      "Epoch 2321/6000, Training Loss: 13.5305, Validation Loss: 13.5268\n",
      "Epoch 2322/6000, Training Loss: 13.5256, Validation Loss: 13.5220\n",
      "Epoch 2323/6000, Training Loss: 13.5207, Validation Loss: 13.5171\n",
      "Epoch 2324/6000, Training Loss: 13.5158, Validation Loss: 13.5123\n",
      "Epoch 2325/6000, Training Loss: 13.5110, Validation Loss: 13.5075\n",
      "Epoch 2326/6000, Training Loss: 13.5061, Validation Loss: 13.5027\n",
      "Epoch 2327/6000, Training Loss: 13.5012, Validation Loss: 13.4979\n",
      "Epoch 2328/6000, Training Loss: 13.4964, Validation Loss: 13.4931\n",
      "Epoch 2329/6000, Training Loss: 13.4916, Validation Loss: 13.4883\n",
      "Epoch 2330/6000, Training Loss: 13.4867, Validation Loss: 13.4835\n",
      "Epoch 2331/6000, Training Loss: 13.4819, Validation Loss: 13.4787\n",
      "Epoch 2332/6000, Training Loss: 13.4771, Validation Loss: 13.4739\n",
      "Epoch 2333/6000, Training Loss: 13.4722, Validation Loss: 13.4690\n",
      "Epoch 2334/6000, Training Loss: 13.4674, Validation Loss: 13.4642\n",
      "Epoch 2335/6000, Training Loss: 13.4626, Validation Loss: 13.4594\n",
      "Epoch 2336/6000, Training Loss: 13.4578, Validation Loss: 13.4546\n",
      "Epoch 2337/6000, Training Loss: 13.4531, Validation Loss: 13.4499\n",
      "Epoch 2338/6000, Training Loss: 13.4483, Validation Loss: 13.4451\n",
      "Epoch 2339/6000, Training Loss: 13.4435, Validation Loss: 13.4403\n",
      "Epoch 2340/6000, Training Loss: 13.4388, Validation Loss: 13.4356\n",
      "Epoch 2341/6000, Training Loss: 13.4341, Validation Loss: 13.4309\n",
      "Epoch 2342/6000, Training Loss: 13.4294, Validation Loss: 13.4262\n",
      "Epoch 2343/6000, Training Loss: 13.4247, Validation Loss: 13.4215\n",
      "Epoch 2344/6000, Training Loss: 13.4201, Validation Loss: 13.4168\n",
      "Epoch 2345/6000, Training Loss: 13.4154, Validation Loss: 13.4122\n",
      "Epoch 2346/6000, Training Loss: 13.4108, Validation Loss: 13.4076\n",
      "Epoch 2347/6000, Training Loss: 13.4062, Validation Loss: 13.4030\n",
      "Epoch 2348/6000, Training Loss: 13.4016, Validation Loss: 13.3984\n",
      "Epoch 2349/6000, Training Loss: 13.3970, Validation Loss: 13.3938\n",
      "Epoch 2350/6000, Training Loss: 13.3924, Validation Loss: 13.3892\n",
      "Epoch 2351/6000, Training Loss: 13.3878, Validation Loss: 13.3846\n",
      "Epoch 2352/6000, Training Loss: 13.3832, Validation Loss: 13.3801\n",
      "Epoch 2353/6000, Training Loss: 13.3787, Validation Loss: 13.3755\n",
      "Epoch 2354/6000, Training Loss: 13.3741, Validation Loss: 13.3709\n",
      "Epoch 2355/6000, Training Loss: 13.3695, Validation Loss: 13.3664\n",
      "Epoch 2356/6000, Training Loss: 13.3649, Validation Loss: 13.3618\n",
      "Epoch 2357/6000, Training Loss: 13.3604, Validation Loss: 13.3573\n",
      "Epoch 2358/6000, Training Loss: 13.3558, Validation Loss: 13.3527\n",
      "Epoch 2359/6000, Training Loss: 13.3513, Validation Loss: 13.3482\n",
      "Epoch 2360/6000, Training Loss: 13.3467, Validation Loss: 13.3437\n",
      "Epoch 2361/6000, Training Loss: 13.3422, Validation Loss: 13.3392\n",
      "Epoch 2362/6000, Training Loss: 13.3376, Validation Loss: 13.3347\n",
      "Epoch 2363/6000, Training Loss: 13.3331, Validation Loss: 13.3302\n",
      "Epoch 2364/6000, Training Loss: 13.3286, Validation Loss: 13.3257\n",
      "Epoch 2365/6000, Training Loss: 13.3241, Validation Loss: 13.3212\n",
      "Epoch 2366/6000, Training Loss: 13.3196, Validation Loss: 13.3167\n",
      "Epoch 2367/6000, Training Loss: 13.3151, Validation Loss: 13.3123\n",
      "Epoch 2368/6000, Training Loss: 13.3106, Validation Loss: 13.3078\n",
      "Epoch 2369/6000, Training Loss: 13.3061, Validation Loss: 13.3034\n",
      "Epoch 2370/6000, Training Loss: 13.3016, Validation Loss: 13.2990\n",
      "Epoch 2371/6000, Training Loss: 13.2972, Validation Loss: 13.2946\n",
      "Epoch 2372/6000, Training Loss: 13.2927, Validation Loss: 13.2901\n",
      "Epoch 2373/6000, Training Loss: 13.2882, Validation Loss: 13.2857\n",
      "Epoch 2374/6000, Training Loss: 13.2838, Validation Loss: 13.2813\n",
      "Epoch 2375/6000, Training Loss: 13.2794, Validation Loss: 13.2769\n",
      "Epoch 2376/6000, Training Loss: 13.2749, Validation Loss: 13.2725\n",
      "Epoch 2377/6000, Training Loss: 13.2705, Validation Loss: 13.2681\n",
      "Epoch 2378/6000, Training Loss: 13.2661, Validation Loss: 13.2636\n",
      "Epoch 2379/6000, Training Loss: 13.2616, Validation Loss: 13.2592\n",
      "Epoch 2380/6000, Training Loss: 13.2572, Validation Loss: 13.2548\n",
      "Epoch 2381/6000, Training Loss: 13.2528, Validation Loss: 13.2504\n",
      "Epoch 2382/6000, Training Loss: 13.2484, Validation Loss: 13.2460\n",
      "Epoch 2383/6000, Training Loss: 13.2440, Validation Loss: 13.2416\n",
      "Epoch 2384/6000, Training Loss: 13.2397, Validation Loss: 13.2372\n",
      "Epoch 2385/6000, Training Loss: 13.2353, Validation Loss: 13.2328\n",
      "Epoch 2386/6000, Training Loss: 13.2309, Validation Loss: 13.2285\n",
      "Epoch 2387/6000, Training Loss: 13.2266, Validation Loss: 13.2242\n",
      "Epoch 2388/6000, Training Loss: 13.2223, Validation Loss: 13.2199\n",
      "Epoch 2389/6000, Training Loss: 13.2180, Validation Loss: 13.2156\n",
      "Epoch 2390/6000, Training Loss: 13.2138, Validation Loss: 13.2113\n",
      "Epoch 2391/6000, Training Loss: 13.2095, Validation Loss: 13.2071\n",
      "Epoch 2392/6000, Training Loss: 13.2053, Validation Loss: 13.2028\n",
      "Epoch 2393/6000, Training Loss: 13.2010, Validation Loss: 13.1986\n",
      "Epoch 2394/6000, Training Loss: 13.1968, Validation Loss: 13.1944\n",
      "Epoch 2395/6000, Training Loss: 13.1926, Validation Loss: 13.1902\n",
      "Epoch 2396/6000, Training Loss: 13.1884, Validation Loss: 13.1860\n",
      "Epoch 2397/6000, Training Loss: 13.1842, Validation Loss: 13.1818\n",
      "Epoch 2398/6000, Training Loss: 13.1800, Validation Loss: 13.1776\n",
      "Epoch 2399/6000, Training Loss: 13.1758, Validation Loss: 13.1735\n",
      "Epoch 2400/6000, Training Loss: 13.1716, Validation Loss: 13.1693\n",
      "Epoch 2401/6000, Training Loss: 13.1674, Validation Loss: 13.1651\n",
      "Epoch 2402/6000, Training Loss: 13.1632, Validation Loss: 13.1610\n",
      "Epoch 2403/6000, Training Loss: 13.1591, Validation Loss: 13.1568\n",
      "Epoch 2404/6000, Training Loss: 13.1549, Validation Loss: 13.1526\n",
      "Epoch 2405/6000, Training Loss: 13.1507, Validation Loss: 13.1485\n",
      "Epoch 2406/6000, Training Loss: 13.1465, Validation Loss: 13.1443\n",
      "Epoch 2407/6000, Training Loss: 13.1424, Validation Loss: 13.1402\n",
      "Epoch 2408/6000, Training Loss: 13.1382, Validation Loss: 13.1360\n",
      "Epoch 2409/6000, Training Loss: 13.1341, Validation Loss: 13.1319\n",
      "Epoch 2410/6000, Training Loss: 13.1299, Validation Loss: 13.1278\n",
      "Epoch 2411/6000, Training Loss: 13.1258, Validation Loss: 13.1237\n",
      "Epoch 2412/6000, Training Loss: 13.1216, Validation Loss: 13.1196\n",
      "Epoch 2413/6000, Training Loss: 13.1175, Validation Loss: 13.1155\n",
      "Epoch 2414/6000, Training Loss: 13.1134, Validation Loss: 13.1114\n",
      "Epoch 2415/6000, Training Loss: 13.1093, Validation Loss: 13.1073\n",
      "Epoch 2416/6000, Training Loss: 13.1052, Validation Loss: 13.1032\n",
      "Epoch 2417/6000, Training Loss: 13.1011, Validation Loss: 13.0992\n",
      "Epoch 2418/6000, Training Loss: 13.0970, Validation Loss: 13.0951\n",
      "Epoch 2419/6000, Training Loss: 13.0929, Validation Loss: 13.0910\n",
      "Epoch 2420/6000, Training Loss: 13.0888, Validation Loss: 13.0870\n",
      "Epoch 2421/6000, Training Loss: 13.0847, Validation Loss: 13.0829\n",
      "Epoch 2422/6000, Training Loss: 13.0806, Validation Loss: 13.0789\n",
      "Epoch 2423/6000, Training Loss: 13.0766, Validation Loss: 13.0749\n",
      "Epoch 2424/6000, Training Loss: 13.0725, Validation Loss: 13.0708\n",
      "Epoch 2425/6000, Training Loss: 13.0684, Validation Loss: 13.0668\n",
      "Epoch 2426/6000, Training Loss: 13.0644, Validation Loss: 13.0628\n",
      "Epoch 2427/6000, Training Loss: 13.0603, Validation Loss: 13.0587\n",
      "Epoch 2428/6000, Training Loss: 13.0563, Validation Loss: 13.0547\n",
      "Epoch 2429/6000, Training Loss: 13.0523, Validation Loss: 13.0507\n",
      "Epoch 2430/6000, Training Loss: 13.0482, Validation Loss: 13.0467\n",
      "Epoch 2431/6000, Training Loss: 13.0442, Validation Loss: 13.0427\n",
      "Epoch 2432/6000, Training Loss: 13.0402, Validation Loss: 13.0387\n",
      "Epoch 2433/6000, Training Loss: 13.0362, Validation Loss: 13.0347\n",
      "Epoch 2434/6000, Training Loss: 13.0323, Validation Loss: 13.0307\n",
      "Epoch 2435/6000, Training Loss: 13.0283, Validation Loss: 13.0268\n",
      "Epoch 2436/6000, Training Loss: 13.0244, Validation Loss: 13.0229\n",
      "Epoch 2437/6000, Training Loss: 13.0204, Validation Loss: 13.0189\n",
      "Epoch 2438/6000, Training Loss: 13.0165, Validation Loss: 13.0150\n",
      "Epoch 2439/6000, Training Loss: 13.0126, Validation Loss: 13.0112\n",
      "Epoch 2440/6000, Training Loss: 13.0088, Validation Loss: 13.0073\n",
      "Epoch 2441/6000, Training Loss: 13.0049, Validation Loss: 13.0034\n",
      "Epoch 2442/6000, Training Loss: 13.0010, Validation Loss: 12.9996\n",
      "Epoch 2443/6000, Training Loss: 12.9972, Validation Loss: 12.9957\n",
      "Epoch 2444/6000, Training Loss: 12.9933, Validation Loss: 12.9919\n",
      "Epoch 2445/6000, Training Loss: 12.9895, Validation Loss: 12.9880\n",
      "Epoch 2446/6000, Training Loss: 12.9857, Validation Loss: 12.9842\n",
      "Epoch 2447/6000, Training Loss: 12.9818, Validation Loss: 12.9804\n",
      "Epoch 2448/6000, Training Loss: 12.9780, Validation Loss: 12.9766\n",
      "Epoch 2449/6000, Training Loss: 12.9742, Validation Loss: 12.9728\n",
      "Epoch 2450/6000, Training Loss: 12.9704, Validation Loss: 12.9690\n",
      "Epoch 2451/6000, Training Loss: 12.9666, Validation Loss: 12.9652\n",
      "Epoch 2452/6000, Training Loss: 12.9628, Validation Loss: 12.9613\n",
      "Epoch 2453/6000, Training Loss: 12.9590, Validation Loss: 12.9575\n",
      "Epoch 2454/6000, Training Loss: 12.9552, Validation Loss: 12.9538\n",
      "Epoch 2455/6000, Training Loss: 12.9514, Validation Loss: 12.9500\n",
      "Epoch 2456/6000, Training Loss: 12.9476, Validation Loss: 12.9462\n",
      "Epoch 2457/6000, Training Loss: 12.9438, Validation Loss: 12.9424\n",
      "Epoch 2458/6000, Training Loss: 12.9400, Validation Loss: 12.9386\n",
      "Epoch 2459/6000, Training Loss: 12.9362, Validation Loss: 12.9349\n",
      "Epoch 2460/6000, Training Loss: 12.9325, Validation Loss: 12.9311\n",
      "Epoch 2461/6000, Training Loss: 12.9287, Validation Loss: 12.9274\n",
      "Epoch 2462/6000, Training Loss: 12.9249, Validation Loss: 12.9236\n",
      "Epoch 2463/6000, Training Loss: 12.9212, Validation Loss: 12.9199\n",
      "Epoch 2464/6000, Training Loss: 12.9174, Validation Loss: 12.9162\n",
      "Epoch 2465/6000, Training Loss: 12.9137, Validation Loss: 12.9125\n",
      "Epoch 2466/6000, Training Loss: 12.9100, Validation Loss: 12.9088\n",
      "Epoch 2467/6000, Training Loss: 12.9062, Validation Loss: 12.9051\n",
      "Epoch 2468/6000, Training Loss: 12.9025, Validation Loss: 12.9014\n",
      "Epoch 2469/6000, Training Loss: 12.8988, Validation Loss: 12.8977\n",
      "Epoch 2470/6000, Training Loss: 12.8951, Validation Loss: 12.8940\n",
      "Epoch 2471/6000, Training Loss: 12.8913, Validation Loss: 12.8903\n",
      "Epoch 2472/6000, Training Loss: 12.8876, Validation Loss: 12.8866\n",
      "Epoch 2473/6000, Training Loss: 12.8839, Validation Loss: 12.8830\n",
      "Epoch 2474/6000, Training Loss: 12.8803, Validation Loss: 12.8793\n",
      "Epoch 2475/6000, Training Loss: 12.8766, Validation Loss: 12.8756\n",
      "Epoch 2476/6000, Training Loss: 12.8729, Validation Loss: 12.8719\n",
      "Epoch 2477/6000, Training Loss: 12.8692, Validation Loss: 12.8683\n",
      "Epoch 2478/6000, Training Loss: 12.8655, Validation Loss: 12.8646\n",
      "Epoch 2479/6000, Training Loss: 12.8619, Validation Loss: 12.8610\n",
      "Epoch 2480/6000, Training Loss: 12.8582, Validation Loss: 12.8573\n",
      "Epoch 2481/6000, Training Loss: 12.8546, Validation Loss: 12.8537\n",
      "Epoch 2482/6000, Training Loss: 12.8509, Validation Loss: 12.8500\n",
      "Epoch 2483/6000, Training Loss: 12.8473, Validation Loss: 12.8464\n",
      "Epoch 2484/6000, Training Loss: 12.8437, Validation Loss: 12.8428\n",
      "Epoch 2485/6000, Training Loss: 12.8401, Validation Loss: 12.8392\n",
      "Epoch 2486/6000, Training Loss: 12.8365, Validation Loss: 12.8357\n",
      "Epoch 2487/6000, Training Loss: 12.8330, Validation Loss: 12.8321\n",
      "Epoch 2488/6000, Training Loss: 12.8294, Validation Loss: 12.8286\n",
      "Epoch 2489/6000, Training Loss: 12.8259, Validation Loss: 12.8250\n",
      "Epoch 2490/6000, Training Loss: 12.8224, Validation Loss: 12.8215\n",
      "Epoch 2491/6000, Training Loss: 12.8189, Validation Loss: 12.8180\n",
      "Epoch 2492/6000, Training Loss: 12.8154, Validation Loss: 12.8145\n",
      "Epoch 2493/6000, Training Loss: 12.8119, Validation Loss: 12.8110\n",
      "Epoch 2494/6000, Training Loss: 12.8084, Validation Loss: 12.8076\n",
      "Epoch 2495/6000, Training Loss: 12.8049, Validation Loss: 12.8041\n",
      "Epoch 2496/6000, Training Loss: 12.8014, Validation Loss: 12.8006\n",
      "Epoch 2497/6000, Training Loss: 12.7979, Validation Loss: 12.7972\n",
      "Epoch 2498/6000, Training Loss: 12.7945, Validation Loss: 12.7937\n",
      "Epoch 2499/6000, Training Loss: 12.7910, Validation Loss: 12.7903\n",
      "Epoch 2500/6000, Training Loss: 12.7875, Validation Loss: 12.7868\n",
      "Epoch 2501/6000, Training Loss: 12.7841, Validation Loss: 12.7834\n",
      "Epoch 2502/6000, Training Loss: 12.7806, Validation Loss: 12.7799\n",
      "Epoch 2503/6000, Training Loss: 12.7772, Validation Loss: 12.7765\n",
      "Epoch 2504/6000, Training Loss: 12.7737, Validation Loss: 12.7730\n",
      "Epoch 2505/6000, Training Loss: 12.7703, Validation Loss: 12.7696\n",
      "Epoch 2506/6000, Training Loss: 12.7668, Validation Loss: 12.7661\n",
      "Epoch 2507/6000, Training Loss: 12.7634, Validation Loss: 12.7627\n",
      "Epoch 2508/6000, Training Loss: 12.7600, Validation Loss: 12.7593\n",
      "Epoch 2509/6000, Training Loss: 12.7565, Validation Loss: 12.7559\n",
      "Epoch 2510/6000, Training Loss: 12.7531, Validation Loss: 12.7525\n",
      "Epoch 2511/6000, Training Loss: 12.7497, Validation Loss: 12.7491\n",
      "Epoch 2512/6000, Training Loss: 12.7463, Validation Loss: 12.7457\n",
      "Epoch 2513/6000, Training Loss: 12.7429, Validation Loss: 12.7423\n",
      "Epoch 2514/6000, Training Loss: 12.7395, Validation Loss: 12.7390\n",
      "Epoch 2515/6000, Training Loss: 12.7361, Validation Loss: 12.7356\n",
      "Epoch 2516/6000, Training Loss: 12.7327, Validation Loss: 12.7322\n",
      "Epoch 2517/6000, Training Loss: 12.7293, Validation Loss: 12.7289\n",
      "Epoch 2518/6000, Training Loss: 12.7259, Validation Loss: 12.7255\n",
      "Epoch 2519/6000, Training Loss: 12.7225, Validation Loss: 12.7222\n",
      "Epoch 2520/6000, Training Loss: 12.7192, Validation Loss: 12.7188\n",
      "Epoch 2521/6000, Training Loss: 12.7158, Validation Loss: 12.7155\n",
      "Epoch 2522/6000, Training Loss: 12.7124, Validation Loss: 12.7122\n",
      "Epoch 2523/6000, Training Loss: 12.7091, Validation Loss: 12.7088\n",
      "Epoch 2524/6000, Training Loss: 12.7057, Validation Loss: 12.7055\n",
      "Epoch 2525/6000, Training Loss: 12.7024, Validation Loss: 12.7022\n",
      "Epoch 2526/6000, Training Loss: 12.6990, Validation Loss: 12.6988\n",
      "Epoch 2527/6000, Training Loss: 12.6957, Validation Loss: 12.6955\n",
      "Epoch 2528/6000, Training Loss: 12.6924, Validation Loss: 12.6922\n",
      "Epoch 2529/6000, Training Loss: 12.6890, Validation Loss: 12.6889\n",
      "Epoch 2530/6000, Training Loss: 12.6857, Validation Loss: 12.6856\n",
      "Epoch 2531/6000, Training Loss: 12.6824, Validation Loss: 12.6822\n",
      "Epoch 2532/6000, Training Loss: 12.6791, Validation Loss: 12.6789\n",
      "Epoch 2533/6000, Training Loss: 12.6758, Validation Loss: 12.6756\n",
      "Epoch 2534/6000, Training Loss: 12.6725, Validation Loss: 12.6724\n",
      "Epoch 2535/6000, Training Loss: 12.6692, Validation Loss: 12.6691\n",
      "Epoch 2536/6000, Training Loss: 12.6659, Validation Loss: 12.6658\n",
      "Epoch 2537/6000, Training Loss: 12.6627, Validation Loss: 12.6626\n",
      "Epoch 2538/6000, Training Loss: 12.6595, Validation Loss: 12.6594\n",
      "Epoch 2539/6000, Training Loss: 12.6562, Validation Loss: 12.6562\n",
      "Epoch 2540/6000, Training Loss: 12.6530, Validation Loss: 12.6530\n",
      "Epoch 2541/6000, Training Loss: 12.6498, Validation Loss: 12.6498\n",
      "Epoch 2542/6000, Training Loss: 12.6466, Validation Loss: 12.6466\n",
      "Epoch 2543/6000, Training Loss: 12.6435, Validation Loss: 12.6435\n",
      "Epoch 2544/6000, Training Loss: 12.6403, Validation Loss: 12.6403\n",
      "Epoch 2545/6000, Training Loss: 12.6371, Validation Loss: 12.6372\n",
      "Epoch 2546/6000, Training Loss: 12.6340, Validation Loss: 12.6340\n",
      "Epoch 2547/6000, Training Loss: 12.6308, Validation Loss: 12.6309\n",
      "Epoch 2548/6000, Training Loss: 12.6277, Validation Loss: 12.6278\n",
      "Epoch 2549/6000, Training Loss: 12.6246, Validation Loss: 12.6247\n",
      "Epoch 2550/6000, Training Loss: 12.6214, Validation Loss: 12.6216\n",
      "Epoch 2551/6000, Training Loss: 12.6183, Validation Loss: 12.6185\n",
      "Epoch 2552/6000, Training Loss: 12.6152, Validation Loss: 12.6154\n",
      "Epoch 2553/6000, Training Loss: 12.6120, Validation Loss: 12.6123\n",
      "Epoch 2554/6000, Training Loss: 12.6089, Validation Loss: 12.6092\n",
      "Epoch 2555/6000, Training Loss: 12.6058, Validation Loss: 12.6061\n",
      "Epoch 2556/6000, Training Loss: 12.6027, Validation Loss: 12.6030\n",
      "Epoch 2557/6000, Training Loss: 12.5996, Validation Loss: 12.5999\n",
      "Epoch 2558/6000, Training Loss: 12.5964, Validation Loss: 12.5968\n",
      "Epoch 2559/6000, Training Loss: 12.5933, Validation Loss: 12.5937\n",
      "Epoch 2560/6000, Training Loss: 12.5902, Validation Loss: 12.5907\n",
      "Epoch 2561/6000, Training Loss: 12.5871, Validation Loss: 12.5876\n",
      "Epoch 2562/6000, Training Loss: 12.5841, Validation Loss: 12.5846\n",
      "Epoch 2563/6000, Training Loss: 12.5810, Validation Loss: 12.5815\n",
      "Epoch 2564/6000, Training Loss: 12.5779, Validation Loss: 12.5785\n",
      "Epoch 2565/6000, Training Loss: 12.5748, Validation Loss: 12.5754\n",
      "Epoch 2566/6000, Training Loss: 12.5718, Validation Loss: 12.5724\n",
      "Epoch 2567/6000, Training Loss: 12.5687, Validation Loss: 12.5694\n",
      "Epoch 2568/6000, Training Loss: 12.5656, Validation Loss: 12.5663\n",
      "Epoch 2569/6000, Training Loss: 12.5626, Validation Loss: 12.5633\n",
      "Epoch 2570/6000, Training Loss: 12.5595, Validation Loss: 12.5603\n",
      "Epoch 2571/6000, Training Loss: 12.5565, Validation Loss: 12.5573\n",
      "Epoch 2572/6000, Training Loss: 12.5535, Validation Loss: 12.5543\n",
      "Epoch 2573/6000, Training Loss: 12.5504, Validation Loss: 12.5513\n",
      "Epoch 2574/6000, Training Loss: 12.5474, Validation Loss: 12.5483\n",
      "Epoch 2575/6000, Training Loss: 12.5444, Validation Loss: 12.5453\n",
      "Epoch 2576/6000, Training Loss: 12.5414, Validation Loss: 12.5423\n",
      "Epoch 2577/6000, Training Loss: 12.5384, Validation Loss: 12.5393\n",
      "Epoch 2578/6000, Training Loss: 12.5354, Validation Loss: 12.5363\n",
      "Epoch 2579/6000, Training Loss: 12.5324, Validation Loss: 12.5333\n",
      "Epoch 2580/6000, Training Loss: 12.5294, Validation Loss: 12.5303\n",
      "Epoch 2581/6000, Training Loss: 12.5264, Validation Loss: 12.5273\n",
      "Epoch 2582/6000, Training Loss: 12.5234, Validation Loss: 12.5244\n",
      "Epoch 2583/6000, Training Loss: 12.5204, Validation Loss: 12.5214\n",
      "Epoch 2584/6000, Training Loss: 12.5174, Validation Loss: 12.5184\n",
      "Epoch 2585/6000, Training Loss: 12.5144, Validation Loss: 12.5154\n",
      "Epoch 2586/6000, Training Loss: 12.5114, Validation Loss: 12.5124\n",
      "Epoch 2587/6000, Training Loss: 12.5085, Validation Loss: 12.5095\n",
      "Epoch 2588/6000, Training Loss: 12.5055, Validation Loss: 12.5065\n",
      "Epoch 2589/6000, Training Loss: 12.5026, Validation Loss: 12.5036\n",
      "Epoch 2590/6000, Training Loss: 12.4997, Validation Loss: 12.5007\n",
      "Epoch 2591/6000, Training Loss: 12.4967, Validation Loss: 12.4978\n",
      "Epoch 2592/6000, Training Loss: 12.4938, Validation Loss: 12.4949\n",
      "Epoch 2593/6000, Training Loss: 12.4910, Validation Loss: 12.4920\n",
      "Epoch 2594/6000, Training Loss: 12.4881, Validation Loss: 12.4891\n",
      "Epoch 2595/6000, Training Loss: 12.4852, Validation Loss: 12.4863\n",
      "Epoch 2596/6000, Training Loss: 12.4824, Validation Loss: 12.4834\n",
      "Epoch 2597/6000, Training Loss: 12.4795, Validation Loss: 12.4806\n",
      "Epoch 2598/6000, Training Loss: 12.4767, Validation Loss: 12.4778\n",
      "Epoch 2599/6000, Training Loss: 12.4739, Validation Loss: 12.4750\n",
      "Epoch 2600/6000, Training Loss: 12.4710, Validation Loss: 12.4722\n",
      "Epoch 2601/6000, Training Loss: 12.4682, Validation Loss: 12.4694\n",
      "Epoch 2602/6000, Training Loss: 12.4654, Validation Loss: 12.4666\n",
      "Epoch 2603/6000, Training Loss: 12.4626, Validation Loss: 12.4638\n",
      "Epoch 2604/6000, Training Loss: 12.4598, Validation Loss: 12.4610\n",
      "Epoch 2605/6000, Training Loss: 12.4570, Validation Loss: 12.4582\n",
      "Epoch 2606/6000, Training Loss: 12.4542, Validation Loss: 12.4554\n",
      "Epoch 2607/6000, Training Loss: 12.4514, Validation Loss: 12.4527\n",
      "Epoch 2608/6000, Training Loss: 12.4486, Validation Loss: 12.4499\n",
      "Epoch 2609/6000, Training Loss: 12.4458, Validation Loss: 12.4471\n",
      "Epoch 2610/6000, Training Loss: 12.4430, Validation Loss: 12.4443\n",
      "Epoch 2611/6000, Training Loss: 12.4403, Validation Loss: 12.4416\n",
      "Epoch 2612/6000, Training Loss: 12.4375, Validation Loss: 12.4388\n",
      "Epoch 2613/6000, Training Loss: 12.4347, Validation Loss: 12.4361\n",
      "Epoch 2614/6000, Training Loss: 12.4319, Validation Loss: 12.4333\n",
      "Epoch 2615/6000, Training Loss: 12.4292, Validation Loss: 12.4306\n",
      "Epoch 2616/6000, Training Loss: 12.4264, Validation Loss: 12.4278\n",
      "Epoch 2617/6000, Training Loss: 12.4237, Validation Loss: 12.4251\n",
      "Epoch 2618/6000, Training Loss: 12.4209, Validation Loss: 12.4224\n",
      "Epoch 2619/6000, Training Loss: 12.4182, Validation Loss: 12.4197\n",
      "Epoch 2620/6000, Training Loss: 12.4154, Validation Loss: 12.4169\n",
      "Epoch 2621/6000, Training Loss: 12.4127, Validation Loss: 12.4142\n",
      "Epoch 2622/6000, Training Loss: 12.4100, Validation Loss: 12.4115\n",
      "Epoch 2623/6000, Training Loss: 12.4072, Validation Loss: 12.4088\n",
      "Epoch 2624/6000, Training Loss: 12.4045, Validation Loss: 12.4061\n",
      "Epoch 2625/6000, Training Loss: 12.4018, Validation Loss: 12.4034\n",
      "Epoch 2626/6000, Training Loss: 12.3991, Validation Loss: 12.4007\n",
      "Epoch 2627/6000, Training Loss: 12.3964, Validation Loss: 12.3980\n",
      "Epoch 2628/6000, Training Loss: 12.3937, Validation Loss: 12.3953\n",
      "Epoch 2629/6000, Training Loss: 12.3910, Validation Loss: 12.3926\n",
      "Epoch 2630/6000, Training Loss: 12.3883, Validation Loss: 12.3899\n",
      "Epoch 2631/6000, Training Loss: 12.3856, Validation Loss: 12.3872\n",
      "Epoch 2632/6000, Training Loss: 12.3829, Validation Loss: 12.3845\n",
      "Epoch 2633/6000, Training Loss: 12.3802, Validation Loss: 12.3819\n",
      "Epoch 2634/6000, Training Loss: 12.3775, Validation Loss: 12.3792\n",
      "Epoch 2635/6000, Training Loss: 12.3748, Validation Loss: 12.3765\n",
      "Epoch 2636/6000, Training Loss: 12.3722, Validation Loss: 12.3738\n",
      "Epoch 2637/6000, Training Loss: 12.3695, Validation Loss: 12.3712\n",
      "Epoch 2638/6000, Training Loss: 12.3668, Validation Loss: 12.3685\n",
      "Epoch 2639/6000, Training Loss: 12.3642, Validation Loss: 12.3658\n",
      "Epoch 2640/6000, Training Loss: 12.3615, Validation Loss: 12.3632\n",
      "Epoch 2641/6000, Training Loss: 12.3588, Validation Loss: 12.3605\n",
      "Epoch 2642/6000, Training Loss: 12.3562, Validation Loss: 12.3579\n",
      "Epoch 2643/6000, Training Loss: 12.3536, Validation Loss: 12.3552\n",
      "Epoch 2644/6000, Training Loss: 12.3509, Validation Loss: 12.3526\n",
      "Epoch 2645/6000, Training Loss: 12.3483, Validation Loss: 12.3500\n",
      "Epoch 2646/6000, Training Loss: 12.3457, Validation Loss: 12.3474\n",
      "Epoch 2647/6000, Training Loss: 12.3431, Validation Loss: 12.3448\n",
      "Epoch 2648/6000, Training Loss: 12.3405, Validation Loss: 12.3422\n",
      "Epoch 2649/6000, Training Loss: 12.3380, Validation Loss: 12.3397\n",
      "Epoch 2650/6000, Training Loss: 12.3354, Validation Loss: 12.3371\n",
      "Epoch 2651/6000, Training Loss: 12.3329, Validation Loss: 12.3346\n",
      "Epoch 2652/6000, Training Loss: 12.3303, Validation Loss: 12.3320\n",
      "Epoch 2653/6000, Training Loss: 12.3278, Validation Loss: 12.3295\n",
      "Epoch 2654/6000, Training Loss: 12.3253, Validation Loss: 12.3270\n",
      "Epoch 2655/6000, Training Loss: 12.3228, Validation Loss: 12.3245\n",
      "Epoch 2656/6000, Training Loss: 12.3203, Validation Loss: 12.3220\n",
      "Epoch 2657/6000, Training Loss: 12.3178, Validation Loss: 12.3195\n",
      "Epoch 2658/6000, Training Loss: 12.3153, Validation Loss: 12.3171\n",
      "Epoch 2659/6000, Training Loss: 12.3128, Validation Loss: 12.3146\n",
      "Epoch 2660/6000, Training Loss: 12.3103, Validation Loss: 12.3121\n",
      "Epoch 2661/6000, Training Loss: 12.3078, Validation Loss: 12.3097\n",
      "Epoch 2662/6000, Training Loss: 12.3054, Validation Loss: 12.3072\n",
      "Epoch 2663/6000, Training Loss: 12.3029, Validation Loss: 12.3047\n",
      "Epoch 2664/6000, Training Loss: 12.3004, Validation Loss: 12.3023\n",
      "Epoch 2665/6000, Training Loss: 12.2980, Validation Loss: 12.2998\n",
      "Epoch 2666/6000, Training Loss: 12.2955, Validation Loss: 12.2974\n",
      "Epoch 2667/6000, Training Loss: 12.2930, Validation Loss: 12.2949\n",
      "Epoch 2668/6000, Training Loss: 12.2906, Validation Loss: 12.2925\n",
      "Epoch 2669/6000, Training Loss: 12.2881, Validation Loss: 12.2900\n",
      "Epoch 2670/6000, Training Loss: 12.2857, Validation Loss: 12.2876\n",
      "Epoch 2671/6000, Training Loss: 12.2832, Validation Loss: 12.2852\n",
      "Epoch 2672/6000, Training Loss: 12.2808, Validation Loss: 12.2828\n",
      "Epoch 2673/6000, Training Loss: 12.2784, Validation Loss: 12.2803\n",
      "Epoch 2674/6000, Training Loss: 12.2759, Validation Loss: 12.2779\n",
      "Epoch 2675/6000, Training Loss: 12.2735, Validation Loss: 12.2755\n",
      "Epoch 2676/6000, Training Loss: 12.2711, Validation Loss: 12.2731\n",
      "Epoch 2677/6000, Training Loss: 12.2687, Validation Loss: 12.2707\n",
      "Epoch 2678/6000, Training Loss: 12.2662, Validation Loss: 12.2683\n",
      "Epoch 2679/6000, Training Loss: 12.2638, Validation Loss: 12.2659\n",
      "Epoch 2680/6000, Training Loss: 12.2614, Validation Loss: 12.2635\n",
      "Epoch 2681/6000, Training Loss: 12.2590, Validation Loss: 12.2611\n",
      "Epoch 2682/6000, Training Loss: 12.2566, Validation Loss: 12.2587\n",
      "Epoch 2683/6000, Training Loss: 12.2542, Validation Loss: 12.2563\n",
      "Epoch 2684/6000, Training Loss: 12.2518, Validation Loss: 12.2539\n",
      "Epoch 2685/6000, Training Loss: 12.2494, Validation Loss: 12.2515\n",
      "Epoch 2686/6000, Training Loss: 12.2470, Validation Loss: 12.2491\n",
      "Epoch 2687/6000, Training Loss: 12.2446, Validation Loss: 12.2468\n",
      "Epoch 2688/6000, Training Loss: 12.2423, Validation Loss: 12.2444\n",
      "Epoch 2689/6000, Training Loss: 12.2399, Validation Loss: 12.2420\n",
      "Epoch 2690/6000, Training Loss: 12.2375, Validation Loss: 12.2396\n",
      "Epoch 2691/6000, Training Loss: 12.2351, Validation Loss: 12.2373\n",
      "Epoch 2692/6000, Training Loss: 12.2328, Validation Loss: 12.2349\n",
      "Epoch 2693/6000, Training Loss: 12.2304, Validation Loss: 12.2325\n",
      "Epoch 2694/6000, Training Loss: 12.2280, Validation Loss: 12.2302\n",
      "Epoch 2695/6000, Training Loss: 12.2257, Validation Loss: 12.2278\n",
      "Epoch 2696/6000, Training Loss: 12.2233, Validation Loss: 12.2255\n",
      "Epoch 2697/6000, Training Loss: 12.2210, Validation Loss: 12.2231\n",
      "Epoch 2698/6000, Training Loss: 12.2186, Validation Loss: 12.2208\n",
      "Epoch 2699/6000, Training Loss: 12.2163, Validation Loss: 12.2184\n",
      "Epoch 2700/6000, Training Loss: 12.2140, Validation Loss: 12.2161\n",
      "Epoch 2701/6000, Training Loss: 12.2116, Validation Loss: 12.2137\n",
      "Epoch 2702/6000, Training Loss: 12.2093, Validation Loss: 12.2114\n",
      "Epoch 2703/6000, Training Loss: 12.2070, Validation Loss: 12.2091\n",
      "Epoch 2704/6000, Training Loss: 12.2047, Validation Loss: 12.2068\n",
      "Epoch 2705/6000, Training Loss: 12.2024, Validation Loss: 12.2045\n",
      "Epoch 2706/6000, Training Loss: 12.2001, Validation Loss: 12.2022\n",
      "Epoch 2707/6000, Training Loss: 12.1979, Validation Loss: 12.2000\n",
      "Epoch 2708/6000, Training Loss: 12.1956, Validation Loss: 12.1977\n",
      "Epoch 2709/6000, Training Loss: 12.1933, Validation Loss: 12.1955\n",
      "Epoch 2710/6000, Training Loss: 12.1911, Validation Loss: 12.1932\n",
      "Epoch 2711/6000, Training Loss: 12.1889, Validation Loss: 12.1910\n",
      "Epoch 2712/6000, Training Loss: 12.1866, Validation Loss: 12.1888\n",
      "Epoch 2713/6000, Training Loss: 12.1844, Validation Loss: 12.1866\n",
      "Epoch 2714/6000, Training Loss: 12.1822, Validation Loss: 12.1844\n",
      "Epoch 2715/6000, Training Loss: 12.1800, Validation Loss: 12.1822\n",
      "Epoch 2716/6000, Training Loss: 12.1778, Validation Loss: 12.1800\n",
      "Epoch 2717/6000, Training Loss: 12.1756, Validation Loss: 12.1778\n",
      "Epoch 2718/6000, Training Loss: 12.1735, Validation Loss: 12.1757\n",
      "Epoch 2719/6000, Training Loss: 12.1713, Validation Loss: 12.1735\n",
      "Epoch 2720/6000, Training Loss: 12.1691, Validation Loss: 12.1713\n",
      "Epoch 2721/6000, Training Loss: 12.1669, Validation Loss: 12.1692\n",
      "Epoch 2722/6000, Training Loss: 12.1648, Validation Loss: 12.1670\n",
      "Epoch 2723/6000, Training Loss: 12.1626, Validation Loss: 12.1648\n",
      "Epoch 2724/6000, Training Loss: 12.1604, Validation Loss: 12.1627\n",
      "Epoch 2725/6000, Training Loss: 12.1583, Validation Loss: 12.1605\n",
      "Epoch 2726/6000, Training Loss: 12.1561, Validation Loss: 12.1584\n",
      "Epoch 2727/6000, Training Loss: 12.1540, Validation Loss: 12.1562\n",
      "Epoch 2728/6000, Training Loss: 12.1518, Validation Loss: 12.1541\n",
      "Epoch 2729/6000, Training Loss: 12.1497, Validation Loss: 12.1520\n",
      "Epoch 2730/6000, Training Loss: 12.1475, Validation Loss: 12.1498\n",
      "Epoch 2731/6000, Training Loss: 12.1454, Validation Loss: 12.1477\n",
      "Epoch 2732/6000, Training Loss: 12.1432, Validation Loss: 12.1456\n",
      "Epoch 2733/6000, Training Loss: 12.1411, Validation Loss: 12.1435\n",
      "Epoch 2734/6000, Training Loss: 12.1390, Validation Loss: 12.1413\n",
      "Epoch 2735/6000, Training Loss: 12.1368, Validation Loss: 12.1392\n",
      "Epoch 2736/6000, Training Loss: 12.1347, Validation Loss: 12.1371\n",
      "Epoch 2737/6000, Training Loss: 12.1326, Validation Loss: 12.1350\n",
      "Epoch 2738/6000, Training Loss: 12.1305, Validation Loss: 12.1329\n",
      "Epoch 2739/6000, Training Loss: 12.1284, Validation Loss: 12.1308\n",
      "Epoch 2740/6000, Training Loss: 12.1262, Validation Loss: 12.1287\n",
      "Epoch 2741/6000, Training Loss: 12.1241, Validation Loss: 12.1266\n",
      "Epoch 2742/6000, Training Loss: 12.1220, Validation Loss: 12.1245\n",
      "Epoch 2743/6000, Training Loss: 12.1199, Validation Loss: 12.1224\n",
      "Epoch 2744/6000, Training Loss: 12.1178, Validation Loss: 12.1203\n",
      "Epoch 2745/6000, Training Loss: 12.1157, Validation Loss: 12.1182\n",
      "Epoch 2746/6000, Training Loss: 12.1136, Validation Loss: 12.1162\n",
      "Epoch 2747/6000, Training Loss: 12.1116, Validation Loss: 12.1141\n",
      "Epoch 2748/6000, Training Loss: 12.1095, Validation Loss: 12.1120\n",
      "Epoch 2749/6000, Training Loss: 12.1074, Validation Loss: 12.1099\n",
      "Epoch 2750/6000, Training Loss: 12.1053, Validation Loss: 12.1079\n",
      "Epoch 2751/6000, Training Loss: 12.1032, Validation Loss: 12.1058\n",
      "Epoch 2752/6000, Training Loss: 12.1012, Validation Loss: 12.1037\n",
      "Epoch 2753/6000, Training Loss: 12.0991, Validation Loss: 12.1017\n",
      "Epoch 2754/6000, Training Loss: 12.0970, Validation Loss: 12.0996\n",
      "Epoch 2755/6000, Training Loss: 12.0950, Validation Loss: 12.0975\n",
      "Epoch 2756/6000, Training Loss: 12.0929, Validation Loss: 12.0955\n",
      "Epoch 2757/6000, Training Loss: 12.0908, Validation Loss: 12.0934\n",
      "Epoch 2758/6000, Training Loss: 12.0888, Validation Loss: 12.0914\n",
      "Epoch 2759/6000, Training Loss: 12.0867, Validation Loss: 12.0893\n",
      "Epoch 2760/6000, Training Loss: 12.0847, Validation Loss: 12.0873\n",
      "Epoch 2761/6000, Training Loss: 12.0826, Validation Loss: 12.0852\n",
      "Epoch 2762/6000, Training Loss: 12.0806, Validation Loss: 12.0832\n",
      "Epoch 2763/6000, Training Loss: 12.0786, Validation Loss: 12.0811\n",
      "Epoch 2764/6000, Training Loss: 12.0765, Validation Loss: 12.0791\n",
      "Epoch 2765/6000, Training Loss: 12.0745, Validation Loss: 12.0771\n",
      "Epoch 2766/6000, Training Loss: 12.0725, Validation Loss: 12.0751\n",
      "Epoch 2767/6000, Training Loss: 12.0705, Validation Loss: 12.0731\n",
      "Epoch 2768/6000, Training Loss: 12.0685, Validation Loss: 12.0711\n",
      "Epoch 2769/6000, Training Loss: 12.0665, Validation Loss: 12.0691\n",
      "Epoch 2770/6000, Training Loss: 12.0645, Validation Loss: 12.0671\n",
      "Epoch 2771/6000, Training Loss: 12.0625, Validation Loss: 12.0652\n",
      "Epoch 2772/6000, Training Loss: 12.0606, Validation Loss: 12.0632\n",
      "Epoch 2773/6000, Training Loss: 12.0586, Validation Loss: 12.0613\n",
      "Epoch 2774/6000, Training Loss: 12.0567, Validation Loss: 12.0593\n",
      "Epoch 2775/6000, Training Loss: 12.0548, Validation Loss: 12.0574\n",
      "Epoch 2776/6000, Training Loss: 12.0528, Validation Loss: 12.0555\n",
      "Epoch 2777/6000, Training Loss: 12.0509, Validation Loss: 12.0536\n",
      "Epoch 2778/6000, Training Loss: 12.0490, Validation Loss: 12.0517\n",
      "Epoch 2779/6000, Training Loss: 12.0471, Validation Loss: 12.0498\n",
      "Epoch 2780/6000, Training Loss: 12.0452, Validation Loss: 12.0479\n",
      "Epoch 2781/6000, Training Loss: 12.0433, Validation Loss: 12.0460\n",
      "Epoch 2782/6000, Training Loss: 12.0414, Validation Loss: 12.0441\n",
      "Epoch 2783/6000, Training Loss: 12.0395, Validation Loss: 12.0422\n",
      "Epoch 2784/6000, Training Loss: 12.0376, Validation Loss: 12.0403\n",
      "Epoch 2785/6000, Training Loss: 12.0357, Validation Loss: 12.0385\n",
      "Epoch 2786/6000, Training Loss: 12.0338, Validation Loss: 12.0366\n",
      "Epoch 2787/6000, Training Loss: 12.0319, Validation Loss: 12.0347\n",
      "Epoch 2788/6000, Training Loss: 12.0300, Validation Loss: 12.0329\n",
      "Epoch 2789/6000, Training Loss: 12.0282, Validation Loss: 12.0310\n",
      "Epoch 2790/6000, Training Loss: 12.0263, Validation Loss: 12.0291\n",
      "Epoch 2791/6000, Training Loss: 12.0244, Validation Loss: 12.0273\n",
      "Epoch 2792/6000, Training Loss: 12.0225, Validation Loss: 12.0254\n",
      "Epoch 2793/6000, Training Loss: 12.0207, Validation Loss: 12.0236\n",
      "Epoch 2794/6000, Training Loss: 12.0188, Validation Loss: 12.0217\n",
      "Epoch 2795/6000, Training Loss: 12.0170, Validation Loss: 12.0199\n",
      "Epoch 2796/6000, Training Loss: 12.0151, Validation Loss: 12.0180\n",
      "Epoch 2797/6000, Training Loss: 12.0132, Validation Loss: 12.0162\n",
      "Epoch 2798/6000, Training Loss: 12.0114, Validation Loss: 12.0143\n",
      "Epoch 2799/6000, Training Loss: 12.0095, Validation Loss: 12.0125\n",
      "Epoch 2800/6000, Training Loss: 12.0077, Validation Loss: 12.0107\n",
      "Epoch 2801/6000, Training Loss: 12.0059, Validation Loss: 12.0089\n",
      "Epoch 2802/6000, Training Loss: 12.0040, Validation Loss: 12.0070\n",
      "Epoch 2803/6000, Training Loss: 12.0022, Validation Loss: 12.0052\n",
      "Epoch 2804/6000, Training Loss: 12.0003, Validation Loss: 12.0034\n",
      "Epoch 2805/6000, Training Loss: 11.9985, Validation Loss: 12.0016\n",
      "Epoch 2806/6000, Training Loss: 11.9967, Validation Loss: 11.9998\n",
      "Epoch 2807/6000, Training Loss: 11.9949, Validation Loss: 11.9980\n",
      "Epoch 2808/6000, Training Loss: 11.9930, Validation Loss: 11.9962\n",
      "Epoch 2809/6000, Training Loss: 11.9912, Validation Loss: 11.9944\n",
      "Epoch 2810/6000, Training Loss: 11.9894, Validation Loss: 11.9926\n",
      "Epoch 2811/6000, Training Loss: 11.9876, Validation Loss: 11.9908\n",
      "Epoch 2812/6000, Training Loss: 11.9858, Validation Loss: 11.9890\n",
      "Epoch 2813/6000, Training Loss: 11.9840, Validation Loss: 11.9872\n",
      "Epoch 2814/6000, Training Loss: 11.9822, Validation Loss: 11.9854\n",
      "Epoch 2815/6000, Training Loss: 11.9804, Validation Loss: 11.9836\n",
      "Epoch 2816/6000, Training Loss: 11.9786, Validation Loss: 11.9818\n",
      "Epoch 2817/6000, Training Loss: 11.9768, Validation Loss: 11.9800\n",
      "Epoch 2818/6000, Training Loss: 11.9750, Validation Loss: 11.9782\n",
      "Epoch 2819/6000, Training Loss: 11.9732, Validation Loss: 11.9765\n",
      "Epoch 2820/6000, Training Loss: 11.9714, Validation Loss: 11.9747\n",
      "Epoch 2821/6000, Training Loss: 11.9696, Validation Loss: 11.9729\n",
      "Epoch 2822/6000, Training Loss: 11.9679, Validation Loss: 11.9711\n",
      "Epoch 2823/6000, Training Loss: 11.9661, Validation Loss: 11.9694\n",
      "Epoch 2824/6000, Training Loss: 11.9643, Validation Loss: 11.9676\n",
      "Epoch 2825/6000, Training Loss: 11.9625, Validation Loss: 11.9658\n",
      "Epoch 2826/6000, Training Loss: 11.9608, Validation Loss: 11.9640\n",
      "Epoch 2827/6000, Training Loss: 11.9590, Validation Loss: 11.9623\n",
      "Epoch 2828/6000, Training Loss: 11.9572, Validation Loss: 11.9605\n",
      "Epoch 2829/6000, Training Loss: 11.9555, Validation Loss: 11.9588\n",
      "Epoch 2830/6000, Training Loss: 11.9537, Validation Loss: 11.9570\n",
      "Epoch 2831/6000, Training Loss: 11.9520, Validation Loss: 11.9553\n",
      "Epoch 2832/6000, Training Loss: 11.9503, Validation Loss: 11.9535\n",
      "Epoch 2833/6000, Training Loss: 11.9485, Validation Loss: 11.9518\n",
      "Epoch 2834/6000, Training Loss: 11.9468, Validation Loss: 11.9501\n",
      "Epoch 2835/6000, Training Loss: 11.9451, Validation Loss: 11.9484\n",
      "Epoch 2836/6000, Training Loss: 11.9434, Validation Loss: 11.9467\n",
      "Epoch 2837/6000, Training Loss: 11.9417, Validation Loss: 11.9450\n",
      "Epoch 2838/6000, Training Loss: 11.9400, Validation Loss: 11.9433\n",
      "Epoch 2839/6000, Training Loss: 11.9383, Validation Loss: 11.9417\n",
      "Epoch 2840/6000, Training Loss: 11.9366, Validation Loss: 11.9400\n",
      "Epoch 2841/6000, Training Loss: 11.9350, Validation Loss: 11.9383\n",
      "Epoch 2842/6000, Training Loss: 11.9333, Validation Loss: 11.9367\n",
      "Epoch 2843/6000, Training Loss: 11.9317, Validation Loss: 11.9351\n",
      "Epoch 2844/6000, Training Loss: 11.9300, Validation Loss: 11.9334\n",
      "Epoch 2845/6000, Training Loss: 11.9284, Validation Loss: 11.9318\n",
      "Epoch 2846/6000, Training Loss: 11.9267, Validation Loss: 11.9302\n",
      "Epoch 2847/6000, Training Loss: 11.9251, Validation Loss: 11.9286\n",
      "Epoch 2848/6000, Training Loss: 11.9235, Validation Loss: 11.9269\n",
      "Epoch 2849/6000, Training Loss: 11.9218, Validation Loss: 11.9253\n",
      "Epoch 2850/6000, Training Loss: 11.9202, Validation Loss: 11.9237\n",
      "Epoch 2851/6000, Training Loss: 11.9186, Validation Loss: 11.9221\n",
      "Epoch 2852/6000, Training Loss: 11.9170, Validation Loss: 11.9205\n",
      "Epoch 2853/6000, Training Loss: 11.9153, Validation Loss: 11.9189\n",
      "Epoch 2854/6000, Training Loss: 11.9137, Validation Loss: 11.9173\n",
      "Epoch 2855/6000, Training Loss: 11.9121, Validation Loss: 11.9157\n",
      "Epoch 2856/6000, Training Loss: 11.9105, Validation Loss: 11.9142\n",
      "Epoch 2857/6000, Training Loss: 11.9089, Validation Loss: 11.9126\n",
      "Epoch 2858/6000, Training Loss: 11.9073, Validation Loss: 11.9110\n",
      "Epoch 2859/6000, Training Loss: 11.9057, Validation Loss: 11.9094\n",
      "Epoch 2860/6000, Training Loss: 11.9041, Validation Loss: 11.9078\n",
      "Epoch 2861/6000, Training Loss: 11.9025, Validation Loss: 11.9063\n",
      "Epoch 2862/6000, Training Loss: 11.9009, Validation Loss: 11.9047\n",
      "Epoch 2863/6000, Training Loss: 11.8993, Validation Loss: 11.9031\n",
      "Epoch 2864/6000, Training Loss: 11.8977, Validation Loss: 11.9015\n",
      "Epoch 2865/6000, Training Loss: 11.8961, Validation Loss: 11.9000\n",
      "Epoch 2866/6000, Training Loss: 11.8946, Validation Loss: 11.8984\n",
      "Epoch 2867/6000, Training Loss: 11.8930, Validation Loss: 11.8969\n",
      "Epoch 2868/6000, Training Loss: 11.8914, Validation Loss: 11.8953\n",
      "Epoch 2869/6000, Training Loss: 11.8898, Validation Loss: 11.8937\n",
      "Epoch 2870/6000, Training Loss: 11.8883, Validation Loss: 11.8922\n",
      "Epoch 2871/6000, Training Loss: 11.8867, Validation Loss: 11.8906\n",
      "Epoch 2872/6000, Training Loss: 11.8851, Validation Loss: 11.8891\n",
      "Epoch 2873/6000, Training Loss: 11.8835, Validation Loss: 11.8876\n",
      "Epoch 2874/6000, Training Loss: 11.8820, Validation Loss: 11.8860\n",
      "Epoch 2875/6000, Training Loss: 11.8804, Validation Loss: 11.8845\n",
      "Epoch 2876/6000, Training Loss: 11.8789, Validation Loss: 11.8829\n",
      "Epoch 2877/6000, Training Loss: 11.8773, Validation Loss: 11.8814\n",
      "Epoch 2878/6000, Training Loss: 11.8757, Validation Loss: 11.8799\n",
      "Epoch 2879/6000, Training Loss: 11.8742, Validation Loss: 11.8783\n",
      "Epoch 2880/6000, Training Loss: 11.8726, Validation Loss: 11.8768\n",
      "Epoch 2881/6000, Training Loss: 11.8711, Validation Loss: 11.8753\n",
      "Epoch 2882/6000, Training Loss: 11.8696, Validation Loss: 11.8737\n",
      "Epoch 2883/6000, Training Loss: 11.8680, Validation Loss: 11.8722\n",
      "Epoch 2884/6000, Training Loss: 11.8665, Validation Loss: 11.8707\n",
      "Epoch 2885/6000, Training Loss: 11.8649, Validation Loss: 11.8692\n",
      "Epoch 2886/6000, Training Loss: 11.8634, Validation Loss: 11.8677\n",
      "Epoch 2887/6000, Training Loss: 11.8619, Validation Loss: 11.8661\n",
      "Epoch 2888/6000, Training Loss: 11.8604, Validation Loss: 11.8646\n",
      "Epoch 2889/6000, Training Loss: 11.8588, Validation Loss: 11.8631\n",
      "Epoch 2890/6000, Training Loss: 11.8573, Validation Loss: 11.8616\n",
      "Epoch 2891/6000, Training Loss: 11.8558, Validation Loss: 11.8601\n",
      "Epoch 2892/6000, Training Loss: 11.8543, Validation Loss: 11.8586\n",
      "Epoch 2893/6000, Training Loss: 11.8527, Validation Loss: 11.8570\n",
      "Epoch 2894/6000, Training Loss: 11.8512, Validation Loss: 11.8555\n",
      "Epoch 2895/6000, Training Loss: 11.8497, Validation Loss: 11.8540\n",
      "Epoch 2896/6000, Training Loss: 11.8482, Validation Loss: 11.8525\n",
      "Epoch 2897/6000, Training Loss: 11.8467, Validation Loss: 11.8510\n",
      "Epoch 2898/6000, Training Loss: 11.8452, Validation Loss: 11.8495\n",
      "Epoch 2899/6000, Training Loss: 11.8437, Validation Loss: 11.8480\n",
      "Epoch 2900/6000, Training Loss: 11.8422, Validation Loss: 11.8465\n",
      "Epoch 2901/6000, Training Loss: 11.8407, Validation Loss: 11.8451\n",
      "Epoch 2902/6000, Training Loss: 11.8392, Validation Loss: 11.8436\n",
      "Epoch 2903/6000, Training Loss: 11.8377, Validation Loss: 11.8421\n",
      "Epoch 2904/6000, Training Loss: 11.8363, Validation Loss: 11.8407\n",
      "Epoch 2905/6000, Training Loss: 11.8348, Validation Loss: 11.8392\n",
      "Epoch 2906/6000, Training Loss: 11.8333, Validation Loss: 11.8378\n",
      "Epoch 2907/6000, Training Loss: 11.8319, Validation Loss: 11.8364\n",
      "Epoch 2908/6000, Training Loss: 11.8304, Validation Loss: 11.8349\n",
      "Epoch 2909/6000, Training Loss: 11.8290, Validation Loss: 11.8335\n",
      "Epoch 2910/6000, Training Loss: 11.8276, Validation Loss: 11.8321\n",
      "Epoch 2911/6000, Training Loss: 11.8262, Validation Loss: 11.8307\n",
      "Epoch 2912/6000, Training Loss: 11.8248, Validation Loss: 11.8293\n",
      "Epoch 2913/6000, Training Loss: 11.8233, Validation Loss: 11.8279\n",
      "Epoch 2914/6000, Training Loss: 11.8219, Validation Loss: 11.8266\n",
      "Epoch 2915/6000, Training Loss: 11.8205, Validation Loss: 11.8252\n",
      "Epoch 2916/6000, Training Loss: 11.8191, Validation Loss: 11.8238\n",
      "Epoch 2917/6000, Training Loss: 11.8178, Validation Loss: 11.8224\n",
      "Epoch 2918/6000, Training Loss: 11.8164, Validation Loss: 11.8211\n",
      "Epoch 2919/6000, Training Loss: 11.8150, Validation Loss: 11.8197\n",
      "Epoch 2920/6000, Training Loss: 11.8136, Validation Loss: 11.8184\n",
      "Epoch 2921/6000, Training Loss: 11.8122, Validation Loss: 11.8170\n",
      "Epoch 2922/6000, Training Loss: 11.8109, Validation Loss: 11.8157\n",
      "Epoch 2923/6000, Training Loss: 11.8095, Validation Loss: 11.8143\n",
      "Epoch 2924/6000, Training Loss: 11.8081, Validation Loss: 11.8130\n",
      "Epoch 2925/6000, Training Loss: 11.8068, Validation Loss: 11.8116\n",
      "Epoch 2926/6000, Training Loss: 11.8054, Validation Loss: 11.8103\n",
      "Epoch 2927/6000, Training Loss: 11.8041, Validation Loss: 11.8090\n",
      "Epoch 2928/6000, Training Loss: 11.8027, Validation Loss: 11.8076\n",
      "Epoch 2929/6000, Training Loss: 11.8013, Validation Loss: 11.8063\n",
      "Epoch 2930/6000, Training Loss: 11.8000, Validation Loss: 11.8050\n",
      "Epoch 2931/6000, Training Loss: 11.7986, Validation Loss: 11.8037\n",
      "Epoch 2932/6000, Training Loss: 11.7973, Validation Loss: 11.8023\n",
      "Epoch 2933/6000, Training Loss: 11.7959, Validation Loss: 11.8010\n",
      "Epoch 2934/6000, Training Loss: 11.7946, Validation Loss: 11.7997\n",
      "Epoch 2935/6000, Training Loss: 11.7933, Validation Loss: 11.7984\n",
      "Epoch 2936/6000, Training Loss: 11.7919, Validation Loss: 11.7970\n",
      "Epoch 2937/6000, Training Loss: 11.7906, Validation Loss: 11.7957\n",
      "Epoch 2938/6000, Training Loss: 11.7892, Validation Loss: 11.7944\n",
      "Epoch 2939/6000, Training Loss: 11.7879, Validation Loss: 11.7931\n",
      "Epoch 2940/6000, Training Loss: 11.7866, Validation Loss: 11.7918\n",
      "Epoch 2941/6000, Training Loss: 11.7852, Validation Loss: 11.7905\n",
      "Epoch 2942/6000, Training Loss: 11.7839, Validation Loss: 11.7892\n",
      "Epoch 2943/6000, Training Loss: 11.7826, Validation Loss: 11.7879\n",
      "Epoch 2944/6000, Training Loss: 11.7813, Validation Loss: 11.7866\n",
      "Epoch 2945/6000, Training Loss: 11.7800, Validation Loss: 11.7853\n",
      "Epoch 2946/6000, Training Loss: 11.7786, Validation Loss: 11.7840\n",
      "Epoch 2947/6000, Training Loss: 11.7773, Validation Loss: 11.7827\n",
      "Epoch 2948/6000, Training Loss: 11.7760, Validation Loss: 11.7814\n",
      "Epoch 2949/6000, Training Loss: 11.7747, Validation Loss: 11.7801\n",
      "Epoch 2950/6000, Training Loss: 11.7734, Validation Loss: 11.7788\n",
      "Epoch 2951/6000, Training Loss: 11.7721, Validation Loss: 11.7775\n",
      "Epoch 2952/6000, Training Loss: 11.7708, Validation Loss: 11.7763\n",
      "Epoch 2953/6000, Training Loss: 11.7695, Validation Loss: 11.7750\n",
      "Epoch 2954/6000, Training Loss: 11.7682, Validation Loss: 11.7737\n",
      "Epoch 2955/6000, Training Loss: 11.7669, Validation Loss: 11.7724\n",
      "Epoch 2956/6000, Training Loss: 11.7656, Validation Loss: 11.7711\n",
      "Epoch 2957/6000, Training Loss: 11.7643, Validation Loss: 11.7699\n",
      "Epoch 2958/6000, Training Loss: 11.7630, Validation Loss: 11.7686\n",
      "Epoch 2959/6000, Training Loss: 11.7617, Validation Loss: 11.7673\n",
      "Epoch 2960/6000, Training Loss: 11.7604, Validation Loss: 11.7660\n",
      "Epoch 2961/6000, Training Loss: 11.7591, Validation Loss: 11.7648\n",
      "Epoch 2962/6000, Training Loss: 11.7578, Validation Loss: 11.7635\n",
      "Epoch 2963/6000, Training Loss: 11.7565, Validation Loss: 11.7622\n",
      "Epoch 2964/6000, Training Loss: 11.7553, Validation Loss: 11.7610\n",
      "Epoch 2965/6000, Training Loss: 11.7540, Validation Loss: 11.7597\n",
      "Epoch 2966/6000, Training Loss: 11.7527, Validation Loss: 11.7584\n",
      "Epoch 2967/6000, Training Loss: 11.7514, Validation Loss: 11.7572\n",
      "Epoch 2968/6000, Training Loss: 11.7501, Validation Loss: 11.7559\n",
      "Epoch 2969/6000, Training Loss: 11.7489, Validation Loss: 11.7546\n",
      "Epoch 2970/6000, Training Loss: 11.7476, Validation Loss: 11.7534\n",
      "Epoch 2971/6000, Training Loss: 11.7463, Validation Loss: 11.7521\n",
      "Epoch 2972/6000, Training Loss: 11.7450, Validation Loss: 11.7509\n",
      "Epoch 2973/6000, Training Loss: 11.7438, Validation Loss: 11.7496\n",
      "Epoch 2974/6000, Training Loss: 11.7425, Validation Loss: 11.7484\n",
      "Epoch 2975/6000, Training Loss: 11.7413, Validation Loss: 11.7472\n",
      "Epoch 2976/6000, Training Loss: 11.7400, Validation Loss: 11.7459\n",
      "Epoch 2977/6000, Training Loss: 11.7388, Validation Loss: 11.7447\n",
      "Epoch 2978/6000, Training Loss: 11.7375, Validation Loss: 11.7435\n",
      "Epoch 2979/6000, Training Loss: 11.7363, Validation Loss: 11.7423\n",
      "Epoch 2980/6000, Training Loss: 11.7351, Validation Loss: 11.7411\n",
      "Epoch 2981/6000, Training Loss: 11.7339, Validation Loss: 11.7399\n",
      "Epoch 2982/6000, Training Loss: 11.7327, Validation Loss: 11.7387\n",
      "Epoch 2983/6000, Training Loss: 11.7315, Validation Loss: 11.7375\n",
      "Epoch 2984/6000, Training Loss: 11.7303, Validation Loss: 11.7363\n",
      "Epoch 2985/6000, Training Loss: 11.7291, Validation Loss: 11.7351\n",
      "Epoch 2986/6000, Training Loss: 11.7279, Validation Loss: 11.7340\n",
      "Epoch 2987/6000, Training Loss: 11.7267, Validation Loss: 11.7328\n",
      "Epoch 2988/6000, Training Loss: 11.7255, Validation Loss: 11.7317\n",
      "Epoch 2989/6000, Training Loss: 11.7243, Validation Loss: 11.7305\n",
      "Epoch 2990/6000, Training Loss: 11.7232, Validation Loss: 11.7294\n",
      "Epoch 2991/6000, Training Loss: 11.7220, Validation Loss: 11.7282\n",
      "Epoch 2992/6000, Training Loss: 11.7209, Validation Loss: 11.7271\n",
      "Epoch 2993/6000, Training Loss: 11.7197, Validation Loss: 11.7260\n",
      "Epoch 2994/6000, Training Loss: 11.7186, Validation Loss: 11.7249\n",
      "Epoch 2995/6000, Training Loss: 11.7174, Validation Loss: 11.7237\n",
      "Epoch 2996/6000, Training Loss: 11.7163, Validation Loss: 11.7226\n",
      "Epoch 2997/6000, Training Loss: 11.7152, Validation Loss: 11.7215\n",
      "Epoch 2998/6000, Training Loss: 11.7140, Validation Loss: 11.7204\n",
      "Epoch 2999/6000, Training Loss: 11.7129, Validation Loss: 11.7193\n",
      "Epoch 3000/6000, Training Loss: 11.7118, Validation Loss: 11.7182\n",
      "Epoch 3001/6000, Training Loss: 11.7106, Validation Loss: 11.7171\n",
      "Epoch 3002/6000, Training Loss: 11.7095, Validation Loss: 11.7160\n",
      "Epoch 3003/6000, Training Loss: 11.7084, Validation Loss: 11.7149\n",
      "Epoch 3004/6000, Training Loss: 11.7073, Validation Loss: 11.7138\n",
      "Epoch 3005/6000, Training Loss: 11.7062, Validation Loss: 11.7127\n",
      "Epoch 3006/6000, Training Loss: 11.7051, Validation Loss: 11.7116\n",
      "Epoch 3007/6000, Training Loss: 11.7039, Validation Loss: 11.7105\n",
      "Epoch 3008/6000, Training Loss: 11.7028, Validation Loss: 11.7094\n",
      "Epoch 3009/6000, Training Loss: 11.7017, Validation Loss: 11.7084\n",
      "Epoch 3010/6000, Training Loss: 11.7006, Validation Loss: 11.7073\n",
      "Epoch 3011/6000, Training Loss: 11.6995, Validation Loss: 11.7062\n",
      "Epoch 3012/6000, Training Loss: 11.6984, Validation Loss: 11.7051\n",
      "Epoch 3013/6000, Training Loss: 11.6973, Validation Loss: 11.7040\n",
      "Epoch 3014/6000, Training Loss: 11.6962, Validation Loss: 11.7029\n",
      "Epoch 3015/6000, Training Loss: 11.6951, Validation Loss: 11.7019\n",
      "Epoch 3016/6000, Training Loss: 11.6940, Validation Loss: 11.7008\n",
      "Epoch 3017/6000, Training Loss: 11.6929, Validation Loss: 11.6997\n",
      "Epoch 3018/6000, Training Loss: 11.6918, Validation Loss: 11.6987\n",
      "Epoch 3019/6000, Training Loss: 11.6907, Validation Loss: 11.6976\n",
      "Epoch 3020/6000, Training Loss: 11.6896, Validation Loss: 11.6965\n",
      "Epoch 3021/6000, Training Loss: 11.6885, Validation Loss: 11.6954\n",
      "Epoch 3022/6000, Training Loss: 11.6874, Validation Loss: 11.6944\n",
      "Epoch 3023/6000, Training Loss: 11.6863, Validation Loss: 11.6933\n",
      "Epoch 3024/6000, Training Loss: 11.6853, Validation Loss: 11.6923\n",
      "Epoch 3025/6000, Training Loss: 11.6842, Validation Loss: 11.6912\n",
      "Epoch 3026/6000, Training Loss: 11.6831, Validation Loss: 11.6901\n",
      "Epoch 3027/6000, Training Loss: 11.6820, Validation Loss: 11.6891\n",
      "Epoch 3028/6000, Training Loss: 11.6810, Validation Loss: 11.6880\n",
      "Epoch 3029/6000, Training Loss: 11.6799, Validation Loss: 11.6870\n",
      "Epoch 3030/6000, Training Loss: 11.6788, Validation Loss: 11.6859\n",
      "Epoch 3031/6000, Training Loss: 11.6777, Validation Loss: 11.6849\n",
      "Epoch 3032/6000, Training Loss: 11.6767, Validation Loss: 11.6838\n",
      "Epoch 3033/6000, Training Loss: 11.6756, Validation Loss: 11.6828\n",
      "Epoch 3034/6000, Training Loss: 11.6745, Validation Loss: 11.6817\n",
      "Epoch 3035/6000, Training Loss: 11.6735, Validation Loss: 11.6807\n",
      "Epoch 3036/6000, Training Loss: 11.6724, Validation Loss: 11.6797\n",
      "Epoch 3037/6000, Training Loss: 11.6714, Validation Loss: 11.6786\n",
      "Epoch 3038/6000, Training Loss: 11.6703, Validation Loss: 11.6776\n",
      "Epoch 3039/6000, Training Loss: 11.6693, Validation Loss: 11.6765\n",
      "Epoch 3040/6000, Training Loss: 11.6682, Validation Loss: 11.6755\n",
      "Epoch 3041/6000, Training Loss: 11.6671, Validation Loss: 11.6745\n",
      "Epoch 3042/6000, Training Loss: 11.6661, Validation Loss: 11.6734\n",
      "Epoch 3043/6000, Training Loss: 11.6650, Validation Loss: 11.6724\n",
      "Epoch 3044/6000, Training Loss: 11.6640, Validation Loss: 11.6714\n",
      "Epoch 3045/6000, Training Loss: 11.6630, Validation Loss: 11.6704\n",
      "Epoch 3046/6000, Training Loss: 11.6619, Validation Loss: 11.6693\n",
      "Epoch 3047/6000, Training Loss: 11.6609, Validation Loss: 11.6683\n",
      "Epoch 3048/6000, Training Loss: 11.6598, Validation Loss: 11.6673\n",
      "Epoch 3049/6000, Training Loss: 11.6588, Validation Loss: 11.6662\n",
      "Epoch 3050/6000, Training Loss: 11.6577, Validation Loss: 11.6652\n",
      "Epoch 3051/6000, Training Loss: 11.6567, Validation Loss: 11.6642\n",
      "Epoch 3052/6000, Training Loss: 11.6557, Validation Loss: 11.6632\n",
      "Epoch 3053/6000, Training Loss: 11.6546, Validation Loss: 11.6622\n",
      "Epoch 3054/6000, Training Loss: 11.6536, Validation Loss: 11.6611\n",
      "Epoch 3055/6000, Training Loss: 11.6526, Validation Loss: 11.6601\n",
      "Epoch 3056/6000, Training Loss: 11.6516, Validation Loss: 11.6591\n",
      "Epoch 3057/6000, Training Loss: 11.6505, Validation Loss: 11.6581\n",
      "Epoch 3058/6000, Training Loss: 11.6495, Validation Loss: 11.6571\n",
      "Epoch 3059/6000, Training Loss: 11.6485, Validation Loss: 11.6561\n",
      "Epoch 3060/6000, Training Loss: 11.6475, Validation Loss: 11.6552\n",
      "Epoch 3061/6000, Training Loss: 11.6465, Validation Loss: 11.6542\n",
      "Epoch 3062/6000, Training Loss: 11.6455, Validation Loss: 11.6532\n",
      "Epoch 3063/6000, Training Loss: 11.6445, Validation Loss: 11.6522\n",
      "Epoch 3064/6000, Training Loss: 11.6435, Validation Loss: 11.6513\n",
      "Epoch 3065/6000, Training Loss: 11.6426, Validation Loss: 11.6503\n",
      "Epoch 3066/6000, Training Loss: 11.6416, Validation Loss: 11.6494\n",
      "Epoch 3067/6000, Training Loss: 11.6406, Validation Loss: 11.6484\n",
      "Epoch 3068/6000, Training Loss: 11.6397, Validation Loss: 11.6475\n",
      "Epoch 3069/6000, Training Loss: 11.6387, Validation Loss: 11.6466\n",
      "Epoch 3070/6000, Training Loss: 11.6378, Validation Loss: 11.6456\n",
      "Epoch 3071/6000, Training Loss: 11.6368, Validation Loss: 11.6447\n",
      "Epoch 3072/6000, Training Loss: 11.6359, Validation Loss: 11.6438\n",
      "Epoch 3073/6000, Training Loss: 11.6349, Validation Loss: 11.6429\n",
      "Epoch 3074/6000, Training Loss: 11.6340, Validation Loss: 11.6420\n",
      "Epoch 3075/6000, Training Loss: 11.6331, Validation Loss: 11.6411\n",
      "Epoch 3076/6000, Training Loss: 11.6321, Validation Loss: 11.6402\n",
      "Epoch 3077/6000, Training Loss: 11.6312, Validation Loss: 11.6393\n",
      "Epoch 3078/6000, Training Loss: 11.6303, Validation Loss: 11.6384\n",
      "Epoch 3079/6000, Training Loss: 11.6294, Validation Loss: 11.6375\n",
      "Epoch 3080/6000, Training Loss: 11.6284, Validation Loss: 11.6366\n",
      "Epoch 3081/6000, Training Loss: 11.6275, Validation Loss: 11.6357\n",
      "Epoch 3082/6000, Training Loss: 11.6266, Validation Loss: 11.6348\n",
      "Epoch 3083/6000, Training Loss: 11.6257, Validation Loss: 11.6340\n",
      "Epoch 3084/6000, Training Loss: 11.6248, Validation Loss: 11.6331\n",
      "Epoch 3085/6000, Training Loss: 11.6239, Validation Loss: 11.6322\n",
      "Epoch 3086/6000, Training Loss: 11.6230, Validation Loss: 11.6313\n",
      "Epoch 3087/6000, Training Loss: 11.6221, Validation Loss: 11.6304\n",
      "Epoch 3088/6000, Training Loss: 11.6212, Validation Loss: 11.6296\n",
      "Epoch 3089/6000, Training Loss: 11.6203, Validation Loss: 11.6287\n",
      "Epoch 3090/6000, Training Loss: 11.6194, Validation Loss: 11.6278\n",
      "Epoch 3091/6000, Training Loss: 11.6185, Validation Loss: 11.6269\n",
      "Epoch 3092/6000, Training Loss: 11.6176, Validation Loss: 11.6261\n",
      "Epoch 3093/6000, Training Loss: 11.6167, Validation Loss: 11.6252\n",
      "Epoch 3094/6000, Training Loss: 11.6158, Validation Loss: 11.6243\n",
      "Epoch 3095/6000, Training Loss: 11.6149, Validation Loss: 11.6235\n",
      "Epoch 3096/6000, Training Loss: 11.6141, Validation Loss: 11.6226\n",
      "Epoch 3097/6000, Training Loss: 11.6132, Validation Loss: 11.6217\n",
      "Epoch 3098/6000, Training Loss: 11.6123, Validation Loss: 11.6209\n",
      "Epoch 3099/6000, Training Loss: 11.6114, Validation Loss: 11.6200\n",
      "Epoch 3100/6000, Training Loss: 11.6105, Validation Loss: 11.6192\n",
      "Epoch 3101/6000, Training Loss: 11.6096, Validation Loss: 11.6183\n",
      "Epoch 3102/6000, Training Loss: 11.6088, Validation Loss: 11.6174\n",
      "Epoch 3103/6000, Training Loss: 11.6079, Validation Loss: 11.6166\n",
      "Epoch 3104/6000, Training Loss: 11.6070, Validation Loss: 11.6157\n",
      "Epoch 3105/6000, Training Loss: 11.6061, Validation Loss: 11.6149\n",
      "Epoch 3106/6000, Training Loss: 11.6052, Validation Loss: 11.6140\n",
      "Epoch 3107/6000, Training Loss: 11.6044, Validation Loss: 11.6132\n",
      "Epoch 3108/6000, Training Loss: 11.6035, Validation Loss: 11.6123\n",
      "Epoch 3109/6000, Training Loss: 11.6026, Validation Loss: 11.6115\n",
      "Epoch 3110/6000, Training Loss: 11.6018, Validation Loss: 11.6107\n",
      "Epoch 3111/6000, Training Loss: 11.6009, Validation Loss: 11.6098\n",
      "Epoch 3112/6000, Training Loss: 11.6000, Validation Loss: 11.6090\n",
      "Epoch 3113/6000, Training Loss: 11.5992, Validation Loss: 11.6081\n",
      "Epoch 3114/6000, Training Loss: 11.5983, Validation Loss: 11.6073\n",
      "Epoch 3115/6000, Training Loss: 11.5974, Validation Loss: 11.6065\n",
      "Epoch 3116/6000, Training Loss: 11.5966, Validation Loss: 11.6056\n",
      "Epoch 3117/6000, Training Loss: 11.5957, Validation Loss: 11.6048\n",
      "Epoch 3118/6000, Training Loss: 11.5949, Validation Loss: 11.6040\n",
      "Epoch 3119/6000, Training Loss: 11.5940, Validation Loss: 11.6031\n",
      "Epoch 3120/6000, Training Loss: 11.5932, Validation Loss: 11.6023\n",
      "Epoch 3121/6000, Training Loss: 11.5923, Validation Loss: 11.6015\n",
      "Epoch 3122/6000, Training Loss: 11.5915, Validation Loss: 11.6006\n",
      "Epoch 3123/6000, Training Loss: 11.5906, Validation Loss: 11.5998\n",
      "Epoch 3124/6000, Training Loss: 11.5898, Validation Loss: 11.5990\n",
      "Epoch 3125/6000, Training Loss: 11.5889, Validation Loss: 11.5982\n",
      "Epoch 3126/6000, Training Loss: 11.5881, Validation Loss: 11.5973\n",
      "Epoch 3127/6000, Training Loss: 11.5872, Validation Loss: 11.5965\n",
      "Epoch 3128/6000, Training Loss: 11.5864, Validation Loss: 11.5957\n",
      "Epoch 3129/6000, Training Loss: 11.5855, Validation Loss: 11.5949\n",
      "Epoch 3130/6000, Training Loss: 11.5847, Validation Loss: 11.5941\n",
      "Epoch 3131/6000, Training Loss: 11.5839, Validation Loss: 11.5933\n",
      "Epoch 3132/6000, Training Loss: 11.5830, Validation Loss: 11.5924\n",
      "Epoch 3133/6000, Training Loss: 11.5822, Validation Loss: 11.5916\n",
      "Epoch 3134/6000, Training Loss: 11.5814, Validation Loss: 11.5908\n",
      "Epoch 3135/6000, Training Loss: 11.5805, Validation Loss: 11.5900\n",
      "Epoch 3136/6000, Training Loss: 11.5797, Validation Loss: 11.5892\n",
      "Epoch 3137/6000, Training Loss: 11.5788, Validation Loss: 11.5884\n",
      "Epoch 3138/6000, Training Loss: 11.5780, Validation Loss: 11.5876\n",
      "Epoch 3139/6000, Training Loss: 11.5772, Validation Loss: 11.5868\n",
      "Epoch 3140/6000, Training Loss: 11.5764, Validation Loss: 11.5859\n",
      "Epoch 3141/6000, Training Loss: 11.5755, Validation Loss: 11.5851\n",
      "Epoch 3142/6000, Training Loss: 11.5747, Validation Loss: 11.5843\n",
      "Epoch 3143/6000, Training Loss: 11.5739, Validation Loss: 11.5835\n",
      "Epoch 3144/6000, Training Loss: 11.5731, Validation Loss: 11.5827\n",
      "Epoch 3145/6000, Training Loss: 11.5723, Validation Loss: 11.5820\n",
      "Epoch 3146/6000, Training Loss: 11.5714, Validation Loss: 11.5812\n",
      "Epoch 3147/6000, Training Loss: 11.5706, Validation Loss: 11.5804\n",
      "Epoch 3148/6000, Training Loss: 11.5698, Validation Loss: 11.5796\n",
      "Epoch 3149/6000, Training Loss: 11.5690, Validation Loss: 11.5788\n",
      "Epoch 3150/6000, Training Loss: 11.5682, Validation Loss: 11.5780\n",
      "Epoch 3151/6000, Training Loss: 11.5674, Validation Loss: 11.5773\n",
      "Epoch 3152/6000, Training Loss: 11.5667, Validation Loss: 11.5765\n",
      "Epoch 3153/6000, Training Loss: 11.5659, Validation Loss: 11.5757\n",
      "Epoch 3154/6000, Training Loss: 11.5651, Validation Loss: 11.5750\n",
      "Epoch 3155/6000, Training Loss: 11.5643, Validation Loss: 11.5742\n",
      "Epoch 3156/6000, Training Loss: 11.5636, Validation Loss: 11.5735\n",
      "Epoch 3157/6000, Training Loss: 11.5628, Validation Loss: 11.5728\n",
      "Epoch 3158/6000, Training Loss: 11.5620, Validation Loss: 11.5720\n",
      "Epoch 3159/6000, Training Loss: 11.5613, Validation Loss: 11.5713\n",
      "Epoch 3160/6000, Training Loss: 11.5605, Validation Loss: 11.5706\n",
      "Epoch 3161/6000, Training Loss: 11.5598, Validation Loss: 11.5698\n",
      "Epoch 3162/6000, Training Loss: 11.5590, Validation Loss: 11.5691\n",
      "Epoch 3163/6000, Training Loss: 11.5583, Validation Loss: 11.5684\n",
      "Epoch 3164/6000, Training Loss: 11.5576, Validation Loss: 11.5677\n",
      "Epoch 3165/6000, Training Loss: 11.5568, Validation Loss: 11.5670\n",
      "Epoch 3166/6000, Training Loss: 11.5561, Validation Loss: 11.5663\n",
      "Epoch 3167/6000, Training Loss: 11.5554, Validation Loss: 11.5656\n",
      "Epoch 3168/6000, Training Loss: 11.5547, Validation Loss: 11.5649\n",
      "Epoch 3169/6000, Training Loss: 11.5539, Validation Loss: 11.5642\n",
      "Epoch 3170/6000, Training Loss: 11.5532, Validation Loss: 11.5635\n",
      "Epoch 3171/6000, Training Loss: 11.5525, Validation Loss: 11.5628\n",
      "Epoch 3172/6000, Training Loss: 11.5518, Validation Loss: 11.5621\n",
      "Epoch 3173/6000, Training Loss: 11.5511, Validation Loss: 11.5614\n",
      "Epoch 3174/6000, Training Loss: 11.5504, Validation Loss: 11.5607\n",
      "Epoch 3175/6000, Training Loss: 11.5497, Validation Loss: 11.5600\n",
      "Epoch 3176/6000, Training Loss: 11.5490, Validation Loss: 11.5594\n",
      "Epoch 3177/6000, Training Loss: 11.5483, Validation Loss: 11.5587\n",
      "Epoch 3178/6000, Training Loss: 11.5476, Validation Loss: 11.5580\n",
      "Epoch 3179/6000, Training Loss: 11.5469, Validation Loss: 11.5573\n",
      "Epoch 3180/6000, Training Loss: 11.5462, Validation Loss: 11.5566\n",
      "Epoch 3181/6000, Training Loss: 11.5455, Validation Loss: 11.5559\n",
      "Epoch 3182/6000, Training Loss: 11.5448, Validation Loss: 11.5553\n",
      "Epoch 3183/6000, Training Loss: 11.5441, Validation Loss: 11.5546\n",
      "Epoch 3184/6000, Training Loss: 11.5434, Validation Loss: 11.5539\n",
      "Epoch 3185/6000, Training Loss: 11.5427, Validation Loss: 11.5532\n",
      "Epoch 3186/6000, Training Loss: 11.5420, Validation Loss: 11.5526\n",
      "Epoch 3187/6000, Training Loss: 11.5413, Validation Loss: 11.5519\n",
      "Epoch 3188/6000, Training Loss: 11.5406, Validation Loss: 11.5512\n",
      "Epoch 3189/6000, Training Loss: 11.5399, Validation Loss: 11.5505\n",
      "Epoch 3190/6000, Training Loss: 11.5392, Validation Loss: 11.5499\n",
      "Epoch 3191/6000, Training Loss: 11.5385, Validation Loss: 11.5492\n",
      "Epoch 3192/6000, Training Loss: 11.5378, Validation Loss: 11.5485\n",
      "Epoch 3193/6000, Training Loss: 11.5372, Validation Loss: 11.5479\n",
      "Epoch 3194/6000, Training Loss: 11.5365, Validation Loss: 11.5472\n",
      "Epoch 3195/6000, Training Loss: 11.5358, Validation Loss: 11.5465\n",
      "Epoch 3196/6000, Training Loss: 11.5351, Validation Loss: 11.5459\n",
      "Epoch 3197/6000, Training Loss: 11.5344, Validation Loss: 11.5452\n",
      "Epoch 3198/6000, Training Loss: 11.5337, Validation Loss: 11.5445\n",
      "Epoch 3199/6000, Training Loss: 11.5331, Validation Loss: 11.5439\n",
      "Epoch 3200/6000, Training Loss: 11.5324, Validation Loss: 11.5432\n",
      "Epoch 3201/6000, Training Loss: 11.5317, Validation Loss: 11.5426\n",
      "Epoch 3202/6000, Training Loss: 11.5310, Validation Loss: 11.5419\n",
      "Epoch 3203/6000, Training Loss: 11.5304, Validation Loss: 11.5413\n",
      "Epoch 3204/6000, Training Loss: 11.5297, Validation Loss: 11.5406\n",
      "Epoch 3205/6000, Training Loss: 11.5290, Validation Loss: 11.5400\n",
      "Epoch 3206/6000, Training Loss: 11.5284, Validation Loss: 11.5393\n",
      "Epoch 3207/6000, Training Loss: 11.5277, Validation Loss: 11.5387\n",
      "Epoch 3208/6000, Training Loss: 11.5270, Validation Loss: 11.5380\n",
      "Epoch 3209/6000, Training Loss: 11.5264, Validation Loss: 11.5374\n",
      "Epoch 3210/6000, Training Loss: 11.5257, Validation Loss: 11.5367\n",
      "Epoch 3211/6000, Training Loss: 11.5250, Validation Loss: 11.5361\n",
      "Epoch 3212/6000, Training Loss: 11.5244, Validation Loss: 11.5354\n",
      "Epoch 3213/6000, Training Loss: 11.5237, Validation Loss: 11.5348\n",
      "Epoch 3214/6000, Training Loss: 11.5231, Validation Loss: 11.5342\n",
      "Epoch 3215/6000, Training Loss: 11.5224, Validation Loss: 11.5335\n",
      "Epoch 3216/6000, Training Loss: 11.5218, Validation Loss: 11.5329\n",
      "Epoch 3217/6000, Training Loss: 11.5211, Validation Loss: 11.5322\n",
      "Epoch 3218/6000, Training Loss: 11.5205, Validation Loss: 11.5316\n",
      "Epoch 3219/6000, Training Loss: 11.5198, Validation Loss: 11.5310\n",
      "Epoch 3220/6000, Training Loss: 11.5192, Validation Loss: 11.5303\n",
      "Epoch 3221/6000, Training Loss: 11.5185, Validation Loss: 11.5297\n",
      "Epoch 3222/6000, Training Loss: 11.5179, Validation Loss: 11.5291\n",
      "Epoch 3223/6000, Training Loss: 11.5172, Validation Loss: 11.5284\n",
      "Epoch 3224/6000, Training Loss: 11.5166, Validation Loss: 11.5278\n",
      "Epoch 3225/6000, Training Loss: 11.5159, Validation Loss: 11.5272\n",
      "Epoch 3226/6000, Training Loss: 11.5153, Validation Loss: 11.5265\n",
      "Epoch 3227/6000, Training Loss: 11.5147, Validation Loss: 11.5259\n",
      "Epoch 3228/6000, Training Loss: 11.5140, Validation Loss: 11.5253\n",
      "Epoch 3229/6000, Training Loss: 11.5134, Validation Loss: 11.5247\n",
      "Epoch 3230/6000, Training Loss: 11.5127, Validation Loss: 11.5240\n",
      "Epoch 3231/6000, Training Loss: 11.5121, Validation Loss: 11.5234\n",
      "Epoch 3232/6000, Training Loss: 11.5115, Validation Loss: 11.5228\n",
      "Epoch 3233/6000, Training Loss: 11.5108, Validation Loss: 11.5221\n",
      "Epoch 3234/6000, Training Loss: 11.5102, Validation Loss: 11.5215\n",
      "Epoch 3235/6000, Training Loss: 11.5096, Validation Loss: 11.5209\n",
      "Epoch 3236/6000, Training Loss: 11.5089, Validation Loss: 11.5203\n",
      "Epoch 3237/6000, Training Loss: 11.5083, Validation Loss: 11.5196\n",
      "Epoch 3238/6000, Training Loss: 11.5077, Validation Loss: 11.5190\n",
      "Epoch 3239/6000, Training Loss: 11.5070, Validation Loss: 11.5184\n",
      "Epoch 3240/6000, Training Loss: 11.5064, Validation Loss: 11.5178\n",
      "Epoch 3241/6000, Training Loss: 11.5058, Validation Loss: 11.5172\n",
      "Epoch 3242/6000, Training Loss: 11.5052, Validation Loss: 11.5166\n",
      "Epoch 3243/6000, Training Loss: 11.5045, Validation Loss: 11.5160\n",
      "Epoch 3244/6000, Training Loss: 11.5039, Validation Loss: 11.5154\n",
      "Epoch 3245/6000, Training Loss: 11.5033, Validation Loss: 11.5147\n",
      "Epoch 3246/6000, Training Loss: 11.5027, Validation Loss: 11.5141\n",
      "Epoch 3247/6000, Training Loss: 11.5021, Validation Loss: 11.5136\n",
      "Epoch 3248/6000, Training Loss: 11.5015, Validation Loss: 11.5130\n",
      "Epoch 3249/6000, Training Loss: 11.5009, Validation Loss: 11.5124\n",
      "Epoch 3250/6000, Training Loss: 11.5003, Validation Loss: 11.5118\n",
      "Epoch 3251/6000, Training Loss: 11.4997, Validation Loss: 11.5112\n",
      "Epoch 3252/6000, Training Loss: 11.4991, Validation Loss: 11.5106\n",
      "Epoch 3253/6000, Training Loss: 11.4985, Validation Loss: 11.5101\n",
      "Epoch 3254/6000, Training Loss: 11.4979, Validation Loss: 11.5095\n",
      "Epoch 3255/6000, Training Loss: 11.4973, Validation Loss: 11.5089\n",
      "Epoch 3256/6000, Training Loss: 11.4968, Validation Loss: 11.5084\n",
      "Epoch 3257/6000, Training Loss: 11.4962, Validation Loss: 11.5078\n",
      "Epoch 3258/6000, Training Loss: 11.4956, Validation Loss: 11.5073\n",
      "Epoch 3259/6000, Training Loss: 11.4951, Validation Loss: 11.5067\n",
      "Epoch 3260/6000, Training Loss: 11.4945, Validation Loss: 11.5062\n",
      "Epoch 3261/6000, Training Loss: 11.4939, Validation Loss: 11.5056\n",
      "Epoch 3262/6000, Training Loss: 11.4934, Validation Loss: 11.5051\n",
      "Epoch 3263/6000, Training Loss: 11.4928, Validation Loss: 11.5046\n",
      "Epoch 3264/6000, Training Loss: 11.4923, Validation Loss: 11.5040\n",
      "Epoch 3265/6000, Training Loss: 11.4917, Validation Loss: 11.5035\n",
      "Epoch 3266/6000, Training Loss: 11.4912, Validation Loss: 11.5030\n",
      "Epoch 3267/6000, Training Loss: 11.4907, Validation Loss: 11.5024\n",
      "Epoch 3268/6000, Training Loss: 11.4901, Validation Loss: 11.5019\n",
      "Epoch 3269/6000, Training Loss: 11.4896, Validation Loss: 11.5014\n",
      "Epoch 3270/6000, Training Loss: 11.4891, Validation Loss: 11.5009\n",
      "Epoch 3271/6000, Training Loss: 11.4885, Validation Loss: 11.5004\n",
      "Epoch 3272/6000, Training Loss: 11.4880, Validation Loss: 11.4999\n",
      "Epoch 3273/6000, Training Loss: 11.4875, Validation Loss: 11.4993\n",
      "Epoch 3274/6000, Training Loss: 11.4869, Validation Loss: 11.4988\n",
      "Epoch 3275/6000, Training Loss: 11.4864, Validation Loss: 11.4983\n",
      "Epoch 3276/6000, Training Loss: 11.4859, Validation Loss: 11.4978\n",
      "Epoch 3277/6000, Training Loss: 11.4854, Validation Loss: 11.4973\n",
      "Epoch 3278/6000, Training Loss: 11.4849, Validation Loss: 11.4968\n",
      "Epoch 3279/6000, Training Loss: 11.4843, Validation Loss: 11.4963\n",
      "Epoch 3280/6000, Training Loss: 11.4838, Validation Loss: 11.4958\n",
      "Epoch 3281/6000, Training Loss: 11.4833, Validation Loss: 11.4953\n",
      "Epoch 3282/6000, Training Loss: 11.4828, Validation Loss: 11.4948\n",
      "Epoch 3283/6000, Training Loss: 11.4823, Validation Loss: 11.4943\n",
      "Epoch 3284/6000, Training Loss: 11.4818, Validation Loss: 11.4938\n",
      "Epoch 3285/6000, Training Loss: 11.4812, Validation Loss: 11.4933\n",
      "Epoch 3286/6000, Training Loss: 11.4807, Validation Loss: 11.4928\n",
      "Epoch 3287/6000, Training Loss: 11.4802, Validation Loss: 11.4923\n",
      "Epoch 3288/6000, Training Loss: 11.4797, Validation Loss: 11.4918\n",
      "Epoch 3289/6000, Training Loss: 11.4792, Validation Loss: 11.4913\n",
      "Epoch 3290/6000, Training Loss: 11.4787, Validation Loss: 11.4908\n",
      "Epoch 3291/6000, Training Loss: 11.4782, Validation Loss: 11.4903\n",
      "Epoch 3292/6000, Training Loss: 11.4777, Validation Loss: 11.4898\n",
      "Epoch 3293/6000, Training Loss: 11.4772, Validation Loss: 11.4893\n",
      "Epoch 3294/6000, Training Loss: 11.4767, Validation Loss: 11.4888\n",
      "Epoch 3295/6000, Training Loss: 11.4761, Validation Loss: 11.4883\n",
      "Epoch 3296/6000, Training Loss: 11.4756, Validation Loss: 11.4878\n",
      "Epoch 3297/6000, Training Loss: 11.4751, Validation Loss: 11.4873\n",
      "Epoch 3298/6000, Training Loss: 11.4746, Validation Loss: 11.4868\n",
      "Epoch 3299/6000, Training Loss: 11.4741, Validation Loss: 11.4864\n",
      "Epoch 3300/6000, Training Loss: 11.4736, Validation Loss: 11.4859\n",
      "Epoch 3301/6000, Training Loss: 11.4731, Validation Loss: 11.4854\n",
      "Epoch 3302/6000, Training Loss: 11.4726, Validation Loss: 11.4849\n",
      "Epoch 3303/6000, Training Loss: 11.4721, Validation Loss: 11.4844\n",
      "Epoch 3304/6000, Training Loss: 11.4716, Validation Loss: 11.4839\n",
      "Epoch 3305/6000, Training Loss: 11.4711, Validation Loss: 11.4834\n",
      "Epoch 3306/6000, Training Loss: 11.4706, Validation Loss: 11.4830\n",
      "Epoch 3307/6000, Training Loss: 11.4702, Validation Loss: 11.4825\n",
      "Epoch 3308/6000, Training Loss: 11.4697, Validation Loss: 11.4820\n",
      "Epoch 3309/6000, Training Loss: 11.4692, Validation Loss: 11.4815\n",
      "Epoch 3310/6000, Training Loss: 11.4687, Validation Loss: 11.4811\n",
      "Epoch 3311/6000, Training Loss: 11.4682, Validation Loss: 11.4806\n",
      "Epoch 3312/6000, Training Loss: 11.4677, Validation Loss: 11.4801\n",
      "Epoch 3313/6000, Training Loss: 11.4672, Validation Loss: 11.4796\n",
      "Epoch 3314/6000, Training Loss: 11.4667, Validation Loss: 11.4792\n",
      "Epoch 3315/6000, Training Loss: 11.4662, Validation Loss: 11.4787\n",
      "Epoch 3316/6000, Training Loss: 11.4658, Validation Loss: 11.4782\n",
      "Epoch 3317/6000, Training Loss: 11.4653, Validation Loss: 11.4777\n",
      "Epoch 3318/6000, Training Loss: 11.4648, Validation Loss: 11.4773\n",
      "Epoch 3319/6000, Training Loss: 11.4643, Validation Loss: 11.4768\n",
      "Epoch 3320/6000, Training Loss: 11.4638, Validation Loss: 11.4763\n",
      "Epoch 3321/6000, Training Loss: 11.4634, Validation Loss: 11.4759\n",
      "Epoch 3322/6000, Training Loss: 11.4629, Validation Loss: 11.4754\n",
      "Epoch 3323/6000, Training Loss: 11.4624, Validation Loss: 11.4750\n",
      "Epoch 3324/6000, Training Loss: 11.4619, Validation Loss: 11.4745\n",
      "Epoch 3325/6000, Training Loss: 11.4615, Validation Loss: 11.4740\n",
      "Epoch 3326/6000, Training Loss: 11.4610, Validation Loss: 11.4736\n",
      "Epoch 3327/6000, Training Loss: 11.4605, Validation Loss: 11.4731\n",
      "Epoch 3328/6000, Training Loss: 11.4601, Validation Loss: 11.4727\n",
      "Epoch 3329/6000, Training Loss: 11.4596, Validation Loss: 11.4722\n",
      "Epoch 3330/6000, Training Loss: 11.4591, Validation Loss: 11.4717\n",
      "Epoch 3331/6000, Training Loss: 11.4587, Validation Loss: 11.4713\n",
      "Epoch 3332/6000, Training Loss: 11.4582, Validation Loss: 11.4708\n",
      "Epoch 3333/6000, Training Loss: 11.4577, Validation Loss: 11.4704\n",
      "Epoch 3334/6000, Training Loss: 11.4573, Validation Loss: 11.4699\n",
      "Epoch 3335/6000, Training Loss: 11.4568, Validation Loss: 11.4695\n",
      "Epoch 3336/6000, Training Loss: 11.4563, Validation Loss: 11.4690\n",
      "Epoch 3337/6000, Training Loss: 11.4559, Validation Loss: 11.4686\n",
      "Epoch 3338/6000, Training Loss: 11.4554, Validation Loss: 11.4681\n",
      "Epoch 3339/6000, Training Loss: 11.4550, Validation Loss: 11.4677\n",
      "Epoch 3340/6000, Training Loss: 11.4545, Validation Loss: 11.4672\n",
      "Epoch 3341/6000, Training Loss: 11.4540, Validation Loss: 11.4668\n",
      "Epoch 3342/6000, Training Loss: 11.4536, Validation Loss: 11.4663\n",
      "Epoch 3343/6000, Training Loss: 11.4531, Validation Loss: 11.4658\n",
      "Epoch 3344/6000, Training Loss: 11.4527, Validation Loss: 11.4654\n",
      "Epoch 3345/6000, Training Loss: 11.4522, Validation Loss: 11.4649\n",
      "Epoch 3346/6000, Training Loss: 11.4517, Validation Loss: 11.4645\n",
      "Epoch 3347/6000, Training Loss: 11.4513, Validation Loss: 11.4640\n",
      "Epoch 3348/6000, Training Loss: 11.4508, Validation Loss: 11.4636\n",
      "Epoch 3349/6000, Training Loss: 11.4504, Validation Loss: 11.4632\n",
      "Epoch 3350/6000, Training Loss: 11.4499, Validation Loss: 11.4627\n",
      "Epoch 3351/6000, Training Loss: 11.4495, Validation Loss: 11.4623\n",
      "Epoch 3352/6000, Training Loss: 11.4490, Validation Loss: 11.4618\n",
      "Epoch 3353/6000, Training Loss: 11.4486, Validation Loss: 11.4614\n",
      "Epoch 3354/6000, Training Loss: 11.4481, Validation Loss: 11.4609\n",
      "Epoch 3355/6000, Training Loss: 11.4477, Validation Loss: 11.4605\n",
      "Epoch 3356/6000, Training Loss: 11.4472, Validation Loss: 11.4601\n",
      "Epoch 3357/6000, Training Loss: 11.4468, Validation Loss: 11.4596\n",
      "Epoch 3358/6000, Training Loss: 11.4463, Validation Loss: 11.4592\n",
      "Epoch 3359/6000, Training Loss: 11.4459, Validation Loss: 11.4588\n",
      "Epoch 3360/6000, Training Loss: 11.4455, Validation Loss: 11.4584\n",
      "Epoch 3361/6000, Training Loss: 11.4450, Validation Loss: 11.4579\n",
      "Epoch 3362/6000, Training Loss: 11.4446, Validation Loss: 11.4575\n",
      "Epoch 3363/6000, Training Loss: 11.4442, Validation Loss: 11.4571\n",
      "Epoch 3364/6000, Training Loss: 11.4438, Validation Loss: 11.4567\n",
      "Epoch 3365/6000, Training Loss: 11.4433, Validation Loss: 11.4563\n",
      "Epoch 3366/6000, Training Loss: 11.4429, Validation Loss: 11.4559\n",
      "Epoch 3367/6000, Training Loss: 11.4425, Validation Loss: 11.4555\n",
      "Epoch 3368/6000, Training Loss: 11.4421, Validation Loss: 11.4551\n",
      "Epoch 3369/6000, Training Loss: 11.4417, Validation Loss: 11.4547\n",
      "Epoch 3370/6000, Training Loss: 11.4413, Validation Loss: 11.4543\n",
      "Epoch 3371/6000, Training Loss: 11.4409, Validation Loss: 11.4539\n",
      "Epoch 3372/6000, Training Loss: 11.4405, Validation Loss: 11.4535\n",
      "Epoch 3373/6000, Training Loss: 11.4401, Validation Loss: 11.4531\n",
      "Epoch 3374/6000, Training Loss: 11.4397, Validation Loss: 11.4527\n",
      "Epoch 3375/6000, Training Loss: 11.4393, Validation Loss: 11.4523\n",
      "Epoch 3376/6000, Training Loss: 11.4389, Validation Loss: 11.4520\n",
      "Epoch 3377/6000, Training Loss: 11.4385, Validation Loss: 11.4516\n",
      "Epoch 3378/6000, Training Loss: 11.4381, Validation Loss: 11.4512\n",
      "Epoch 3379/6000, Training Loss: 11.4377, Validation Loss: 11.4509\n",
      "Epoch 3380/6000, Training Loss: 11.4373, Validation Loss: 11.4505\n",
      "Epoch 3381/6000, Training Loss: 11.4370, Validation Loss: 11.4501\n",
      "Epoch 3382/6000, Training Loss: 11.4366, Validation Loss: 11.4498\n",
      "Epoch 3383/6000, Training Loss: 11.4362, Validation Loss: 11.4494\n",
      "Epoch 3384/6000, Training Loss: 11.4358, Validation Loss: 11.4490\n",
      "Epoch 3385/6000, Training Loss: 11.4355, Validation Loss: 11.4487\n",
      "Epoch 3386/6000, Training Loss: 11.4351, Validation Loss: 11.4483\n",
      "Epoch 3387/6000, Training Loss: 11.4347, Validation Loss: 11.4480\n",
      "Epoch 3388/6000, Training Loss: 11.4344, Validation Loss: 11.4476\n",
      "Epoch 3389/6000, Training Loss: 11.4340, Validation Loss: 11.4473\n",
      "Epoch 3390/6000, Training Loss: 11.4336, Validation Loss: 11.4469\n",
      "Epoch 3391/6000, Training Loss: 11.4333, Validation Loss: 11.4466\n",
      "Epoch 3392/6000, Training Loss: 11.4329, Validation Loss: 11.4462\n",
      "Epoch 3393/6000, Training Loss: 11.4325, Validation Loss: 11.4459\n",
      "Epoch 3394/6000, Training Loss: 11.4322, Validation Loss: 11.4455\n",
      "Epoch 3395/6000, Training Loss: 11.4318, Validation Loss: 11.4452\n",
      "Epoch 3396/6000, Training Loss: 11.4315, Validation Loss: 11.4448\n",
      "Epoch 3397/6000, Training Loss: 11.4311, Validation Loss: 11.4445\n",
      "Epoch 3398/6000, Training Loss: 11.4307, Validation Loss: 11.4441\n",
      "Epoch 3399/6000, Training Loss: 11.4304, Validation Loss: 11.4438\n",
      "Epoch 3400/6000, Training Loss: 11.4300, Validation Loss: 11.4434\n",
      "Epoch 3401/6000, Training Loss: 11.4297, Validation Loss: 11.4431\n",
      "Epoch 3402/6000, Training Loss: 11.4293, Validation Loss: 11.4428\n",
      "Epoch 3403/6000, Training Loss: 11.4290, Validation Loss: 11.4424\n",
      "Epoch 3404/6000, Training Loss: 11.4286, Validation Loss: 11.4421\n",
      "Epoch 3405/6000, Training Loss: 11.4283, Validation Loss: 11.4417\n",
      "Epoch 3406/6000, Training Loss: 11.4279, Validation Loss: 11.4414\n",
      "Epoch 3407/6000, Training Loss: 11.4276, Validation Loss: 11.4410\n",
      "Epoch 3408/6000, Training Loss: 11.4272, Validation Loss: 11.4407\n",
      "Epoch 3409/6000, Training Loss: 11.4268, Validation Loss: 11.4404\n",
      "Epoch 3410/6000, Training Loss: 11.4265, Validation Loss: 11.4400\n",
      "Epoch 3411/6000, Training Loss: 11.4261, Validation Loss: 11.4397\n",
      "Epoch 3412/6000, Training Loss: 11.4258, Validation Loss: 11.4394\n",
      "Epoch 3413/6000, Training Loss: 11.4254, Validation Loss: 11.4390\n",
      "Epoch 3414/6000, Training Loss: 11.4251, Validation Loss: 11.4387\n",
      "Epoch 3415/6000, Training Loss: 11.4247, Validation Loss: 11.4383\n",
      "Epoch 3416/6000, Training Loss: 11.4244, Validation Loss: 11.4380\n",
      "Epoch 3417/6000, Training Loss: 11.4241, Validation Loss: 11.4377\n",
      "Epoch 3418/6000, Training Loss: 11.4237, Validation Loss: 11.4373\n",
      "Epoch 3419/6000, Training Loss: 11.4234, Validation Loss: 11.4370\n",
      "Epoch 3420/6000, Training Loss: 11.4230, Validation Loss: 11.4367\n",
      "Epoch 3421/6000, Training Loss: 11.4227, Validation Loss: 11.4363\n",
      "Epoch 3422/6000, Training Loss: 11.4223, Validation Loss: 11.4360\n",
      "Epoch 3423/6000, Training Loss: 11.4220, Validation Loss: 11.4357\n",
      "Epoch 3424/6000, Training Loss: 11.4216, Validation Loss: 11.4354\n",
      "Epoch 3425/6000, Training Loss: 11.4213, Validation Loss: 11.4350\n",
      "Epoch 3426/6000, Training Loss: 11.4210, Validation Loss: 11.4347\n",
      "Epoch 3427/6000, Training Loss: 11.4206, Validation Loss: 11.4344\n",
      "Epoch 3428/6000, Training Loss: 11.4203, Validation Loss: 11.4340\n",
      "Epoch 3429/6000, Training Loss: 11.4200, Validation Loss: 11.4337\n",
      "Epoch 3430/6000, Training Loss: 11.4196, Validation Loss: 11.4334\n",
      "Epoch 3431/6000, Training Loss: 11.4193, Validation Loss: 11.4331\n",
      "Epoch 3432/6000, Training Loss: 11.4189, Validation Loss: 11.4327\n",
      "Epoch 3433/6000, Training Loss: 11.4186, Validation Loss: 11.4324\n",
      "Epoch 3434/6000, Training Loss: 11.4183, Validation Loss: 11.4321\n",
      "Epoch 3435/6000, Training Loss: 11.4179, Validation Loss: 11.4318\n",
      "Epoch 3436/6000, Training Loss: 11.4176, Validation Loss: 11.4315\n",
      "Epoch 3437/6000, Training Loss: 11.4173, Validation Loss: 11.4311\n",
      "Epoch 3438/6000, Training Loss: 11.4170, Validation Loss: 11.4308\n",
      "Epoch 3439/6000, Training Loss: 11.4166, Validation Loss: 11.4305\n",
      "Epoch 3440/6000, Training Loss: 11.4163, Validation Loss: 11.4302\n",
      "Epoch 3441/6000, Training Loss: 11.4160, Validation Loss: 11.4299\n",
      "Epoch 3442/6000, Training Loss: 11.4156, Validation Loss: 11.4295\n",
      "Epoch 3443/6000, Training Loss: 11.4153, Validation Loss: 11.4292\n",
      "Epoch 3444/6000, Training Loss: 11.4150, Validation Loss: 11.4289\n",
      "Epoch 3445/6000, Training Loss: 11.4147, Validation Loss: 11.4286\n",
      "Epoch 3446/6000, Training Loss: 11.4143, Validation Loss: 11.4283\n",
      "Epoch 3447/6000, Training Loss: 11.4140, Validation Loss: 11.4280\n",
      "Epoch 3448/6000, Training Loss: 11.4137, Validation Loss: 11.4277\n",
      "Epoch 3449/6000, Training Loss: 11.4134, Validation Loss: 11.4274\n",
      "Epoch 3450/6000, Training Loss: 11.4131, Validation Loss: 11.4271\n",
      "Epoch 3451/6000, Training Loss: 11.4127, Validation Loss: 11.4267\n",
      "Epoch 3452/6000, Training Loss: 11.4124, Validation Loss: 11.4264\n",
      "Epoch 3453/6000, Training Loss: 11.4121, Validation Loss: 11.4261\n",
      "Epoch 3454/6000, Training Loss: 11.4118, Validation Loss: 11.4258\n",
      "Epoch 3455/6000, Training Loss: 11.4115, Validation Loss: 11.4255\n",
      "Epoch 3456/6000, Training Loss: 11.4112, Validation Loss: 11.4252\n",
      "Epoch 3457/6000, Training Loss: 11.4108, Validation Loss: 11.4249\n",
      "Epoch 3458/6000, Training Loss: 11.4105, Validation Loss: 11.4246\n",
      "Epoch 3459/6000, Training Loss: 11.4102, Validation Loss: 11.4243\n",
      "Epoch 3460/6000, Training Loss: 11.4099, Validation Loss: 11.4240\n",
      "Epoch 3461/6000, Training Loss: 11.4096, Validation Loss: 11.4237\n",
      "Epoch 3462/6000, Training Loss: 11.4093, Validation Loss: 11.4234\n",
      "Epoch 3463/6000, Training Loss: 11.4090, Validation Loss: 11.4231\n",
      "Epoch 3464/6000, Training Loss: 11.4087, Validation Loss: 11.4228\n",
      "Epoch 3465/6000, Training Loss: 11.4083, Validation Loss: 11.4225\n",
      "Epoch 3466/6000, Training Loss: 11.4080, Validation Loss: 11.4222\n",
      "Epoch 3467/6000, Training Loss: 11.4077, Validation Loss: 11.4219\n",
      "Epoch 3468/6000, Training Loss: 11.4074, Validation Loss: 11.4216\n",
      "Epoch 3469/6000, Training Loss: 11.4071, Validation Loss: 11.4213\n",
      "Epoch 3470/6000, Training Loss: 11.4068, Validation Loss: 11.4210\n",
      "Epoch 3471/6000, Training Loss: 11.4065, Validation Loss: 11.4207\n",
      "Epoch 3472/6000, Training Loss: 11.4062, Validation Loss: 11.4204\n",
      "Epoch 3473/6000, Training Loss: 11.4059, Validation Loss: 11.4201\n",
      "Epoch 3474/6000, Training Loss: 11.4056, Validation Loss: 11.4198\n",
      "Epoch 3475/6000, Training Loss: 11.4052, Validation Loss: 11.4195\n",
      "Epoch 3476/6000, Training Loss: 11.4049, Validation Loss: 11.4192\n",
      "Epoch 3477/6000, Training Loss: 11.4046, Validation Loss: 11.4189\n",
      "Epoch 3478/6000, Training Loss: 11.4043, Validation Loss: 11.4186\n",
      "Epoch 3479/6000, Training Loss: 11.4040, Validation Loss: 11.4183\n",
      "Epoch 3480/6000, Training Loss: 11.4037, Validation Loss: 11.4180\n",
      "Epoch 3481/6000, Training Loss: 11.4034, Validation Loss: 11.4177\n",
      "Epoch 3482/6000, Training Loss: 11.4031, Validation Loss: 11.4174\n",
      "Epoch 3483/6000, Training Loss: 11.4028, Validation Loss: 11.4171\n",
      "Epoch 3484/6000, Training Loss: 11.4025, Validation Loss: 11.4168\n",
      "Epoch 3485/6000, Training Loss: 11.4022, Validation Loss: 11.4165\n",
      "Epoch 3486/6000, Training Loss: 11.4019, Validation Loss: 11.4162\n",
      "Epoch 3487/6000, Training Loss: 11.4016, Validation Loss: 11.4160\n",
      "Epoch 3488/6000, Training Loss: 11.4013, Validation Loss: 11.4157\n",
      "Epoch 3489/6000, Training Loss: 11.4010, Validation Loss: 11.4154\n",
      "Epoch 3490/6000, Training Loss: 11.4007, Validation Loss: 11.4151\n",
      "Epoch 3491/6000, Training Loss: 11.4005, Validation Loss: 11.4148\n",
      "Epoch 3492/6000, Training Loss: 11.4002, Validation Loss: 11.4145\n",
      "Epoch 3493/6000, Training Loss: 11.3999, Validation Loss: 11.4143\n",
      "Epoch 3494/6000, Training Loss: 11.3996, Validation Loss: 11.4140\n",
      "Epoch 3495/6000, Training Loss: 11.3993, Validation Loss: 11.4137\n",
      "Epoch 3496/6000, Training Loss: 11.3990, Validation Loss: 11.4135\n",
      "Epoch 3497/6000, Training Loss: 11.3988, Validation Loss: 11.4132\n",
      "Epoch 3498/6000, Training Loss: 11.3985, Validation Loss: 11.4129\n",
      "Epoch 3499/6000, Training Loss: 11.3982, Validation Loss: 11.4127\n",
      "Epoch 3500/6000, Training Loss: 11.3979, Validation Loss: 11.4124\n",
      "Epoch 3501/6000, Training Loss: 11.3977, Validation Loss: 11.4121\n",
      "Epoch 3502/6000, Training Loss: 11.3974, Validation Loss: 11.4119\n",
      "Epoch 3503/6000, Training Loss: 11.3971, Validation Loss: 11.4116\n",
      "Epoch 3504/6000, Training Loss: 11.3969, Validation Loss: 11.4114\n",
      "Epoch 3505/6000, Training Loss: 11.3966, Validation Loss: 11.4111\n",
      "Epoch 3506/6000, Training Loss: 11.3963, Validation Loss: 11.4109\n",
      "Epoch 3507/6000, Training Loss: 11.3961, Validation Loss: 11.4106\n",
      "Epoch 3508/6000, Training Loss: 11.3958, Validation Loss: 11.4104\n",
      "Epoch 3509/6000, Training Loss: 11.3956, Validation Loss: 11.4101\n",
      "Epoch 3510/6000, Training Loss: 11.3953, Validation Loss: 11.4099\n",
      "Epoch 3511/6000, Training Loss: 11.3951, Validation Loss: 11.4097\n",
      "Epoch 3512/6000, Training Loss: 11.3948, Validation Loss: 11.4094\n",
      "Epoch 3513/6000, Training Loss: 11.3946, Validation Loss: 11.4092\n",
      "Epoch 3514/6000, Training Loss: 11.3943, Validation Loss: 11.4090\n",
      "Epoch 3515/6000, Training Loss: 11.3941, Validation Loss: 11.4087\n",
      "Epoch 3516/6000, Training Loss: 11.3939, Validation Loss: 11.4085\n",
      "Epoch 3517/6000, Training Loss: 11.3936, Validation Loss: 11.4083\n",
      "Epoch 3518/6000, Training Loss: 11.3934, Validation Loss: 11.4080\n",
      "Epoch 3519/6000, Training Loss: 11.3931, Validation Loss: 11.4078\n",
      "Epoch 3520/6000, Training Loss: 11.3929, Validation Loss: 11.4076\n",
      "Epoch 3521/6000, Training Loss: 11.3927, Validation Loss: 11.4074\n",
      "Epoch 3522/6000, Training Loss: 11.3924, Validation Loss: 11.4071\n",
      "Epoch 3523/6000, Training Loss: 11.3922, Validation Loss: 11.4069\n",
      "Epoch 3524/6000, Training Loss: 11.3920, Validation Loss: 11.4067\n",
      "Epoch 3525/6000, Training Loss: 11.3918, Validation Loss: 11.4065\n",
      "Epoch 3526/6000, Training Loss: 11.3915, Validation Loss: 11.4063\n",
      "Epoch 3527/6000, Training Loss: 11.3913, Validation Loss: 11.4061\n",
      "Epoch 3528/6000, Training Loss: 11.3911, Validation Loss: 11.4058\n",
      "Epoch 3529/6000, Training Loss: 11.3909, Validation Loss: 11.4056\n",
      "Epoch 3530/6000, Training Loss: 11.3906, Validation Loss: 11.4054\n",
      "Epoch 3531/6000, Training Loss: 11.3904, Validation Loss: 11.4052\n",
      "Epoch 3532/6000, Training Loss: 11.3902, Validation Loss: 11.4050\n",
      "Epoch 3533/6000, Training Loss: 11.3900, Validation Loss: 11.4048\n",
      "Epoch 3534/6000, Training Loss: 11.3897, Validation Loss: 11.4046\n",
      "Epoch 3535/6000, Training Loss: 11.3895, Validation Loss: 11.4043\n",
      "Epoch 3536/6000, Training Loss: 11.3893, Validation Loss: 11.4041\n",
      "Epoch 3537/6000, Training Loss: 11.3891, Validation Loss: 11.4039\n",
      "Epoch 3538/6000, Training Loss: 11.3889, Validation Loss: 11.4037\n",
      "Epoch 3539/6000, Training Loss: 11.3886, Validation Loss: 11.4035\n",
      "Epoch 3540/6000, Training Loss: 11.3884, Validation Loss: 11.4033\n",
      "Epoch 3541/6000, Training Loss: 11.3882, Validation Loss: 11.4031\n",
      "Epoch 3542/6000, Training Loss: 11.3880, Validation Loss: 11.4029\n",
      "Epoch 3543/6000, Training Loss: 11.3878, Validation Loss: 11.4027\n",
      "Epoch 3544/6000, Training Loss: 11.3876, Validation Loss: 11.4025\n",
      "Epoch 3545/6000, Training Loss: 11.3873, Validation Loss: 11.4022\n",
      "Epoch 3546/6000, Training Loss: 11.3871, Validation Loss: 11.4020\n",
      "Epoch 3547/6000, Training Loss: 11.3869, Validation Loss: 11.4018\n",
      "Epoch 3548/6000, Training Loss: 11.3867, Validation Loss: 11.4016\n",
      "Epoch 3549/6000, Training Loss: 11.3865, Validation Loss: 11.4014\n",
      "Epoch 3550/6000, Training Loss: 11.3863, Validation Loss: 11.4012\n",
      "Epoch 3551/6000, Training Loss: 11.3860, Validation Loss: 11.4010\n",
      "Epoch 3552/6000, Training Loss: 11.3858, Validation Loss: 11.4008\n",
      "Epoch 3553/6000, Training Loss: 11.3856, Validation Loss: 11.4006\n",
      "Epoch 3554/6000, Training Loss: 11.3854, Validation Loss: 11.4004\n",
      "Epoch 3555/6000, Training Loss: 11.3852, Validation Loss: 11.4002\n",
      "Epoch 3556/6000, Training Loss: 11.3850, Validation Loss: 11.4000\n",
      "Epoch 3557/6000, Training Loss: 11.3848, Validation Loss: 11.3998\n",
      "Epoch 3558/6000, Training Loss: 11.3845, Validation Loss: 11.3996\n",
      "Epoch 3559/6000, Training Loss: 11.3843, Validation Loss: 11.3994\n",
      "Epoch 3560/6000, Training Loss: 11.3841, Validation Loss: 11.3992\n",
      "Epoch 3561/6000, Training Loss: 11.3839, Validation Loss: 11.3990\n",
      "Epoch 3562/6000, Training Loss: 11.3837, Validation Loss: 11.3987\n",
      "Epoch 3563/6000, Training Loss: 11.3835, Validation Loss: 11.3985\n",
      "Epoch 3564/6000, Training Loss: 11.3833, Validation Loss: 11.3983\n",
      "Epoch 3565/6000, Training Loss: 11.3831, Validation Loss: 11.3981\n",
      "Epoch 3566/6000, Training Loss: 11.3829, Validation Loss: 11.3979\n",
      "Epoch 3567/6000, Training Loss: 11.3826, Validation Loss: 11.3977\n",
      "Epoch 3568/6000, Training Loss: 11.3824, Validation Loss: 11.3975\n",
      "Epoch 3569/6000, Training Loss: 11.3822, Validation Loss: 11.3973\n",
      "Epoch 3570/6000, Training Loss: 11.3820, Validation Loss: 11.3971\n",
      "Epoch 3571/6000, Training Loss: 11.3818, Validation Loss: 11.3969\n",
      "Epoch 3572/6000, Training Loss: 11.3816, Validation Loss: 11.3967\n",
      "Epoch 3573/6000, Training Loss: 11.3814, Validation Loss: 11.3965\n",
      "Epoch 3574/6000, Training Loss: 11.3812, Validation Loss: 11.3963\n",
      "Epoch 3575/6000, Training Loss: 11.3810, Validation Loss: 11.3961\n",
      "Epoch 3576/6000, Training Loss: 11.3808, Validation Loss: 11.3960\n",
      "Epoch 3577/6000, Training Loss: 11.3806, Validation Loss: 11.3958\n",
      "Epoch 3578/6000, Training Loss: 11.3804, Validation Loss: 11.3956\n",
      "Epoch 3579/6000, Training Loss: 11.3802, Validation Loss: 11.3954\n",
      "Epoch 3580/6000, Training Loss: 11.3800, Validation Loss: 11.3952\n",
      "Epoch 3581/6000, Training Loss: 11.3798, Validation Loss: 11.3950\n",
      "Epoch 3582/6000, Training Loss: 11.3796, Validation Loss: 11.3948\n",
      "Epoch 3583/6000, Training Loss: 11.3794, Validation Loss: 11.3946\n",
      "Epoch 3584/6000, Training Loss: 11.3792, Validation Loss: 11.3944\n",
      "Epoch 3585/6000, Training Loss: 11.3790, Validation Loss: 11.3942\n",
      "Epoch 3586/6000, Training Loss: 11.3788, Validation Loss: 11.3940\n",
      "Epoch 3587/6000, Training Loss: 11.3786, Validation Loss: 11.3938\n",
      "Epoch 3588/6000, Training Loss: 11.3784, Validation Loss: 11.3936\n",
      "Epoch 3589/6000, Training Loss: 11.3782, Validation Loss: 11.3935\n",
      "Epoch 3590/6000, Training Loss: 11.3780, Validation Loss: 11.3933\n",
      "Epoch 3591/6000, Training Loss: 11.3778, Validation Loss: 11.3931\n",
      "Epoch 3592/6000, Training Loss: 11.3776, Validation Loss: 11.3929\n",
      "Epoch 3593/6000, Training Loss: 11.3774, Validation Loss: 11.3927\n",
      "Epoch 3594/6000, Training Loss: 11.3772, Validation Loss: 11.3925\n",
      "Epoch 3595/6000, Training Loss: 11.3770, Validation Loss: 11.3923\n",
      "Epoch 3596/6000, Training Loss: 11.3768, Validation Loss: 11.3922\n",
      "Epoch 3597/6000, Training Loss: 11.3766, Validation Loss: 11.3920\n",
      "Epoch 3598/6000, Training Loss: 11.3764, Validation Loss: 11.3918\n",
      "Epoch 3599/6000, Training Loss: 11.3762, Validation Loss: 11.3916\n",
      "Epoch 3600/6000, Training Loss: 11.3760, Validation Loss: 11.3914\n",
      "Epoch 3601/6000, Training Loss: 11.3758, Validation Loss: 11.3912\n",
      "Epoch 3602/6000, Training Loss: 11.3756, Validation Loss: 11.3910\n",
      "Epoch 3603/6000, Training Loss: 11.3755, Validation Loss: 11.3909\n",
      "Epoch 3604/6000, Training Loss: 11.3753, Validation Loss: 11.3907\n",
      "Epoch 3605/6000, Training Loss: 11.3751, Validation Loss: 11.3905\n",
      "Epoch 3606/6000, Training Loss: 11.3749, Validation Loss: 11.3903\n",
      "Epoch 3607/6000, Training Loss: 11.3747, Validation Loss: 11.3901\n",
      "Epoch 3608/6000, Training Loss: 11.3745, Validation Loss: 11.3899\n",
      "Epoch 3609/6000, Training Loss: 11.3743, Validation Loss: 11.3898\n",
      "Epoch 3610/6000, Training Loss: 11.3741, Validation Loss: 11.3896\n",
      "Epoch 3611/6000, Training Loss: 11.3739, Validation Loss: 11.3894\n",
      "Epoch 3612/6000, Training Loss: 11.3738, Validation Loss: 11.3892\n",
      "Epoch 3613/6000, Training Loss: 11.3736, Validation Loss: 11.3890\n",
      "Epoch 3614/6000, Training Loss: 11.3734, Validation Loss: 11.3888\n",
      "Epoch 3615/6000, Training Loss: 11.3732, Validation Loss: 11.3887\n",
      "Epoch 3616/6000, Training Loss: 11.3730, Validation Loss: 11.3885\n",
      "Epoch 3617/6000, Training Loss: 11.3728, Validation Loss: 11.3883\n",
      "Epoch 3618/6000, Training Loss: 11.3726, Validation Loss: 11.3881\n",
      "Epoch 3619/6000, Training Loss: 11.3725, Validation Loss: 11.3879\n",
      "Epoch 3620/6000, Training Loss: 11.3723, Validation Loss: 11.3878\n",
      "Epoch 3621/6000, Training Loss: 11.3721, Validation Loss: 11.3876\n",
      "Epoch 3622/6000, Training Loss: 11.3719, Validation Loss: 11.3874\n",
      "Epoch 3623/6000, Training Loss: 11.3717, Validation Loss: 11.3872\n",
      "Epoch 3624/6000, Training Loss: 11.3715, Validation Loss: 11.3870\n",
      "Epoch 3625/6000, Training Loss: 11.3714, Validation Loss: 11.3869\n",
      "Epoch 3626/6000, Training Loss: 11.3712, Validation Loss: 11.3867\n",
      "Epoch 3627/6000, Training Loss: 11.3710, Validation Loss: 11.3865\n",
      "Epoch 3628/6000, Training Loss: 11.3708, Validation Loss: 11.3863\n",
      "Epoch 3629/6000, Training Loss: 11.3706, Validation Loss: 11.3862\n",
      "Epoch 3630/6000, Training Loss: 11.3704, Validation Loss: 11.3860\n",
      "Epoch 3631/6000, Training Loss: 11.3703, Validation Loss: 11.3858\n",
      "Epoch 3632/6000, Training Loss: 11.3701, Validation Loss: 11.3856\n",
      "Epoch 3633/6000, Training Loss: 11.3699, Validation Loss: 11.3855\n",
      "Epoch 3634/6000, Training Loss: 11.3697, Validation Loss: 11.3853\n",
      "Epoch 3635/6000, Training Loss: 11.3695, Validation Loss: 11.3851\n",
      "Epoch 3636/6000, Training Loss: 11.3694, Validation Loss: 11.3849\n",
      "Epoch 3637/6000, Training Loss: 11.3692, Validation Loss: 11.3848\n",
      "Epoch 3638/6000, Training Loss: 11.3690, Validation Loss: 11.3846\n",
      "Epoch 3639/6000, Training Loss: 11.3688, Validation Loss: 11.3844\n",
      "Epoch 3640/6000, Training Loss: 11.3686, Validation Loss: 11.3842\n",
      "Epoch 3641/6000, Training Loss: 11.3685, Validation Loss: 11.3841\n",
      "Epoch 3642/6000, Training Loss: 11.3683, Validation Loss: 11.3839\n",
      "Epoch 3643/6000, Training Loss: 11.3681, Validation Loss: 11.3837\n",
      "Epoch 3644/6000, Training Loss: 11.3679, Validation Loss: 11.3836\n",
      "Epoch 3645/6000, Training Loss: 11.3678, Validation Loss: 11.3834\n",
      "Epoch 3646/6000, Training Loss: 11.3676, Validation Loss: 11.3832\n",
      "Epoch 3647/6000, Training Loss: 11.3674, Validation Loss: 11.3831\n",
      "Epoch 3648/6000, Training Loss: 11.3672, Validation Loss: 11.3829\n",
      "Epoch 3649/6000, Training Loss: 11.3671, Validation Loss: 11.3827\n",
      "Epoch 3650/6000, Training Loss: 11.3669, Validation Loss: 11.3826\n",
      "Epoch 3651/6000, Training Loss: 11.3667, Validation Loss: 11.3824\n",
      "Epoch 3652/6000, Training Loss: 11.3665, Validation Loss: 11.3823\n",
      "Epoch 3653/6000, Training Loss: 11.3664, Validation Loss: 11.3821\n",
      "Epoch 3654/6000, Training Loss: 11.3662, Validation Loss: 11.3819\n",
      "Epoch 3655/6000, Training Loss: 11.3660, Validation Loss: 11.3818\n",
      "Epoch 3656/6000, Training Loss: 11.3659, Validation Loss: 11.3816\n",
      "Epoch 3657/6000, Training Loss: 11.3657, Validation Loss: 11.3815\n",
      "Epoch 3658/6000, Training Loss: 11.3656, Validation Loss: 11.3813\n",
      "Epoch 3659/6000, Training Loss: 11.3654, Validation Loss: 11.3812\n",
      "Epoch 3660/6000, Training Loss: 11.3652, Validation Loss: 11.3810\n",
      "Epoch 3661/6000, Training Loss: 11.3651, Validation Loss: 11.3809\n",
      "Epoch 3662/6000, Training Loss: 11.3649, Validation Loss: 11.3807\n",
      "Epoch 3663/6000, Training Loss: 11.3648, Validation Loss: 11.3806\n",
      "Epoch 3664/6000, Training Loss: 11.3646, Validation Loss: 11.3804\n",
      "Epoch 3665/6000, Training Loss: 11.3645, Validation Loss: 11.3803\n",
      "Epoch 3666/6000, Training Loss: 11.3643, Validation Loss: 11.3801\n",
      "Epoch 3667/6000, Training Loss: 11.3642, Validation Loss: 11.3800\n",
      "Epoch 3668/6000, Training Loss: 11.3640, Validation Loss: 11.3799\n",
      "Epoch 3669/6000, Training Loss: 11.3639, Validation Loss: 11.3797\n",
      "Epoch 3670/6000, Training Loss: 11.3637, Validation Loss: 11.3796\n",
      "Epoch 3671/6000, Training Loss: 11.3636, Validation Loss: 11.3795\n",
      "Epoch 3672/6000, Training Loss: 11.3634, Validation Loss: 11.3793\n",
      "Epoch 3673/6000, Training Loss: 11.3633, Validation Loss: 11.3792\n",
      "Epoch 3674/6000, Training Loss: 11.3631, Validation Loss: 11.3791\n",
      "Epoch 3675/6000, Training Loss: 11.3630, Validation Loss: 11.3789\n",
      "Epoch 3676/6000, Training Loss: 11.3629, Validation Loss: 11.3788\n",
      "Epoch 3677/6000, Training Loss: 11.3627, Validation Loss: 11.3787\n",
      "Epoch 3678/6000, Training Loss: 11.3626, Validation Loss: 11.3785\n",
      "Epoch 3679/6000, Training Loss: 11.3625, Validation Loss: 11.3784\n",
      "Epoch 3680/6000, Training Loss: 11.3623, Validation Loss: 11.3783\n",
      "Epoch 3681/6000, Training Loss: 11.3622, Validation Loss: 11.3782\n",
      "Epoch 3682/6000, Training Loss: 11.3621, Validation Loss: 11.3780\n",
      "Epoch 3683/6000, Training Loss: 11.3619, Validation Loss: 11.3779\n",
      "Epoch 3684/6000, Training Loss: 11.3618, Validation Loss: 11.3778\n",
      "Epoch 3685/6000, Training Loss: 11.3617, Validation Loss: 11.3777\n",
      "Epoch 3686/6000, Training Loss: 11.3615, Validation Loss: 11.3776\n",
      "Epoch 3687/6000, Training Loss: 11.3614, Validation Loss: 11.3775\n",
      "Epoch 3688/6000, Training Loss: 11.3613, Validation Loss: 11.3773\n",
      "Epoch 3689/6000, Training Loss: 11.3612, Validation Loss: 11.3772\n",
      "Epoch 3690/6000, Training Loss: 11.3610, Validation Loss: 11.3771\n",
      "Epoch 3691/6000, Training Loss: 11.3609, Validation Loss: 11.3770\n",
      "Epoch 3692/6000, Training Loss: 11.3608, Validation Loss: 11.3769\n",
      "Epoch 3693/6000, Training Loss: 11.3607, Validation Loss: 11.3768\n",
      "Epoch 3694/6000, Training Loss: 11.3605, Validation Loss: 11.3767\n",
      "Epoch 3695/6000, Training Loss: 11.3604, Validation Loss: 11.3766\n",
      "Epoch 3696/6000, Training Loss: 11.3603, Validation Loss: 11.3764\n",
      "Epoch 3697/6000, Training Loss: 11.3602, Validation Loss: 11.3763\n",
      "Epoch 3698/6000, Training Loss: 11.3601, Validation Loss: 11.3762\n",
      "Epoch 3699/6000, Training Loss: 11.3600, Validation Loss: 11.3761\n",
      "Epoch 3700/6000, Training Loss: 11.3598, Validation Loss: 11.3760\n",
      "Epoch 3701/6000, Training Loss: 11.3597, Validation Loss: 11.3759\n",
      "Epoch 3702/6000, Training Loss: 11.3596, Validation Loss: 11.3758\n",
      "Epoch 3703/6000, Training Loss: 11.3595, Validation Loss: 11.3757\n",
      "Epoch 3704/6000, Training Loss: 11.3594, Validation Loss: 11.3756\n",
      "Epoch 3705/6000, Training Loss: 11.3593, Validation Loss: 11.3755\n",
      "Epoch 3706/6000, Training Loss: 11.3591, Validation Loss: 11.3754\n",
      "Epoch 3707/6000, Training Loss: 11.3590, Validation Loss: 11.3753\n",
      "Epoch 3708/6000, Training Loss: 11.3589, Validation Loss: 11.3752\n",
      "Epoch 3709/6000, Training Loss: 11.3588, Validation Loss: 11.3751\n",
      "Epoch 3710/6000, Training Loss: 11.3587, Validation Loss: 11.3750\n",
      "Epoch 3711/6000, Training Loss: 11.3586, Validation Loss: 11.3749\n",
      "Epoch 3712/6000, Training Loss: 11.3585, Validation Loss: 11.3748\n",
      "Epoch 3713/6000, Training Loss: 11.3583, Validation Loss: 11.3747\n",
      "Epoch 3714/6000, Training Loss: 11.3582, Validation Loss: 11.3745\n",
      "Epoch 3715/6000, Training Loss: 11.3581, Validation Loss: 11.3744\n",
      "Epoch 3716/6000, Training Loss: 11.3580, Validation Loss: 11.3743\n",
      "Epoch 3717/6000, Training Loss: 11.3579, Validation Loss: 11.3742\n",
      "Epoch 3718/6000, Training Loss: 11.3578, Validation Loss: 11.3741\n",
      "Epoch 3719/6000, Training Loss: 11.3577, Validation Loss: 11.3740\n",
      "Epoch 3720/6000, Training Loss: 11.3576, Validation Loss: 11.3739\n",
      "Epoch 3721/6000, Training Loss: 11.3574, Validation Loss: 11.3738\n",
      "Epoch 3722/6000, Training Loss: 11.3573, Validation Loss: 11.3737\n",
      "Epoch 3723/6000, Training Loss: 11.3572, Validation Loss: 11.3736\n",
      "Epoch 3724/6000, Training Loss: 11.3571, Validation Loss: 11.3735\n",
      "Epoch 3725/6000, Training Loss: 11.3570, Validation Loss: 11.3734\n",
      "Epoch 3726/6000, Training Loss: 11.3569, Validation Loss: 11.3733\n",
      "Epoch 3727/6000, Training Loss: 11.3568, Validation Loss: 11.3732\n",
      "Epoch 3728/6000, Training Loss: 11.3567, Validation Loss: 11.3731\n",
      "Epoch 3729/6000, Training Loss: 11.3566, Validation Loss: 11.3730\n",
      "Epoch 3730/6000, Training Loss: 11.3564, Validation Loss: 11.3729\n",
      "Epoch 3731/6000, Training Loss: 11.3563, Validation Loss: 11.3728\n",
      "Epoch 3732/6000, Training Loss: 11.3562, Validation Loss: 11.3727\n",
      "Epoch 3733/6000, Training Loss: 11.3561, Validation Loss: 11.3726\n",
      "Epoch 3734/6000, Training Loss: 11.3560, Validation Loss: 11.3725\n",
      "Epoch 3735/6000, Training Loss: 11.3559, Validation Loss: 11.3724\n",
      "Epoch 3736/6000, Training Loss: 11.3558, Validation Loss: 11.3723\n",
      "Epoch 3737/6000, Training Loss: 11.3557, Validation Loss: 11.3722\n",
      "Epoch 3738/6000, Training Loss: 11.3556, Validation Loss: 11.3721\n",
      "Epoch 3739/6000, Training Loss: 11.3555, Validation Loss: 11.3720\n",
      "Epoch 3740/6000, Training Loss: 11.3553, Validation Loss: 11.3719\n",
      "Epoch 3741/6000, Training Loss: 11.3552, Validation Loss: 11.3718\n",
      "Epoch 3742/6000, Training Loss: 11.3551, Validation Loss: 11.3717\n",
      "Epoch 3743/6000, Training Loss: 11.3550, Validation Loss: 11.3716\n",
      "Epoch 3744/6000, Training Loss: 11.3549, Validation Loss: 11.3715\n",
      "Epoch 3745/6000, Training Loss: 11.3548, Validation Loss: 11.3714\n",
      "Epoch 3746/6000, Training Loss: 11.3547, Validation Loss: 11.3713\n",
      "Epoch 3747/6000, Training Loss: 11.3546, Validation Loss: 11.3713\n",
      "Epoch 3748/6000, Training Loss: 11.3545, Validation Loss: 11.3712\n",
      "Epoch 3749/6000, Training Loss: 11.3544, Validation Loss: 11.3711\n",
      "Epoch 3750/6000, Training Loss: 11.3543, Validation Loss: 11.3710\n",
      "Epoch 3751/6000, Training Loss: 11.3542, Validation Loss: 11.3709\n",
      "Epoch 3752/6000, Training Loss: 11.3541, Validation Loss: 11.3708\n",
      "Epoch 3753/6000, Training Loss: 11.3540, Validation Loss: 11.3707\n",
      "Epoch 3754/6000, Training Loss: 11.3538, Validation Loss: 11.3706\n",
      "Epoch 3755/6000, Training Loss: 11.3537, Validation Loss: 11.3705\n",
      "Epoch 3756/6000, Training Loss: 11.3536, Validation Loss: 11.3704\n",
      "Epoch 3757/6000, Training Loss: 11.3535, Validation Loss: 11.3703\n",
      "Epoch 3758/6000, Training Loss: 11.3534, Validation Loss: 11.3702\n",
      "Epoch 3759/6000, Training Loss: 11.3533, Validation Loss: 11.3701\n",
      "Epoch 3760/6000, Training Loss: 11.3532, Validation Loss: 11.3700\n",
      "Epoch 3761/6000, Training Loss: 11.3531, Validation Loss: 11.3699\n",
      "Epoch 3762/6000, Training Loss: 11.3530, Validation Loss: 11.3698\n",
      "Epoch 3763/6000, Training Loss: 11.3529, Validation Loss: 11.3697\n",
      "Epoch 3764/6000, Training Loss: 11.3528, Validation Loss: 11.3697\n",
      "Epoch 3765/6000, Training Loss: 11.3527, Validation Loss: 11.3696\n",
      "Epoch 3766/6000, Training Loss: 11.3526, Validation Loss: 11.3695\n",
      "Epoch 3767/6000, Training Loss: 11.3525, Validation Loss: 11.3694\n",
      "Epoch 3768/6000, Training Loss: 11.3524, Validation Loss: 11.3693\n",
      "Epoch 3769/6000, Training Loss: 11.3523, Validation Loss: 11.3692\n",
      "Epoch 3770/6000, Training Loss: 11.3522, Validation Loss: 11.3691\n",
      "Epoch 3771/6000, Training Loss: 11.3521, Validation Loss: 11.3690\n",
      "Epoch 3772/6000, Training Loss: 11.3520, Validation Loss: 11.3689\n",
      "Epoch 3773/6000, Training Loss: 11.3519, Validation Loss: 11.3688\n",
      "Epoch 3774/6000, Training Loss: 11.3518, Validation Loss: 11.3688\n",
      "Epoch 3775/6000, Training Loss: 11.3517, Validation Loss: 11.3687\n",
      "Epoch 3776/6000, Training Loss: 11.3516, Validation Loss: 11.3686\n",
      "Epoch 3777/6000, Training Loss: 11.3515, Validation Loss: 11.3685\n",
      "Epoch 3778/6000, Training Loss: 11.3514, Validation Loss: 11.3684\n",
      "Epoch 3779/6000, Training Loss: 11.3513, Validation Loss: 11.3683\n",
      "Epoch 3780/6000, Training Loss: 11.3512, Validation Loss: 11.3682\n",
      "Epoch 3781/6000, Training Loss: 11.3511, Validation Loss: 11.3682\n",
      "Epoch 3782/6000, Training Loss: 11.3510, Validation Loss: 11.3681\n",
      "Epoch 3783/6000, Training Loss: 11.3509, Validation Loss: 11.3680\n",
      "Epoch 3784/6000, Training Loss: 11.3508, Validation Loss: 11.3679\n",
      "Epoch 3785/6000, Training Loss: 11.3507, Validation Loss: 11.3678\n",
      "Epoch 3786/6000, Training Loss: 11.3506, Validation Loss: 11.3677\n",
      "Epoch 3787/6000, Training Loss: 11.3505, Validation Loss: 11.3676\n",
      "Epoch 3788/6000, Training Loss: 11.3504, Validation Loss: 11.3676\n",
      "Epoch 3789/6000, Training Loss: 11.3504, Validation Loss: 11.3675\n",
      "Epoch 3790/6000, Training Loss: 11.3503, Validation Loss: 11.3674\n",
      "Epoch 3791/6000, Training Loss: 11.3502, Validation Loss: 11.3673\n",
      "Epoch 3792/6000, Training Loss: 11.3501, Validation Loss: 11.3672\n",
      "Epoch 3793/6000, Training Loss: 11.3500, Validation Loss: 11.3671\n",
      "Epoch 3794/6000, Training Loss: 11.3499, Validation Loss: 11.3671\n",
      "Epoch 3795/6000, Training Loss: 11.3498, Validation Loss: 11.3670\n",
      "Epoch 3796/6000, Training Loss: 11.3497, Validation Loss: 11.3669\n",
      "Epoch 3797/6000, Training Loss: 11.3496, Validation Loss: 11.3668\n",
      "Epoch 3798/6000, Training Loss: 11.3495, Validation Loss: 11.3667\n",
      "Epoch 3799/6000, Training Loss: 11.3494, Validation Loss: 11.3666\n",
      "Epoch 3800/6000, Training Loss: 11.3494, Validation Loss: 11.3665\n",
      "Epoch 3801/6000, Training Loss: 11.3493, Validation Loss: 11.3665\n",
      "Epoch 3802/6000, Training Loss: 11.3492, Validation Loss: 11.3664\n",
      "Epoch 3803/6000, Training Loss: 11.3491, Validation Loss: 11.3663\n",
      "Epoch 3804/6000, Training Loss: 11.3490, Validation Loss: 11.3662\n",
      "Epoch 3805/6000, Training Loss: 11.3489, Validation Loss: 11.3661\n",
      "Epoch 3806/6000, Training Loss: 11.3488, Validation Loss: 11.3660\n",
      "Epoch 3807/6000, Training Loss: 11.3487, Validation Loss: 11.3659\n",
      "Epoch 3808/6000, Training Loss: 11.3486, Validation Loss: 11.3659\n",
      "Epoch 3809/6000, Training Loss: 11.3486, Validation Loss: 11.3658\n",
      "Epoch 3810/6000, Training Loss: 11.3485, Validation Loss: 11.3657\n",
      "Epoch 3811/6000, Training Loss: 11.3484, Validation Loss: 11.3656\n",
      "Epoch 3812/6000, Training Loss: 11.3483, Validation Loss: 11.3655\n",
      "Epoch 3813/6000, Training Loss: 11.3482, Validation Loss: 11.3654\n",
      "Epoch 3814/6000, Training Loss: 11.3481, Validation Loss: 11.3653\n",
      "Epoch 3815/6000, Training Loss: 11.3480, Validation Loss: 11.3653\n",
      "Epoch 3816/6000, Training Loss: 11.3479, Validation Loss: 11.3652\n",
      "Epoch 3817/6000, Training Loss: 11.3479, Validation Loss: 11.3651\n",
      "Epoch 3818/6000, Training Loss: 11.3478, Validation Loss: 11.3650\n",
      "Epoch 3819/6000, Training Loss: 11.3477, Validation Loss: 11.3649\n",
      "Epoch 3820/6000, Training Loss: 11.3476, Validation Loss: 11.3649\n",
      "Epoch 3821/6000, Training Loss: 11.3475, Validation Loss: 11.3648\n",
      "Epoch 3822/6000, Training Loss: 11.3474, Validation Loss: 11.3647\n",
      "Epoch 3823/6000, Training Loss: 11.3473, Validation Loss: 11.3646\n",
      "Epoch 3824/6000, Training Loss: 11.3473, Validation Loss: 11.3645\n",
      "Epoch 3825/6000, Training Loss: 11.3472, Validation Loss: 11.3644\n",
      "Epoch 3826/6000, Training Loss: 11.3471, Validation Loss: 11.3644\n",
      "Epoch 3827/6000, Training Loss: 11.3470, Validation Loss: 11.3643\n",
      "Epoch 3828/6000, Training Loss: 11.3469, Validation Loss: 11.3642\n",
      "Epoch 3829/6000, Training Loss: 11.3468, Validation Loss: 11.3641\n",
      "Epoch 3830/6000, Training Loss: 11.3468, Validation Loss: 11.3641\n",
      "Epoch 3831/6000, Training Loss: 11.3467, Validation Loss: 11.3640\n",
      "Epoch 3832/6000, Training Loss: 11.3466, Validation Loss: 11.3639\n",
      "Epoch 3833/6000, Training Loss: 11.3465, Validation Loss: 11.3638\n",
      "Epoch 3834/6000, Training Loss: 11.3464, Validation Loss: 11.3637\n",
      "Epoch 3835/6000, Training Loss: 11.3463, Validation Loss: 11.3637\n",
      "Epoch 3836/6000, Training Loss: 11.3463, Validation Loss: 11.3636\n",
      "Epoch 3837/6000, Training Loss: 11.3462, Validation Loss: 11.3635\n",
      "Epoch 3838/6000, Training Loss: 11.3461, Validation Loss: 11.3634\n",
      "Epoch 3839/6000, Training Loss: 11.3460, Validation Loss: 11.3634\n",
      "Epoch 3840/6000, Training Loss: 11.3459, Validation Loss: 11.3633\n",
      "Epoch 3841/6000, Training Loss: 11.3458, Validation Loss: 11.3632\n",
      "Epoch 3842/6000, Training Loss: 11.3458, Validation Loss: 11.3631\n",
      "Epoch 3843/6000, Training Loss: 11.3457, Validation Loss: 11.3630\n",
      "Epoch 3844/6000, Training Loss: 11.3456, Validation Loss: 11.3630\n",
      "Epoch 3845/6000, Training Loss: 11.3455, Validation Loss: 11.3629\n",
      "Epoch 3846/6000, Training Loss: 11.3454, Validation Loss: 11.3628\n",
      "Epoch 3847/6000, Training Loss: 11.3453, Validation Loss: 11.3627\n",
      "Epoch 3848/6000, Training Loss: 11.3453, Validation Loss: 11.3627\n",
      "Epoch 3849/6000, Training Loss: 11.3452, Validation Loss: 11.3626\n",
      "Epoch 3850/6000, Training Loss: 11.3451, Validation Loss: 11.3625\n",
      "Epoch 3851/6000, Training Loss: 11.3450, Validation Loss: 11.3624\n",
      "Epoch 3852/6000, Training Loss: 11.3449, Validation Loss: 11.3623\n",
      "Epoch 3853/6000, Training Loss: 11.3449, Validation Loss: 11.3623\n",
      "Epoch 3854/6000, Training Loss: 11.3448, Validation Loss: 11.3622\n",
      "Epoch 3855/6000, Training Loss: 11.3447, Validation Loss: 11.3621\n",
      "Epoch 3856/6000, Training Loss: 11.3446, Validation Loss: 11.3620\n",
      "Epoch 3857/6000, Training Loss: 11.3445, Validation Loss: 11.3620\n",
      "Epoch 3858/6000, Training Loss: 11.3445, Validation Loss: 11.3619\n",
      "Epoch 3859/6000, Training Loss: 11.3444, Validation Loss: 11.3618\n",
      "Epoch 3860/6000, Training Loss: 11.3443, Validation Loss: 11.3617\n",
      "Epoch 3861/6000, Training Loss: 11.3442, Validation Loss: 11.3617\n",
      "Epoch 3862/6000, Training Loss: 11.3441, Validation Loss: 11.3616\n",
      "Epoch 3863/6000, Training Loss: 11.3441, Validation Loss: 11.3615\n",
      "Epoch 3864/6000, Training Loss: 11.3440, Validation Loss: 11.3615\n",
      "Epoch 3865/6000, Training Loss: 11.3439, Validation Loss: 11.3614\n",
      "Epoch 3866/6000, Training Loss: 11.3438, Validation Loss: 11.3613\n",
      "Epoch 3867/6000, Training Loss: 11.3438, Validation Loss: 11.3612\n",
      "Epoch 3868/6000, Training Loss: 11.3437, Validation Loss: 11.3612\n",
      "Epoch 3869/6000, Training Loss: 11.3436, Validation Loss: 11.3611\n",
      "Epoch 3870/6000, Training Loss: 11.3435, Validation Loss: 11.3610\n",
      "Epoch 3871/6000, Training Loss: 11.3434, Validation Loss: 11.3610\n",
      "Epoch 3872/6000, Training Loss: 11.3434, Validation Loss: 11.3609\n",
      "Epoch 3873/6000, Training Loss: 11.3433, Validation Loss: 11.3608\n",
      "Epoch 3874/6000, Training Loss: 11.3432, Validation Loss: 11.3608\n",
      "Epoch 3875/6000, Training Loss: 11.3432, Validation Loss: 11.3607\n",
      "Epoch 3876/6000, Training Loss: 11.3431, Validation Loss: 11.3606\n",
      "Epoch 3877/6000, Training Loss: 11.3430, Validation Loss: 11.3606\n",
      "Epoch 3878/6000, Training Loss: 11.3429, Validation Loss: 11.3605\n",
      "Epoch 3879/6000, Training Loss: 11.3429, Validation Loss: 11.3604\n",
      "Epoch 3880/6000, Training Loss: 11.3428, Validation Loss: 11.3604\n",
      "Epoch 3881/6000, Training Loss: 11.3427, Validation Loss: 11.3603\n",
      "Epoch 3882/6000, Training Loss: 11.3427, Validation Loss: 11.3602\n",
      "Epoch 3883/6000, Training Loss: 11.3426, Validation Loss: 11.3602\n",
      "Epoch 3884/6000, Training Loss: 11.3425, Validation Loss: 11.3601\n",
      "Epoch 3885/6000, Training Loss: 11.3425, Validation Loss: 11.3601\n",
      "Epoch 3886/6000, Training Loss: 11.3424, Validation Loss: 11.3600\n",
      "Epoch 3887/6000, Training Loss: 11.3423, Validation Loss: 11.3599\n",
      "Epoch 3888/6000, Training Loss: 11.3423, Validation Loss: 11.3599\n",
      "Epoch 3889/6000, Training Loss: 11.3422, Validation Loss: 11.3598\n",
      "Epoch 3890/6000, Training Loss: 11.3421, Validation Loss: 11.3598\n",
      "Epoch 3891/6000, Training Loss: 11.3421, Validation Loss: 11.3597\n",
      "Epoch 3892/6000, Training Loss: 11.3420, Validation Loss: 11.3597\n",
      "Epoch 3893/6000, Training Loss: 11.3419, Validation Loss: 11.3596\n",
      "Epoch 3894/6000, Training Loss: 11.3419, Validation Loss: 11.3595\n",
      "Epoch 3895/6000, Training Loss: 11.3418, Validation Loss: 11.3595\n",
      "Epoch 3896/6000, Training Loss: 11.3418, Validation Loss: 11.3594\n",
      "Epoch 3897/6000, Training Loss: 11.3417, Validation Loss: 11.3594\n",
      "Epoch 3898/6000, Training Loss: 11.3417, Validation Loss: 11.3593\n",
      "Epoch 3899/6000, Training Loss: 11.3416, Validation Loss: 11.3593\n",
      "Epoch 3900/6000, Training Loss: 11.3415, Validation Loss: 11.3592\n",
      "Epoch 3901/6000, Training Loss: 11.3415, Validation Loss: 11.3592\n",
      "Epoch 3902/6000, Training Loss: 11.3414, Validation Loss: 11.3591\n",
      "Epoch 3903/6000, Training Loss: 11.3414, Validation Loss: 11.3591\n",
      "Epoch 3904/6000, Training Loss: 11.3413, Validation Loss: 11.3590\n",
      "Epoch 3905/6000, Training Loss: 11.3413, Validation Loss: 11.3590\n",
      "Epoch 3906/6000, Training Loss: 11.3412, Validation Loss: 11.3589\n",
      "Epoch 3907/6000, Training Loss: 11.3411, Validation Loss: 11.3589\n",
      "Epoch 3908/6000, Training Loss: 11.3411, Validation Loss: 11.3588\n",
      "Epoch 3909/6000, Training Loss: 11.3410, Validation Loss: 11.3588\n",
      "Epoch 3910/6000, Training Loss: 11.3410, Validation Loss: 11.3587\n",
      "Epoch 3911/6000, Training Loss: 11.3409, Validation Loss: 11.3587\n",
      "Epoch 3912/6000, Training Loss: 11.3409, Validation Loss: 11.3586\n",
      "Epoch 3913/6000, Training Loss: 11.3408, Validation Loss: 11.3586\n",
      "Epoch 3914/6000, Training Loss: 11.3408, Validation Loss: 11.3586\n",
      "Epoch 3915/6000, Training Loss: 11.3407, Validation Loss: 11.3585\n",
      "Epoch 3916/6000, Training Loss: 11.3407, Validation Loss: 11.3585\n",
      "Epoch 3917/6000, Training Loss: 11.3406, Validation Loss: 11.3584\n",
      "Epoch 3918/6000, Training Loss: 11.3406, Validation Loss: 11.3584\n",
      "Epoch 3919/6000, Training Loss: 11.3405, Validation Loss: 11.3583\n",
      "Epoch 3920/6000, Training Loss: 11.3405, Validation Loss: 11.3583\n",
      "Epoch 3921/6000, Training Loss: 11.3404, Validation Loss: 11.3582\n",
      "Epoch 3922/6000, Training Loss: 11.3404, Validation Loss: 11.3582\n",
      "Epoch 3923/6000, Training Loss: 11.3404, Validation Loss: 11.3582\n",
      "Epoch 3924/6000, Training Loss: 11.3403, Validation Loss: 11.3581\n",
      "Epoch 3925/6000, Training Loss: 11.3403, Validation Loss: 11.3581\n",
      "Epoch 3926/6000, Training Loss: 11.3402, Validation Loss: 11.3580\n",
      "Epoch 3927/6000, Training Loss: 11.3402, Validation Loss: 11.3580\n",
      "Epoch 3928/6000, Training Loss: 11.3401, Validation Loss: 11.3580\n",
      "Epoch 3929/6000, Training Loss: 11.3401, Validation Loss: 11.3579\n",
      "Epoch 3930/6000, Training Loss: 11.3400, Validation Loss: 11.3579\n",
      "Epoch 3931/6000, Training Loss: 11.3400, Validation Loss: 11.3578\n",
      "Epoch 3932/6000, Training Loss: 11.3400, Validation Loss: 11.3578\n",
      "Epoch 3933/6000, Training Loss: 11.3399, Validation Loss: 11.3578\n",
      "Epoch 3934/6000, Training Loss: 11.3399, Validation Loss: 11.3577\n",
      "Epoch 3935/6000, Training Loss: 11.3398, Validation Loss: 11.3577\n",
      "Epoch 3936/6000, Training Loss: 11.3398, Validation Loss: 11.3577\n",
      "Epoch 3937/6000, Training Loss: 11.3397, Validation Loss: 11.3576\n",
      "Epoch 3938/6000, Training Loss: 11.3397, Validation Loss: 11.3576\n",
      "Epoch 3939/6000, Training Loss: 11.3397, Validation Loss: 11.3575\n",
      "Epoch 3940/6000, Training Loss: 11.3396, Validation Loss: 11.3575\n",
      "Epoch 3941/6000, Training Loss: 11.3396, Validation Loss: 11.3575\n",
      "Epoch 3942/6000, Training Loss: 11.3395, Validation Loss: 11.3574\n",
      "Epoch 3943/6000, Training Loss: 11.3395, Validation Loss: 11.3574\n",
      "Epoch 3944/6000, Training Loss: 11.3395, Validation Loss: 11.3574\n",
      "Epoch 3945/6000, Training Loss: 11.3394, Validation Loss: 11.3573\n",
      "Epoch 3946/6000, Training Loss: 11.3394, Validation Loss: 11.3573\n",
      "Epoch 3947/6000, Training Loss: 11.3393, Validation Loss: 11.3573\n",
      "Epoch 3948/6000, Training Loss: 11.3393, Validation Loss: 11.3572\n",
      "Epoch 3949/6000, Training Loss: 11.3393, Validation Loss: 11.3572\n",
      "Epoch 3950/6000, Training Loss: 11.3392, Validation Loss: 11.3572\n",
      "Epoch 3951/6000, Training Loss: 11.3392, Validation Loss: 11.3571\n",
      "Epoch 3952/6000, Training Loss: 11.3391, Validation Loss: 11.3571\n",
      "Epoch 3953/6000, Training Loss: 11.3391, Validation Loss: 11.3571\n",
      "Epoch 3954/6000, Training Loss: 11.3391, Validation Loss: 11.3570\n",
      "Epoch 3955/6000, Training Loss: 11.3390, Validation Loss: 11.3570\n",
      "Epoch 3956/6000, Training Loss: 11.3390, Validation Loss: 11.3569\n",
      "Epoch 3957/6000, Training Loss: 11.3389, Validation Loss: 11.3569\n",
      "Epoch 3958/6000, Training Loss: 11.3389, Validation Loss: 11.3569\n",
      "Epoch 3959/6000, Training Loss: 11.3389, Validation Loss: 11.3568\n",
      "Epoch 3960/6000, Training Loss: 11.3388, Validation Loss: 11.3568\n",
      "Epoch 3961/6000, Training Loss: 11.3388, Validation Loss: 11.3568\n",
      "Epoch 3962/6000, Training Loss: 11.3387, Validation Loss: 11.3567\n",
      "Epoch 3963/6000, Training Loss: 11.3387, Validation Loss: 11.3567\n",
      "Epoch 3964/6000, Training Loss: 11.3387, Validation Loss: 11.3567\n",
      "Epoch 3965/6000, Training Loss: 11.3386, Validation Loss: 11.3566\n",
      "Epoch 3966/6000, Training Loss: 11.3386, Validation Loss: 11.3566\n",
      "Epoch 3967/6000, Training Loss: 11.3386, Validation Loss: 11.3566\n",
      "Epoch 3968/6000, Training Loss: 11.3385, Validation Loss: 11.3565\n",
      "Epoch 3969/6000, Training Loss: 11.3385, Validation Loss: 11.3565\n",
      "Epoch 3970/6000, Training Loss: 11.3384, Validation Loss: 11.3565\n",
      "Epoch 3971/6000, Training Loss: 11.3384, Validation Loss: 11.3564\n",
      "Epoch 3972/6000, Training Loss: 11.3384, Validation Loss: 11.3564\n",
      "Epoch 3973/6000, Training Loss: 11.3383, Validation Loss: 11.3564\n",
      "Epoch 3974/6000, Training Loss: 11.3383, Validation Loss: 11.3563\n",
      "Epoch 3975/6000, Training Loss: 11.3383, Validation Loss: 11.3563\n",
      "Epoch 3976/6000, Training Loss: 11.3382, Validation Loss: 11.3563\n",
      "Epoch 3977/6000, Training Loss: 11.3382, Validation Loss: 11.3562\n",
      "Epoch 3978/6000, Training Loss: 11.3381, Validation Loss: 11.3562\n",
      "Epoch 3979/6000, Training Loss: 11.3381, Validation Loss: 11.3562\n",
      "Epoch 3980/6000, Training Loss: 11.3381, Validation Loss: 11.3561\n",
      "Epoch 3981/6000, Training Loss: 11.3380, Validation Loss: 11.3561\n",
      "Epoch 3982/6000, Training Loss: 11.3380, Validation Loss: 11.3561\n",
      "Epoch 3983/6000, Training Loss: 11.3380, Validation Loss: 11.3561\n",
      "Epoch 3984/6000, Training Loss: 11.3379, Validation Loss: 11.3560\n",
      "Epoch 3985/6000, Training Loss: 11.3379, Validation Loss: 11.3560\n",
      "Epoch 3986/6000, Training Loss: 11.3378, Validation Loss: 11.3560\n",
      "Epoch 3987/6000, Training Loss: 11.3378, Validation Loss: 11.3559\n",
      "Epoch 3988/6000, Training Loss: 11.3378, Validation Loss: 11.3559\n",
      "Epoch 3989/6000, Training Loss: 11.3377, Validation Loss: 11.3559\n",
      "Epoch 3990/6000, Training Loss: 11.3377, Validation Loss: 11.3558\n",
      "Epoch 3991/6000, Training Loss: 11.3377, Validation Loss: 11.3558\n",
      "Epoch 3992/6000, Training Loss: 11.3376, Validation Loss: 11.3558\n",
      "Epoch 3993/6000, Training Loss: 11.3376, Validation Loss: 11.3557\n",
      "Epoch 3994/6000, Training Loss: 11.3375, Validation Loss: 11.3557\n",
      "Epoch 3995/6000, Training Loss: 11.3375, Validation Loss: 11.3557\n",
      "Epoch 3996/6000, Training Loss: 11.3375, Validation Loss: 11.3556\n",
      "Epoch 3997/6000, Training Loss: 11.3374, Validation Loss: 11.3556\n",
      "Epoch 3998/6000, Training Loss: 11.3374, Validation Loss: 11.3556\n",
      "Epoch 3999/6000, Training Loss: 11.3374, Validation Loss: 11.3555\n",
      "Epoch 4000/6000, Training Loss: 11.3373, Validation Loss: 11.3555\n",
      "Epoch 4001/6000, Training Loss: 11.3373, Validation Loss: 11.3555\n",
      "Epoch 4002/6000, Training Loss: 11.3373, Validation Loss: 11.3554\n",
      "Epoch 4003/6000, Training Loss: 11.3372, Validation Loss: 11.3554\n",
      "Epoch 4004/6000, Training Loss: 11.3372, Validation Loss: 11.3554\n",
      "Epoch 4005/6000, Training Loss: 11.3371, Validation Loss: 11.3554\n",
      "Epoch 4006/6000, Training Loss: 11.3371, Validation Loss: 11.3553\n",
      "Epoch 4007/6000, Training Loss: 11.3371, Validation Loss: 11.3553\n",
      "Epoch 4008/6000, Training Loss: 11.3370, Validation Loss: 11.3553\n",
      "Epoch 4009/6000, Training Loss: 11.3370, Validation Loss: 11.3552\n",
      "Epoch 4010/6000, Training Loss: 11.3370, Validation Loss: 11.3552\n",
      "Epoch 4011/6000, Training Loss: 11.3369, Validation Loss: 11.3552\n",
      "Epoch 4012/6000, Training Loss: 11.3369, Validation Loss: 11.3551\n",
      "Epoch 4013/6000, Training Loss: 11.3369, Validation Loss: 11.3551\n",
      "Epoch 4014/6000, Training Loss: 11.3368, Validation Loss: 11.3551\n",
      "Epoch 4015/6000, Training Loss: 11.3368, Validation Loss: 11.3550\n",
      "Epoch 4016/6000, Training Loss: 11.3368, Validation Loss: 11.3550\n",
      "Epoch 4017/6000, Training Loss: 11.3367, Validation Loss: 11.3550\n",
      "Epoch 4018/6000, Training Loss: 11.3367, Validation Loss: 11.3550\n",
      "Epoch 4019/6000, Training Loss: 11.3367, Validation Loss: 11.3549\n",
      "Epoch 4020/6000, Training Loss: 11.3366, Validation Loss: 11.3549\n",
      "Epoch 4021/6000, Training Loss: 11.3366, Validation Loss: 11.3549\n",
      "Epoch 4022/6000, Training Loss: 11.3366, Validation Loss: 11.3548\n",
      "Epoch 4023/6000, Training Loss: 11.3365, Validation Loss: 11.3548\n",
      "Epoch 4024/6000, Training Loss: 11.3365, Validation Loss: 11.3548\n",
      "Epoch 4025/6000, Training Loss: 11.3365, Validation Loss: 11.3548\n",
      "Epoch 4026/6000, Training Loss: 11.3364, Validation Loss: 11.3547\n",
      "Epoch 4027/6000, Training Loss: 11.3364, Validation Loss: 11.3547\n",
      "Epoch 4028/6000, Training Loss: 11.3364, Validation Loss: 11.3547\n",
      "Epoch 4029/6000, Training Loss: 11.3363, Validation Loss: 11.3546\n",
      "Epoch 4030/6000, Training Loss: 11.3363, Validation Loss: 11.3546\n",
      "Epoch 4031/6000, Training Loss: 11.3363, Validation Loss: 11.3546\n",
      "Epoch 4032/6000, Training Loss: 11.3362, Validation Loss: 11.3545\n",
      "Epoch 4033/6000, Training Loss: 11.3362, Validation Loss: 11.3545\n",
      "Epoch 4034/6000, Training Loss: 11.3362, Validation Loss: 11.3545\n",
      "Epoch 4035/6000, Training Loss: 11.3361, Validation Loss: 11.3545\n",
      "Epoch 4036/6000, Training Loss: 11.3361, Validation Loss: 11.3544\n",
      "Epoch 4037/6000, Training Loss: 11.3361, Validation Loss: 11.3544\n",
      "Epoch 4038/6000, Training Loss: 11.3360, Validation Loss: 11.3544\n",
      "Epoch 4039/6000, Training Loss: 11.3360, Validation Loss: 11.3544\n",
      "Epoch 4040/6000, Training Loss: 11.3360, Validation Loss: 11.3543\n",
      "Epoch 4041/6000, Training Loss: 11.3359, Validation Loss: 11.3543\n",
      "Epoch 4042/6000, Training Loss: 11.3359, Validation Loss: 11.3543\n",
      "Epoch 4043/6000, Training Loss: 11.3359, Validation Loss: 11.3542\n",
      "Epoch 4044/6000, Training Loss: 11.3358, Validation Loss: 11.3542\n",
      "Epoch 4045/6000, Training Loss: 11.3358, Validation Loss: 11.3542\n",
      "Epoch 4046/6000, Training Loss: 11.3358, Validation Loss: 11.3542\n",
      "Epoch 4047/6000, Training Loss: 11.3357, Validation Loss: 11.3541\n",
      "Epoch 4048/6000, Training Loss: 11.3357, Validation Loss: 11.3541\n",
      "Epoch 4049/6000, Training Loss: 11.3357, Validation Loss: 11.3541\n",
      "Epoch 4050/6000, Training Loss: 11.3357, Validation Loss: 11.3540\n",
      "Epoch 4051/6000, Training Loss: 11.3356, Validation Loss: 11.3540\n",
      "Epoch 4052/6000, Training Loss: 11.3356, Validation Loss: 11.3540\n",
      "Epoch 4053/6000, Training Loss: 11.3356, Validation Loss: 11.3540\n",
      "Epoch 4054/6000, Training Loss: 11.3355, Validation Loss: 11.3539\n",
      "Epoch 4055/6000, Training Loss: 11.3355, Validation Loss: 11.3539\n",
      "Epoch 4056/6000, Training Loss: 11.3355, Validation Loss: 11.3539\n",
      "Epoch 4057/6000, Training Loss: 11.3354, Validation Loss: 11.3539\n",
      "Epoch 4058/6000, Training Loss: 11.3354, Validation Loss: 11.3538\n",
      "Epoch 4059/6000, Training Loss: 11.3354, Validation Loss: 11.3538\n",
      "Epoch 4060/6000, Training Loss: 11.3354, Validation Loss: 11.3538\n",
      "Epoch 4061/6000, Training Loss: 11.3353, Validation Loss: 11.3538\n",
      "Epoch 4062/6000, Training Loss: 11.3353, Validation Loss: 11.3537\n",
      "Epoch 4063/6000, Training Loss: 11.3353, Validation Loss: 11.3537\n",
      "Epoch 4064/6000, Training Loss: 11.3352, Validation Loss: 11.3537\n",
      "Epoch 4065/6000, Training Loss: 11.3352, Validation Loss: 11.3537\n",
      "Epoch 4066/6000, Training Loss: 11.3352, Validation Loss: 11.3536\n",
      "Epoch 4067/6000, Training Loss: 11.3352, Validation Loss: 11.3536\n",
      "Epoch 4068/6000, Training Loss: 11.3351, Validation Loss: 11.3536\n",
      "Epoch 4069/6000, Training Loss: 11.3351, Validation Loss: 11.3536\n",
      "Epoch 4070/6000, Training Loss: 11.3351, Validation Loss: 11.3535\n",
      "Epoch 4071/6000, Training Loss: 11.3350, Validation Loss: 11.3535\n",
      "Epoch 4072/6000, Training Loss: 11.3350, Validation Loss: 11.3535\n",
      "Epoch 4073/6000, Training Loss: 11.3350, Validation Loss: 11.3535\n",
      "Epoch 4074/6000, Training Loss: 11.3350, Validation Loss: 11.3534\n",
      "Epoch 4075/6000, Training Loss: 11.3349, Validation Loss: 11.3534\n",
      "Epoch 4076/6000, Training Loss: 11.3349, Validation Loss: 11.3534\n",
      "Epoch 4077/6000, Training Loss: 11.3349, Validation Loss: 11.3534\n",
      "Epoch 4078/6000, Training Loss: 11.3348, Validation Loss: 11.3533\n",
      "Epoch 4079/6000, Training Loss: 11.3348, Validation Loss: 11.3533\n",
      "Epoch 4080/6000, Training Loss: 11.3348, Validation Loss: 11.3533\n",
      "Epoch 4081/6000, Training Loss: 11.3348, Validation Loss: 11.3533\n",
      "Epoch 4082/6000, Training Loss: 11.3347, Validation Loss: 11.3533\n",
      "Epoch 4083/6000, Training Loss: 11.3347, Validation Loss: 11.3532\n",
      "Epoch 4084/6000, Training Loss: 11.3347, Validation Loss: 11.3532\n",
      "Epoch 4085/6000, Training Loss: 11.3347, Validation Loss: 11.3532\n",
      "Epoch 4086/6000, Training Loss: 11.3346, Validation Loss: 11.3532\n",
      "Epoch 4087/6000, Training Loss: 11.3346, Validation Loss: 11.3531\n",
      "Epoch 4088/6000, Training Loss: 11.3346, Validation Loss: 11.3531\n",
      "Epoch 4089/6000, Training Loss: 11.3346, Validation Loss: 11.3531\n",
      "Epoch 4090/6000, Training Loss: 11.3345, Validation Loss: 11.3531\n",
      "Epoch 4091/6000, Training Loss: 11.3345, Validation Loss: 11.3530\n",
      "Epoch 4092/6000, Training Loss: 11.3345, Validation Loss: 11.3530\n",
      "Epoch 4093/6000, Training Loss: 11.3344, Validation Loss: 11.3530\n",
      "Epoch 4094/6000, Training Loss: 11.3344, Validation Loss: 11.3530\n",
      "Epoch 4095/6000, Training Loss: 11.3344, Validation Loss: 11.3530\n",
      "Epoch 4096/6000, Training Loss: 11.3344, Validation Loss: 11.3529\n",
      "Epoch 4097/6000, Training Loss: 11.3343, Validation Loss: 11.3529\n",
      "Epoch 4098/6000, Training Loss: 11.3343, Validation Loss: 11.3529\n",
      "Epoch 4099/6000, Training Loss: 11.3343, Validation Loss: 11.3529\n",
      "Epoch 4100/6000, Training Loss: 11.3343, Validation Loss: 11.3528\n",
      "Epoch 4101/6000, Training Loss: 11.3342, Validation Loss: 11.3528\n",
      "Epoch 4102/6000, Training Loss: 11.3342, Validation Loss: 11.3528\n",
      "Epoch 4103/6000, Training Loss: 11.3342, Validation Loss: 11.3528\n",
      "Epoch 4104/6000, Training Loss: 11.3342, Validation Loss: 11.3528\n",
      "Epoch 4105/6000, Training Loss: 11.3341, Validation Loss: 11.3527\n",
      "Epoch 4106/6000, Training Loss: 11.3341, Validation Loss: 11.3527\n",
      "Epoch 4107/6000, Training Loss: 11.3341, Validation Loss: 11.3527\n",
      "Epoch 4108/6000, Training Loss: 11.3341, Validation Loss: 11.3527\n",
      "Epoch 4109/6000, Training Loss: 11.3340, Validation Loss: 11.3527\n",
      "Epoch 4110/6000, Training Loss: 11.3340, Validation Loss: 11.3526\n",
      "Epoch 4111/6000, Training Loss: 11.3340, Validation Loss: 11.3526\n",
      "Epoch 4112/6000, Training Loss: 11.3340, Validation Loss: 11.3526\n",
      "Epoch 4113/6000, Training Loss: 11.3340, Validation Loss: 11.3526\n",
      "Epoch 4114/6000, Training Loss: 11.3339, Validation Loss: 11.3525\n",
      "Epoch 4115/6000, Training Loss: 11.3339, Validation Loss: 11.3525\n",
      "Epoch 4116/6000, Training Loss: 11.3339, Validation Loss: 11.3525\n",
      "Epoch 4117/6000, Training Loss: 11.3339, Validation Loss: 11.3525\n",
      "Epoch 4118/6000, Training Loss: 11.3338, Validation Loss: 11.3525\n",
      "Epoch 4119/6000, Training Loss: 11.3338, Validation Loss: 11.3524\n",
      "Epoch 4120/6000, Training Loss: 11.3338, Validation Loss: 11.3524\n",
      "Epoch 4121/6000, Training Loss: 11.3338, Validation Loss: 11.3524\n",
      "Epoch 4122/6000, Training Loss: 11.3337, Validation Loss: 11.3524\n",
      "Epoch 4123/6000, Training Loss: 11.3337, Validation Loss: 11.3524\n",
      "Epoch 4124/6000, Training Loss: 11.3337, Validation Loss: 11.3523\n",
      "Epoch 4125/6000, Training Loss: 11.3337, Validation Loss: 11.3523\n",
      "Epoch 4126/6000, Training Loss: 11.3336, Validation Loss: 11.3523\n",
      "Epoch 4127/6000, Training Loss: 11.3336, Validation Loss: 11.3523\n",
      "Epoch 4128/6000, Training Loss: 11.3336, Validation Loss: 11.3523\n",
      "Epoch 4129/6000, Training Loss: 11.3336, Validation Loss: 11.3522\n",
      "Epoch 4130/6000, Training Loss: 11.3336, Validation Loss: 11.3522\n",
      "Epoch 4131/6000, Training Loss: 11.3335, Validation Loss: 11.3522\n",
      "Epoch 4132/6000, Training Loss: 11.3335, Validation Loss: 11.3522\n",
      "Epoch 4133/6000, Training Loss: 11.3335, Validation Loss: 11.3522\n",
      "Epoch 4134/6000, Training Loss: 11.3335, Validation Loss: 11.3521\n",
      "Epoch 4135/6000, Training Loss: 11.3334, Validation Loss: 11.3521\n",
      "Epoch 4136/6000, Training Loss: 11.3334, Validation Loss: 11.3521\n",
      "Epoch 4137/6000, Training Loss: 11.3334, Validation Loss: 11.3521\n",
      "Epoch 4138/6000, Training Loss: 11.3334, Validation Loss: 11.3521\n",
      "Epoch 4139/6000, Training Loss: 11.3334, Validation Loss: 11.3520\n",
      "Epoch 4140/6000, Training Loss: 11.3333, Validation Loss: 11.3520\n",
      "Epoch 4141/6000, Training Loss: 11.3333, Validation Loss: 11.3520\n",
      "Epoch 4142/6000, Training Loss: 11.3333, Validation Loss: 11.3520\n",
      "Epoch 4143/6000, Training Loss: 11.3333, Validation Loss: 11.3520\n",
      "Epoch 4144/6000, Training Loss: 11.3333, Validation Loss: 11.3520\n",
      "Epoch 4145/6000, Training Loss: 11.3332, Validation Loss: 11.3519\n",
      "Epoch 4146/6000, Training Loss: 11.3332, Validation Loss: 11.3519\n",
      "Epoch 4147/6000, Training Loss: 11.3332, Validation Loss: 11.3519\n",
      "Epoch 4148/6000, Training Loss: 11.3332, Validation Loss: 11.3519\n",
      "Epoch 4149/6000, Training Loss: 11.3331, Validation Loss: 11.3519\n",
      "Epoch 4150/6000, Training Loss: 11.3331, Validation Loss: 11.3518\n",
      "Epoch 4151/6000, Training Loss: 11.3331, Validation Loss: 11.3518\n",
      "Epoch 4152/6000, Training Loss: 11.3331, Validation Loss: 11.3518\n",
      "Epoch 4153/6000, Training Loss: 11.3331, Validation Loss: 11.3518\n",
      "Epoch 4154/6000, Training Loss: 11.3330, Validation Loss: 11.3518\n",
      "Epoch 4155/6000, Training Loss: 11.3330, Validation Loss: 11.3517\n",
      "Epoch 4156/6000, Training Loss: 11.3330, Validation Loss: 11.3517\n",
      "Epoch 4157/6000, Training Loss: 11.3330, Validation Loss: 11.3517\n",
      "Epoch 4158/6000, Training Loss: 11.3330, Validation Loss: 11.3517\n",
      "Epoch 4159/6000, Training Loss: 11.3329, Validation Loss: 11.3517\n",
      "Epoch 4160/6000, Training Loss: 11.3329, Validation Loss: 11.3516\n",
      "Epoch 4161/6000, Training Loss: 11.3329, Validation Loss: 11.3516\n",
      "Epoch 4162/6000, Training Loss: 11.3329, Validation Loss: 11.3516\n",
      "Epoch 4163/6000, Training Loss: 11.3329, Validation Loss: 11.3516\n",
      "Epoch 4164/6000, Training Loss: 11.3328, Validation Loss: 11.3516\n",
      "Epoch 4165/6000, Training Loss: 11.3328, Validation Loss: 11.3516\n",
      "Epoch 4166/6000, Training Loss: 11.3328, Validation Loss: 11.3515\n",
      "Epoch 4167/6000, Training Loss: 11.3328, Validation Loss: 11.3515\n",
      "Epoch 4168/6000, Training Loss: 11.3328, Validation Loss: 11.3515\n",
      "Epoch 4169/6000, Training Loss: 11.3327, Validation Loss: 11.3515\n",
      "Epoch 4170/6000, Training Loss: 11.3327, Validation Loss: 11.3515\n",
      "Epoch 4171/6000, Training Loss: 11.3327, Validation Loss: 11.3514\n",
      "Epoch 4172/6000, Training Loss: 11.3327, Validation Loss: 11.3514\n",
      "Epoch 4173/6000, Training Loss: 11.3327, Validation Loss: 11.3514\n",
      "Epoch 4174/6000, Training Loss: 11.3326, Validation Loss: 11.3514\n",
      "Epoch 4175/6000, Training Loss: 11.3326, Validation Loss: 11.3514\n",
      "Epoch 4176/6000, Training Loss: 11.3326, Validation Loss: 11.3514\n",
      "Epoch 4177/6000, Training Loss: 11.3326, Validation Loss: 11.3513\n",
      "Epoch 4178/6000, Training Loss: 11.3326, Validation Loss: 11.3513\n",
      "Epoch 4179/6000, Training Loss: 11.3325, Validation Loss: 11.3513\n",
      "Epoch 4180/6000, Training Loss: 11.3325, Validation Loss: 11.3513\n",
      "Epoch 4181/6000, Training Loss: 11.3325, Validation Loss: 11.3513\n",
      "Epoch 4182/6000, Training Loss: 11.3325, Validation Loss: 11.3513\n",
      "Epoch 4183/6000, Training Loss: 11.3325, Validation Loss: 11.3512\n",
      "Epoch 4184/6000, Training Loss: 11.3324, Validation Loss: 11.3512\n",
      "Epoch 4185/6000, Training Loss: 11.3324, Validation Loss: 11.3512\n",
      "Epoch 4186/6000, Training Loss: 11.3324, Validation Loss: 11.3512\n",
      "Epoch 4187/6000, Training Loss: 11.3324, Validation Loss: 11.3512\n",
      "Epoch 4188/6000, Training Loss: 11.3324, Validation Loss: 11.3511\n",
      "Epoch 4189/6000, Training Loss: 11.3323, Validation Loss: 11.3511\n",
      "Epoch 4190/6000, Training Loss: 11.3323, Validation Loss: 11.3511\n",
      "Epoch 4191/6000, Training Loss: 11.3323, Validation Loss: 11.3511\n",
      "Epoch 4192/6000, Training Loss: 11.3323, Validation Loss: 11.3511\n",
      "Epoch 4193/6000, Training Loss: 11.3323, Validation Loss: 11.3511\n",
      "Epoch 4194/6000, Training Loss: 11.3323, Validation Loss: 11.3510\n",
      "Epoch 4195/6000, Training Loss: 11.3322, Validation Loss: 11.3510\n",
      "Epoch 4196/6000, Training Loss: 11.3322, Validation Loss: 11.3510\n",
      "Epoch 4197/6000, Training Loss: 11.3322, Validation Loss: 11.3510\n",
      "Epoch 4198/6000, Training Loss: 11.3322, Validation Loss: 11.3510\n",
      "Epoch 4199/6000, Training Loss: 11.3322, Validation Loss: 11.3510\n",
      "Epoch 4200/6000, Training Loss: 11.3321, Validation Loss: 11.3509\n",
      "Epoch 4201/6000, Training Loss: 11.3321, Validation Loss: 11.3509\n",
      "Epoch 4202/6000, Training Loss: 11.3321, Validation Loss: 11.3509\n",
      "Epoch 4203/6000, Training Loss: 11.3321, Validation Loss: 11.3509\n",
      "Epoch 4204/6000, Training Loss: 11.3321, Validation Loss: 11.3509\n",
      "Epoch 4205/6000, Training Loss: 11.3320, Validation Loss: 11.3509\n",
      "Epoch 4206/6000, Training Loss: 11.3320, Validation Loss: 11.3508\n",
      "Epoch 4207/6000, Training Loss: 11.3320, Validation Loss: 11.3508\n",
      "Epoch 4208/6000, Training Loss: 11.3320, Validation Loss: 11.3508\n",
      "Epoch 4209/6000, Training Loss: 11.3320, Validation Loss: 11.3508\n",
      "Epoch 4210/6000, Training Loss: 11.3320, Validation Loss: 11.3508\n",
      "Epoch 4211/6000, Training Loss: 11.3319, Validation Loss: 11.3508\n",
      "Epoch 4212/6000, Training Loss: 11.3319, Validation Loss: 11.3507\n",
      "Epoch 4213/6000, Training Loss: 11.3319, Validation Loss: 11.3507\n",
      "Epoch 4214/6000, Training Loss: 11.3319, Validation Loss: 11.3507\n",
      "Epoch 4215/6000, Training Loss: 11.3319, Validation Loss: 11.3507\n",
      "Epoch 4216/6000, Training Loss: 11.3318, Validation Loss: 11.3507\n",
      "Epoch 4217/6000, Training Loss: 11.3318, Validation Loss: 11.3507\n",
      "Epoch 4218/6000, Training Loss: 11.3318, Validation Loss: 11.3506\n",
      "Epoch 4219/6000, Training Loss: 11.3318, Validation Loss: 11.3506\n",
      "Epoch 4220/6000, Training Loss: 11.3318, Validation Loss: 11.3506\n",
      "Epoch 4221/6000, Training Loss: 11.3318, Validation Loss: 11.3506\n",
      "Epoch 4222/6000, Training Loss: 11.3317, Validation Loss: 11.3506\n",
      "Epoch 4223/6000, Training Loss: 11.3317, Validation Loss: 11.3506\n",
      "Epoch 4224/6000, Training Loss: 11.3317, Validation Loss: 11.3506\n",
      "Epoch 4225/6000, Training Loss: 11.3317, Validation Loss: 11.3505\n",
      "Epoch 4226/6000, Training Loss: 11.3317, Validation Loss: 11.3505\n",
      "Epoch 4227/6000, Training Loss: 11.3317, Validation Loss: 11.3505\n",
      "Epoch 4228/6000, Training Loss: 11.3316, Validation Loss: 11.3505\n",
      "Epoch 4229/6000, Training Loss: 11.3316, Validation Loss: 11.3505\n",
      "Epoch 4230/6000, Training Loss: 11.3316, Validation Loss: 11.3505\n",
      "Epoch 4231/6000, Training Loss: 11.3316, Validation Loss: 11.3504\n",
      "Epoch 4232/6000, Training Loss: 11.3316, Validation Loss: 11.3504\n",
      "Epoch 4233/6000, Training Loss: 11.3316, Validation Loss: 11.3504\n",
      "Epoch 4234/6000, Training Loss: 11.3315, Validation Loss: 11.3504\n",
      "Epoch 4235/6000, Training Loss: 11.3315, Validation Loss: 11.3504\n",
      "Epoch 4236/6000, Training Loss: 11.3315, Validation Loss: 11.3504\n",
      "Epoch 4237/6000, Training Loss: 11.3315, Validation Loss: 11.3504\n",
      "Epoch 4238/6000, Training Loss: 11.3315, Validation Loss: 11.3503\n",
      "Epoch 4239/6000, Training Loss: 11.3315, Validation Loss: 11.3503\n",
      "Epoch 4240/6000, Training Loss: 11.3315, Validation Loss: 11.3503\n",
      "Epoch 4241/6000, Training Loss: 11.3314, Validation Loss: 11.3503\n",
      "Epoch 4242/6000, Training Loss: 11.3314, Validation Loss: 11.3503\n",
      "Epoch 4243/6000, Training Loss: 11.3314, Validation Loss: 11.3503\n",
      "Epoch 4244/6000, Training Loss: 11.3314, Validation Loss: 11.3503\n",
      "Epoch 4245/6000, Training Loss: 11.3314, Validation Loss: 11.3503\n",
      "Epoch 4246/6000, Training Loss: 11.3314, Validation Loss: 11.3502\n",
      "Epoch 4247/6000, Training Loss: 11.3313, Validation Loss: 11.3502\n",
      "Epoch 4248/6000, Training Loss: 11.3313, Validation Loss: 11.3502\n",
      "Epoch 4249/6000, Training Loss: 11.3313, Validation Loss: 11.3502\n",
      "Epoch 4250/6000, Training Loss: 11.3313, Validation Loss: 11.3502\n",
      "Epoch 4251/6000, Training Loss: 11.3313, Validation Loss: 11.3502\n",
      "Epoch 4252/6000, Training Loss: 11.3313, Validation Loss: 11.3502\n",
      "Epoch 4253/6000, Training Loss: 11.3313, Validation Loss: 11.3502\n",
      "Epoch 4254/6000, Training Loss: 11.3312, Validation Loss: 11.3501\n",
      "Epoch 4255/6000, Training Loss: 11.3312, Validation Loss: 11.3501\n",
      "Epoch 4256/6000, Training Loss: 11.3312, Validation Loss: 11.3501\n",
      "Epoch 4257/6000, Training Loss: 11.3312, Validation Loss: 11.3501\n",
      "Epoch 4258/6000, Training Loss: 11.3312, Validation Loss: 11.3501\n",
      "Epoch 4259/6000, Training Loss: 11.3312, Validation Loss: 11.3501\n",
      "Epoch 4260/6000, Training Loss: 11.3312, Validation Loss: 11.3501\n",
      "Epoch 4261/6000, Training Loss: 11.3312, Validation Loss: 11.3501\n",
      "Epoch 4262/6000, Training Loss: 11.3311, Validation Loss: 11.3501\n",
      "Epoch 4263/6000, Training Loss: 11.3311, Validation Loss: 11.3500\n",
      "Epoch 4264/6000, Training Loss: 11.3311, Validation Loss: 11.3500\n",
      "Epoch 4265/6000, Training Loss: 11.3311, Validation Loss: 11.3500\n",
      "Epoch 4266/6000, Training Loss: 11.3311, Validation Loss: 11.3500\n",
      "Epoch 4267/6000, Training Loss: 11.3311, Validation Loss: 11.3500\n",
      "Epoch 4268/6000, Training Loss: 11.3311, Validation Loss: 11.3500\n",
      "Epoch 4269/6000, Training Loss: 11.3311, Validation Loss: 11.3500\n",
      "Epoch 4270/6000, Training Loss: 11.3310, Validation Loss: 11.3500\n",
      "Epoch 4271/6000, Training Loss: 11.3310, Validation Loss: 11.3500\n",
      "Epoch 4272/6000, Training Loss: 11.3310, Validation Loss: 11.3500\n",
      "Epoch 4273/6000, Training Loss: 11.3310, Validation Loss: 11.3499\n",
      "Epoch 4274/6000, Training Loss: 11.3310, Validation Loss: 11.3499\n",
      "Epoch 4275/6000, Training Loss: 11.3310, Validation Loss: 11.3499\n",
      "Epoch 4276/6000, Training Loss: 11.3310, Validation Loss: 11.3499\n",
      "Epoch 4277/6000, Training Loss: 11.3310, Validation Loss: 11.3499\n",
      "Epoch 4278/6000, Training Loss: 11.3310, Validation Loss: 11.3499\n",
      "Epoch 4279/6000, Training Loss: 11.3309, Validation Loss: 11.3499\n",
      "Epoch 4280/6000, Training Loss: 11.3309, Validation Loss: 11.3499\n",
      "Epoch 4281/6000, Training Loss: 11.3309, Validation Loss: 11.3499\n",
      "Epoch 4282/6000, Training Loss: 11.3309, Validation Loss: 11.3499\n",
      "Epoch 4283/6000, Training Loss: 11.3309, Validation Loss: 11.3499\n",
      "Epoch 4284/6000, Training Loss: 11.3309, Validation Loss: 11.3499\n",
      "Epoch 4285/6000, Training Loss: 11.3309, Validation Loss: 11.3498\n",
      "Epoch 4286/6000, Training Loss: 11.3309, Validation Loss: 11.3498\n",
      "Epoch 4287/6000, Training Loss: 11.3309, Validation Loss: 11.3498\n",
      "Epoch 4288/6000, Training Loss: 11.3309, Validation Loss: 11.3498\n",
      "Epoch 4289/6000, Training Loss: 11.3308, Validation Loss: 11.3498\n",
      "Epoch 4290/6000, Training Loss: 11.3308, Validation Loss: 11.3498\n",
      "Epoch 4291/6000, Training Loss: 11.3308, Validation Loss: 11.3498\n",
      "Epoch 4292/6000, Training Loss: 11.3308, Validation Loss: 11.3498\n",
      "Epoch 4293/6000, Training Loss: 11.3308, Validation Loss: 11.3498\n",
      "Epoch 4294/6000, Training Loss: 11.3308, Validation Loss: 11.3498\n",
      "Epoch 4295/6000, Training Loss: 11.3308, Validation Loss: 11.3498\n",
      "Epoch 4296/6000, Training Loss: 11.3308, Validation Loss: 11.3498\n",
      "Epoch 4297/6000, Training Loss: 11.3308, Validation Loss: 11.3498\n",
      "Epoch 4298/6000, Training Loss: 11.3308, Validation Loss: 11.3498\n",
      "Epoch 4299/6000, Training Loss: 11.3308, Validation Loss: 11.3497\n",
      "Epoch 4300/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4301/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4302/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4303/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4304/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4305/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4306/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4307/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4308/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4309/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4310/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4311/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4312/6000, Training Loss: 11.3307, Validation Loss: 11.3497\n",
      "Epoch 4313/6000, Training Loss: 11.3306, Validation Loss: 11.3497\n",
      "Epoch 4314/6000, Training Loss: 11.3306, Validation Loss: 11.3497\n",
      "Epoch 4315/6000, Training Loss: 11.3306, Validation Loss: 11.3497\n",
      "Epoch 4316/6000, Training Loss: 11.3306, Validation Loss: 11.3497\n",
      "Epoch 4317/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4318/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4319/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4320/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4321/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4322/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4323/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4324/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4325/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4326/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4327/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4328/6000, Training Loss: 11.3306, Validation Loss: 11.3496\n",
      "Epoch 4329/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4330/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4331/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4332/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4333/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4334/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4335/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4336/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4337/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4338/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4339/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4340/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4341/6000, Training Loss: 11.3305, Validation Loss: 11.3496\n",
      "Epoch 4342/6000, Training Loss: 11.3305, Validation Loss: 11.3495\n",
      "Epoch 4343/6000, Training Loss: 11.3305, Validation Loss: 11.3495\n",
      "Epoch 4344/6000, Training Loss: 11.3305, Validation Loss: 11.3495\n",
      "Epoch 4345/6000, Training Loss: 11.3305, Validation Loss: 11.3495\n",
      "Epoch 4346/6000, Training Loss: 11.3305, Validation Loss: 11.3495\n",
      "Epoch 4347/6000, Training Loss: 11.3305, Validation Loss: 11.3495\n",
      "Epoch 4348/6000, Training Loss: 11.3305, Validation Loss: 11.3495\n",
      "Epoch 4349/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4350/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4351/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4352/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4353/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4354/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4355/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4356/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4357/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4358/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4359/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4360/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4361/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4362/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4363/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4364/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4365/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4366/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4367/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4368/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4369/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4370/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4371/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4372/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4373/6000, Training Loss: 11.3304, Validation Loss: 11.3495\n",
      "Epoch 4374/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4375/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4376/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4377/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4378/6000, Training Loss: 11.3303, Validation Loss: 11.3495\n",
      "Epoch 4379/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4380/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4381/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4382/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4383/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4384/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4385/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4386/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4387/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4388/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4389/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4390/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4391/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4392/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4393/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4394/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4395/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4396/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4397/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4398/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4399/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4400/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4401/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4402/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4403/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4404/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4405/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4406/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4407/6000, Training Loss: 11.3303, Validation Loss: 11.3494\n",
      "Epoch 4408/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4409/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4410/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4411/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4412/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4413/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4414/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4415/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4416/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4417/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4418/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4419/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4420/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4421/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4422/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4423/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4424/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4425/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4426/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4427/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4428/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4429/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4430/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4431/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4432/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4433/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4434/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4435/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4436/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4437/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4438/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4439/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4440/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4441/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4442/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4443/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4444/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4445/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4446/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4447/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4448/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4449/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4450/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4451/6000, Training Loss: 11.3302, Validation Loss: 11.3494\n",
      "Epoch 4452/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4453/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4454/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4455/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4456/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4457/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4458/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4459/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4460/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4461/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4462/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4463/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4464/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4465/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4466/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4467/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4468/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4469/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4470/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4471/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4472/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4473/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4474/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4475/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4476/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4477/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4478/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4479/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4480/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4481/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4482/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4483/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4484/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4485/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4486/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4487/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4488/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4489/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4490/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4491/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4492/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4493/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4494/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4495/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4496/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4497/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4498/6000, Training Loss: 11.3301, Validation Loss: 11.3494\n",
      "Epoch 4499/6000, Training Loss: 11.3301, Validation Loss: 11.3493\n",
      "Epoch 4500/6000, Training Loss: 11.3301, Validation Loss: 11.3493\n",
      "Epoch 4501/6000, Training Loss: 11.3301, Validation Loss: 11.3493\n",
      "Epoch 4502/6000, Training Loss: 11.3301, Validation Loss: 11.3493\n",
      "Epoch 4503/6000, Training Loss: 11.3301, Validation Loss: 11.3493\n",
      "Epoch 4504/6000, Training Loss: 11.3301, Validation Loss: 11.3493\n",
      "Epoch 4505/6000, Training Loss: 11.3301, Validation Loss: 11.3493\n",
      "Epoch 4506/6000, Training Loss: 11.3301, Validation Loss: 11.3493\n",
      "Epoch 4507/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4508/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4509/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4510/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4511/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4512/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4513/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4514/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4515/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4516/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4517/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4518/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4519/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4520/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4521/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4522/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4523/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4524/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4525/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4526/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4527/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4528/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4529/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4530/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4531/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4532/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4533/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4534/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4535/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4536/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4537/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4538/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4539/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4540/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4541/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4542/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4543/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4544/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4545/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4546/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4547/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4548/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4549/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4550/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4551/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4552/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4553/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4554/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4555/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4556/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4557/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4558/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4559/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4560/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4561/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4562/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4563/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4564/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4565/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4566/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4567/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4568/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4569/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4570/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4571/6000, Training Loss: 11.3300, Validation Loss: 11.3493\n",
      "Epoch 4572/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4573/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4574/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4575/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4576/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4577/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4578/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4579/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4580/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4581/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4582/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4583/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4584/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4585/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4586/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4587/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4588/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4589/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4590/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4591/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4592/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4593/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4594/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4595/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4596/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4597/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4598/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4599/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4600/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4601/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4602/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4603/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4604/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4605/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4606/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4607/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4608/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4609/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4610/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4611/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4612/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4613/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4614/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4615/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4616/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4617/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4618/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4619/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4620/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4621/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4622/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4623/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4624/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4625/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4626/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4627/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4628/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4629/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4630/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4631/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4632/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4633/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4634/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4635/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4636/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4637/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4638/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4639/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4640/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4641/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4642/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4643/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4644/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4645/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4646/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4647/6000, Training Loss: 11.3299, Validation Loss: 11.3493\n",
      "Epoch 4648/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4649/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4650/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4651/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4652/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4653/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4654/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4655/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4656/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4657/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4658/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4659/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4660/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4661/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4662/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4663/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4664/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4665/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4666/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4667/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4668/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4669/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4670/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4671/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4672/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4673/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4674/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4675/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4676/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4677/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4678/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4679/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4680/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4681/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4682/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4683/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4684/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4685/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4686/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4687/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4688/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4689/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4690/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4691/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4692/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4693/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4694/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4695/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4696/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4697/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4698/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4699/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4700/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4701/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4702/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4703/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4704/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4705/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4706/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4707/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4708/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4709/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4710/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4711/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4712/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4713/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4714/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4715/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4716/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4717/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4718/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4719/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4720/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4721/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4722/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4723/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4724/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4725/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4726/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4727/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4728/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4729/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4730/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4731/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4732/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4733/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4734/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4735/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4736/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4737/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4738/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4739/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4740/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4741/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4742/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4743/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4744/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4745/6000, Training Loss: 11.3298, Validation Loss: 11.3493\n",
      "Epoch 4746/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4747/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4748/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4749/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4750/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4751/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4752/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4753/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4754/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4755/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4756/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4757/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4758/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4759/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4760/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4761/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4762/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4763/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4764/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4765/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4766/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4767/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4768/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4769/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4770/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4771/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4772/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4773/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4774/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4775/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4776/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4777/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4778/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4779/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4780/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4781/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4782/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4783/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4784/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4785/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4786/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4787/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4788/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4789/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4790/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4791/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4792/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4793/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4794/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4795/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4796/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4797/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4798/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4799/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4800/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4801/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4802/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4803/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4804/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4805/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4806/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4807/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4808/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4809/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4810/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4811/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4812/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4813/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4814/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4815/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4816/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4817/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4818/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4819/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4820/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4821/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4822/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4823/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4824/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4825/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4826/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4827/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4828/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4829/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4830/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4831/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4832/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4833/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4834/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4835/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4836/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4837/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4838/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4839/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4840/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4841/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4842/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4843/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4844/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4845/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4846/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4847/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4848/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4849/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4850/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4851/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4852/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4853/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4854/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4855/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4856/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4857/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4858/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4859/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4860/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4861/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4862/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4863/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4864/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4865/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4866/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4867/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4868/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4869/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4870/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4871/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4872/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4873/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4874/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4875/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4876/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4877/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4878/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4879/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4880/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4881/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4882/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4883/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4884/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4885/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4886/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4887/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4888/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4889/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4890/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4891/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4892/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4893/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4894/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4895/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4896/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4897/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4898/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4899/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4900/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4901/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4902/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4903/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4904/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4905/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4906/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4907/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4908/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4909/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4910/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4911/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4912/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4913/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4914/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4915/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4916/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4917/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4918/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4919/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4920/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4921/6000, Training Loss: 11.3297, Validation Loss: 11.3493\n",
      "Epoch 4922/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4923/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4924/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4925/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4926/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4927/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4928/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4929/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4930/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4931/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4932/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4933/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4934/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4935/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4936/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4937/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4938/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4939/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4940/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4941/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4942/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4943/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4944/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4945/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4946/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4947/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4948/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4949/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4950/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4951/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4952/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4953/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4954/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4955/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4956/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4957/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4958/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4959/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4960/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4961/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4962/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4963/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4964/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4965/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4966/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4967/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4968/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4969/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4970/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4971/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4972/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4973/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4974/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4975/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4976/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4977/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4978/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4979/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4980/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4981/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4982/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4983/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4984/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4985/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4986/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4987/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4988/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4989/6000, Training Loss: 11.3296, Validation Loss: 11.3493\n",
      "Epoch 4990/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4991/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4992/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4993/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4994/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4995/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4996/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4997/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4998/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 4999/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5000/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5001/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5002/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5003/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5004/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5005/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5006/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5007/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5008/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5009/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5010/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5011/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5012/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5013/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5014/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5015/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5016/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5017/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5018/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5019/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5020/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5021/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5022/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5023/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5024/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5025/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5026/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5027/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5028/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5029/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5030/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5031/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5032/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5033/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5034/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5035/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5036/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5037/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5038/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5039/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5040/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5041/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5042/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5043/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5044/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5045/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5046/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5047/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5048/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5049/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5050/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5051/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5052/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5053/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5054/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5055/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5056/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5057/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5058/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5059/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5060/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5061/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5062/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5063/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5064/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5065/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5066/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5067/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5068/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5069/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5070/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5071/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5072/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5073/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5074/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5075/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5076/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5077/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5078/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5079/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5080/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5081/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5082/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5083/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5084/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5085/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5086/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5087/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5088/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5089/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5090/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5091/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5092/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5093/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5094/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5095/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5096/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5097/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5098/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5099/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5100/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5101/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5102/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5103/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5104/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5105/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5106/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5107/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5108/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5109/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5110/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5111/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5112/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5113/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5114/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5115/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5116/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5117/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5118/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5119/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5120/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5121/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5122/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5123/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5124/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5125/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5126/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5127/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5128/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5129/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5130/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5131/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5132/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5133/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5134/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5135/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5136/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5137/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5138/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5139/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5140/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5141/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5142/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5143/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5144/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5145/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5146/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5147/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5148/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5149/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5150/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5151/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5152/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5153/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5154/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5155/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5156/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5157/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5158/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5159/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5160/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5161/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5162/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5163/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5164/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5165/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5166/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5167/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5168/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5169/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5170/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5171/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5172/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5173/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5174/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5175/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5176/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5177/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5178/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5179/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5180/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5181/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5182/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5183/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5184/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5185/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5186/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5187/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5188/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5189/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5190/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5191/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5192/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5193/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5194/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5195/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5196/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5197/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5198/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5199/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5200/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5201/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5202/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5203/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5204/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5205/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5206/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5207/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5208/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5209/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5210/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5211/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5212/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5213/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5214/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5215/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5216/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5217/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5218/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5219/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5220/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5221/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5222/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5223/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5224/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5225/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5226/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5227/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5228/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5229/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5230/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5231/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5232/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5233/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5234/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5235/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5236/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5237/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5238/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5239/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5240/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5241/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5242/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5243/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5244/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5245/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5246/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5247/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5248/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5249/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5250/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5251/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5252/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5253/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5254/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5255/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5256/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5257/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5258/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5259/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5260/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5261/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5262/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5263/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5264/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5265/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5266/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5267/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5268/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5269/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5270/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5271/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5272/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5273/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5274/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5275/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5276/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5277/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5278/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5279/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5280/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5281/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5282/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5283/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5284/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5285/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5286/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5287/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5288/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5289/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5290/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5291/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5292/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5293/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5294/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5295/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5296/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5297/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5298/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5299/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5300/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5301/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5302/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5303/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5304/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5305/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5306/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5307/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5308/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5309/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5310/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5311/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5312/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5313/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5314/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5315/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5316/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5317/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5318/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5319/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5320/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5321/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5322/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5323/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5324/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5325/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5326/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5327/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5328/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5329/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5330/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5331/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5332/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5333/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5334/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5335/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5336/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5337/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5338/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5339/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5340/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5341/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5342/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5343/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5344/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5345/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5346/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5347/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5348/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5349/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5350/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5351/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5352/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5353/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5354/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5355/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5356/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5357/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5358/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5359/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5360/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5361/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5362/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5363/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5364/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5365/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5366/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5367/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5368/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5369/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5370/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5371/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5372/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5373/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5374/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5375/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5376/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5377/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5378/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5379/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5380/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5381/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5382/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5383/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5384/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5385/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5386/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5387/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5388/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5389/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5390/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5391/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5392/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5393/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5394/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5395/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5396/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5397/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5398/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5399/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5400/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5401/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5402/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5403/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5404/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5405/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5406/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5407/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5408/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5409/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5410/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5411/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5412/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5413/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5414/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5415/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5416/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5417/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5418/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5419/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5420/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5421/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5422/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5423/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5424/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5425/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5426/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5427/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5428/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5429/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5430/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5431/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5432/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5433/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5434/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5435/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5436/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5437/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5438/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5439/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5440/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5441/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5442/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5443/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5444/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5445/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5446/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5447/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5448/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5449/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5450/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5451/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5452/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5453/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5454/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5455/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5456/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5457/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5458/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5459/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5460/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5461/6000, Training Loss: 11.3296, Validation Loss: 11.3494\n",
      "Epoch 5462/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5463/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5464/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5465/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5466/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5467/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5468/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5469/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5470/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5471/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5472/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5473/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5474/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5475/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5476/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5477/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5478/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5479/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5480/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5481/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5482/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5483/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5484/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5485/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5486/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5487/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5488/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5489/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5490/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5491/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5492/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5493/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5494/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5495/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5496/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5497/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5498/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5499/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5500/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5501/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5502/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5503/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5504/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5505/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5506/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5507/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5508/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5509/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5510/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5511/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5512/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5513/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5514/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5515/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5516/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5517/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5518/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5519/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5520/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5521/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5522/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5523/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5524/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5525/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5526/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5527/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5528/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5529/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5530/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5531/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5532/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5533/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5534/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5535/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5536/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5537/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5538/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5539/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5540/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5541/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5542/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5543/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5544/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5545/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5546/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5547/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5548/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5549/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5550/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5551/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5552/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5553/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5554/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5555/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5556/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5557/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5558/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5559/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5560/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5561/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5562/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5563/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5564/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5565/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5566/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5567/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5568/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5569/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5570/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5571/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5572/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5573/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5574/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5575/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5576/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5577/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5578/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5579/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5580/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5581/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5582/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5583/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5584/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5585/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5586/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5587/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5588/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5589/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5590/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5591/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5592/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5593/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5594/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5595/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5596/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5597/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5598/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5599/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5600/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5601/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5602/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5603/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5604/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5605/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5606/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5607/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5608/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5609/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5610/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5611/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5612/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5613/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5614/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5615/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5616/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5617/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5618/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5619/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5620/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5621/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5622/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5623/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5624/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5625/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5626/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5627/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5628/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5629/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5630/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5631/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5632/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5633/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5634/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5635/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5636/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5637/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5638/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5639/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5640/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5641/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5642/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5643/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5644/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5645/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5646/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5647/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5648/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5649/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5650/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5651/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5652/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5653/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5654/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5655/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5656/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5657/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5658/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5659/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5660/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5661/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5662/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5663/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5664/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5665/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5666/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5667/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5668/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5669/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5670/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5671/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5672/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5673/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5674/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5675/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5676/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5677/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5678/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5679/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5680/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5681/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5682/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5683/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5684/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5685/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5686/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5687/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5688/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5689/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5690/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5691/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5692/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5693/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5694/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5695/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5696/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5697/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5698/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5699/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5700/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5701/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5702/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5703/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5704/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5705/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5706/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5707/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5708/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5709/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5710/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5711/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5712/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5713/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5714/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5715/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5716/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5717/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5718/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5719/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5720/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5721/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5722/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5723/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5724/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5725/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5726/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5727/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5728/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5729/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5730/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5731/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5732/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5733/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5734/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5735/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5736/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5737/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5738/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5739/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5740/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5741/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5742/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5743/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5744/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5745/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5746/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5747/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5748/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5749/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5750/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5751/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5752/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5753/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5754/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5755/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5756/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5757/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5758/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5759/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5760/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5761/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5762/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5763/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5764/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5765/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5766/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5767/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5768/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5769/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5770/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5771/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5772/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5773/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5774/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5775/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5776/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5777/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5778/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5779/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5780/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5781/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5782/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5783/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5784/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5785/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5786/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5787/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5788/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5789/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5790/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5791/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5792/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5793/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5794/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5795/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5796/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5797/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5798/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5799/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5800/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5801/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5802/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5803/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5804/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5805/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5806/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5807/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5808/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5809/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5810/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5811/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5812/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5813/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5814/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5815/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5816/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5817/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5818/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5819/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5820/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5821/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5822/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5823/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5824/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5825/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5826/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5827/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5828/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5829/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5830/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5831/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5832/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5833/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5834/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5835/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5836/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5837/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5838/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5839/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5840/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5841/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5842/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5843/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5844/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5845/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5846/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5847/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5848/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5849/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5850/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5851/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5852/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5853/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5854/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5855/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5856/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5857/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5858/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5859/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5860/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5861/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5862/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5863/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5864/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5865/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5866/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5867/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5868/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5869/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5870/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5871/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5872/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5873/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5874/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5875/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5876/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5877/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5878/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5879/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5880/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5881/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5882/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5883/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5884/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5885/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5886/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5887/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5888/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5889/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5890/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5891/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5892/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5893/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5894/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5895/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5896/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5897/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5898/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5899/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5900/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5901/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5902/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5903/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5904/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5905/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5906/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5907/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5908/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5909/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5910/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5911/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5912/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5913/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5914/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5915/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5916/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5917/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5918/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5919/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5920/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5921/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5922/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5923/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5924/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5925/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5926/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5927/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5928/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5929/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5930/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5931/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5932/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5933/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5934/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5935/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5936/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5937/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5938/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5939/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5940/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5941/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5942/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5943/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5944/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5945/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5946/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5947/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5948/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5949/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5950/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5951/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5952/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5953/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5954/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5955/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5956/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5957/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5958/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5959/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5960/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5961/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5962/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5963/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5964/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5965/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5966/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5967/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5968/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5969/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5970/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5971/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5972/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5973/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5974/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5975/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5976/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5977/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5978/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5979/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5980/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5981/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5982/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5983/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5984/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5985/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5986/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5987/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5988/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5989/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5990/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5991/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5992/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5993/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5994/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5995/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5996/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5997/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5998/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 5999/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n",
      "Epoch 6000/6000, Training Loss: 11.3296, Validation Loss: 11.3495\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.25%\n",
      "Mean Percent Accuracy (Yield): 89.17%\n",
      "Median Percent Accuracy (Days to Harvest): 79.45%\n",
      "Median Percent Accuracy (Yield): 92.56%\n",
      "Accuracy Within 10% (Days to Harvest): 23.58%\n",
      "Accuracy Within 10% (Yield): 62.50%\n",
      "MAE (Days to Harvest): 22.45\n",
      "MAE (Yield): 0.40\n",
      "RMSE (Days to Harvest): 25.94\n",
      "RMSE (Yield): 0.50\n",
      "\n",
      "Validation Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.17%\n",
      "Mean Percent Accuracy (Yield): 89.00%\n",
      "Median Percent Accuracy (Days to Harvest): 79.42%\n",
      "Median Percent Accuracy (Yield): 92.52%\n",
      "Accuracy Within 10% (Days to Harvest): 23.47%\n",
      "Accuracy Within 10% (Yield): 62.40%\n",
      "MAE (Days to Harvest): 22.49\n",
      "MAE (Yield): 0.40\n",
      "RMSE (Days to Harvest): 25.98\n",
      "RMSE (Yield): 0.50\n",
      "\n",
      "Training completed in 16.93 seconds\n",
      "Final Training Loss: 11.3296\n",
      "Final Validation Loss: 11.3495\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y.values, test_size=0.2, random_state=88)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Move data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor, X_val_tensor = X_train_tensor.to(device), X_val_tensor.to(device)\n",
    "y_train_tensor, y_val_tensor = y_train_tensor.to(device), y_val_tensor.to(device)\n",
    "\n",
    "# Define SVM-like regression model\n",
    "class SVMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVMRegressor, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 2)  # Output two targets: Days to Harvest and Yield\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Define hinge loss for -SVR\n",
    "class EpsilonSVRLoss(nn.Module):\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        super(EpsilonSVRLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        abs_diff = torch.abs(predictions - targets)\n",
    "        loss = torch.mean(torch.clamp(abs_diff - self.epsilon, min=0))\n",
    "        return loss\n",
    "\n",
    "# Instantiate model, loss, and optimizer\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = SVMRegressor(input_dim).to(device)\n",
    "criterion = EpsilonSVRLoss(epsilon=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 6000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_predictions = model(X_train_tensor)\n",
    "    train_loss = criterion(train_predictions, y_train_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_val_tensor)\n",
    "        val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "    # Store losses\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time  # Training time in seconds\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(y_actual, y_pred, tolerance=0.1):\n",
    "    # Percent accuracy\n",
    "    percent_accuracies = 100 * (1 - torch.abs(y_actual - y_pred) / y_actual)\n",
    "    mean_percent_accuracy = torch.mean(percent_accuracies, axis=0).detach().cpu().numpy()\n",
    "    median_percent_accuracy = torch.median(percent_accuracies, axis=0).values.detach().cpu().numpy()\n",
    "\n",
    "    # Accuracy within tolerance\n",
    "    within_tolerance = torch.abs(y_actual - y_pred) <= (tolerance * y_actual)\n",
    "    percent_within_tolerance = torch.mean(within_tolerance.float(), axis=0).detach().cpu().numpy() * 100\n",
    "\n",
    "    # MAE\n",
    "    mae = torch.mean(torch.abs(y_actual - y_pred), axis=0).detach().cpu().numpy()\n",
    "\n",
    "    # RMSE\n",
    "    rmse = torch.sqrt(torch.mean((y_actual - y_pred) ** 2, axis=0)).detach().cpu().numpy()\n",
    "\n",
    "    return {\n",
    "        \"mean_percent_accuracy\": mean_percent_accuracy,\n",
    "        \"median_percent_accuracy\": median_percent_accuracy,\n",
    "        \"percent_within_tolerance\": percent_within_tolerance,\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "    }\n",
    "\n",
    "# Get predictions and metrics\n",
    "train_metrics = evaluate_model(y_train_tensor, train_predictions)\n",
    "val_metrics = evaluate_model(y_val_tensor, val_predictions)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {train_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {train_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {train_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {train_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {train_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {train_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {train_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {train_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {train_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {train_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {val_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {val_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {val_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {val_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {val_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {val_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {val_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {val_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {val_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {val_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "final_train_loss = train_losses[-1]\n",
    "final_val_loss = val_losses[-1]\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500, Training Loss: 5812.4004, Validation Loss: 5617.4453\n",
      "Epoch 2/1500, Training Loss: 5622.4463, Validation Loss: 5434.1865\n",
      "Epoch 3/1500, Training Loss: 5439.0835, Validation Loss: 5257.2881\n",
      "Epoch 4/1500, Training Loss: 5262.0806, Validation Loss: 5086.5278\n",
      "Epoch 5/1500, Training Loss: 5091.2183, Validation Loss: 4921.6934\n",
      "Epoch 6/1500, Training Loss: 4926.2837, Validation Loss: 4762.5786\n",
      "Epoch 7/1500, Training Loss: 4767.0708, Validation Loss: 4608.9849\n",
      "Epoch 8/1500, Training Loss: 4613.3809, Validation Loss: 4460.7217\n",
      "Epoch 9/1500, Training Loss: 4465.0234, Validation Loss: 4317.6030\n",
      "Epoch 10/1500, Training Loss: 4321.8120, Validation Loss: 4179.4521\n",
      "Epoch 11/1500, Training Loss: 4183.5688, Validation Loss: 4046.0947\n",
      "Epoch 12/1500, Training Loss: 4050.1216, Validation Loss: 3917.3645\n",
      "Epoch 13/1500, Training Loss: 3921.3037, Validation Loss: 3793.1025\n",
      "Epoch 14/1500, Training Loss: 3796.9548, Validation Loss: 3673.1523\n",
      "Epoch 15/1500, Training Loss: 3676.9192, Validation Loss: 3557.3650\n",
      "Epoch 16/1500, Training Loss: 3561.0486, Validation Loss: 3445.5955\n",
      "Epoch 17/1500, Training Loss: 3449.1968, Validation Loss: 3337.7046\n",
      "Epoch 18/1500, Training Loss: 3341.2258, Validation Loss: 3233.5586\n",
      "Epoch 19/1500, Training Loss: 3236.9998, Validation Loss: 3133.0264\n",
      "Epoch 20/1500, Training Loss: 3136.3899, Validation Loss: 3035.9829\n",
      "Epoch 21/1500, Training Loss: 3039.2700, Validation Loss: 2942.3076\n",
      "Epoch 22/1500, Training Loss: 2945.5193, Validation Loss: 2851.8828\n",
      "Epoch 23/1500, Training Loss: 2855.0208, Validation Loss: 2764.5967\n",
      "Epoch 24/1500, Training Loss: 2767.6619, Validation Loss: 2680.3394\n",
      "Epoch 25/1500, Training Loss: 2683.3337, Validation Loss: 2599.0061\n",
      "Epoch 26/1500, Training Loss: 2601.9307, Validation Loss: 2520.4958\n",
      "Epoch 27/1500, Training Loss: 2523.3513, Validation Loss: 2444.7104\n",
      "Epoch 28/1500, Training Loss: 2447.4980, Validation Loss: 2371.5549\n",
      "Epoch 29/1500, Training Loss: 2374.2761, Validation Loss: 2300.9385\n",
      "Epoch 30/1500, Training Loss: 2303.5947, Validation Loss: 2232.7725\n",
      "Epoch 31/1500, Training Loss: 2235.3645, Validation Loss: 2166.9727\n",
      "Epoch 32/1500, Training Loss: 2169.5017, Validation Loss: 2103.4563\n",
      "Epoch 33/1500, Training Loss: 2105.9238, Validation Loss: 2042.1444\n",
      "Epoch 34/1500, Training Loss: 2044.5509, Validation Loss: 1982.9604\n",
      "Epoch 35/1500, Training Loss: 1985.3075, Validation Loss: 1925.8306\n",
      "Epoch 36/1500, Training Loss: 1928.1189, Validation Loss: 1870.6835\n",
      "Epoch 37/1500, Training Loss: 1872.9142, Validation Loss: 1817.4504\n",
      "Epoch 38/1500, Training Loss: 1819.6246, Validation Loss: 1766.0649\n",
      "Epoch 39/1500, Training Loss: 1768.1836, Validation Loss: 1716.4631\n",
      "Epoch 40/1500, Training Loss: 1718.5270, Validation Loss: 1668.5829\n",
      "Epoch 41/1500, Training Loss: 1670.5929, Validation Loss: 1622.3644\n",
      "Epoch 42/1500, Training Loss: 1624.3218, Validation Loss: 1577.7501\n",
      "Epoch 43/1500, Training Loss: 1579.6556, Validation Loss: 1534.6843\n",
      "Epoch 44/1500, Training Loss: 1536.5389, Validation Loss: 1493.1134\n",
      "Epoch 45/1500, Training Loss: 1494.9181, Validation Loss: 1452.9852\n",
      "Epoch 46/1500, Training Loss: 1454.7408, Validation Loss: 1414.2501\n",
      "Epoch 47/1500, Training Loss: 1415.9575, Validation Loss: 1376.8594\n",
      "Epoch 48/1500, Training Loss: 1378.5193, Validation Loss: 1340.7666\n",
      "Epoch 49/1500, Training Loss: 1342.3798, Validation Loss: 1305.9266\n",
      "Epoch 50/1500, Training Loss: 1307.4939, Validation Loss: 1272.2958\n",
      "Epoch 51/1500, Training Loss: 1273.8184, Validation Loss: 1239.8325\n",
      "Epoch 52/1500, Training Loss: 1241.3109, Validation Loss: 1208.4961\n",
      "Epoch 53/1500, Training Loss: 1209.9309, Validation Loss: 1178.2476\n",
      "Epoch 54/1500, Training Loss: 1179.6396, Validation Loss: 1149.0490\n",
      "Epoch 55/1500, Training Loss: 1150.3992, Validation Loss: 1120.8639\n",
      "Epoch 56/1500, Training Loss: 1122.1729, Validation Loss: 1093.6571\n",
      "Epoch 57/1500, Training Loss: 1094.9257, Validation Loss: 1067.3949\n",
      "Epoch 58/1500, Training Loss: 1068.6237, Validation Loss: 1042.0443\n",
      "Epoch 59/1500, Training Loss: 1043.2340, Validation Loss: 1017.5738\n",
      "Epoch 60/1500, Training Loss: 1018.7251, Validation Loss: 993.9526\n",
      "Epoch 61/1500, Training Loss: 995.0662, Validation Loss: 971.1517\n",
      "Epoch 62/1500, Training Loss: 972.2281, Validation Loss: 949.1420\n",
      "Epoch 63/1500, Training Loss: 950.1821, Validation Loss: 927.8966\n",
      "Epoch 64/1500, Training Loss: 928.9009, Validation Loss: 907.3888\n",
      "Epoch 65/1500, Training Loss: 908.3580, Validation Loss: 887.5929\n",
      "Epoch 66/1500, Training Loss: 888.5275, Validation Loss: 868.4842\n",
      "Epoch 67/1500, Training Loss: 869.3849, Validation Loss: 850.0389\n",
      "Epoch 68/1500, Training Loss: 850.9062, Validation Loss: 832.2341\n",
      "Epoch 69/1500, Training Loss: 833.0687, Validation Loss: 815.0474\n",
      "Epoch 70/1500, Training Loss: 815.8497, Validation Loss: 798.4574\n",
      "Epoch 71/1500, Training Loss: 799.2281, Validation Loss: 782.4434\n",
      "Epoch 72/1500, Training Loss: 783.1830, Validation Loss: 766.9854\n",
      "Epoch 73/1500, Training Loss: 767.6945, Validation Loss: 752.0641\n",
      "Epoch 74/1500, Training Loss: 752.7432, Validation Loss: 737.6608\n",
      "Epoch 75/1500, Training Loss: 738.3105, Validation Loss: 723.7578\n",
      "Epoch 76/1500, Training Loss: 724.3785, Validation Loss: 710.3373\n",
      "Epoch 77/1500, Training Loss: 710.9296, Validation Loss: 697.3830\n",
      "Epoch 78/1500, Training Loss: 697.9472, Validation Loss: 684.8785\n",
      "Epoch 79/1500, Training Loss: 685.4152, Validation Loss: 672.8080\n",
      "Epoch 80/1500, Training Loss: 673.3177, Validation Loss: 661.1567\n",
      "Epoch 81/1500, Training Loss: 661.6400, Validation Loss: 649.9101\n",
      "Epoch 82/1500, Training Loss: 650.3672, Validation Loss: 639.0540\n",
      "Epoch 83/1500, Training Loss: 639.4855, Validation Loss: 628.5748\n",
      "Epoch 84/1500, Training Loss: 628.9812, Validation Loss: 618.4596\n",
      "Epoch 85/1500, Training Loss: 618.8413, Validation Loss: 608.6956\n",
      "Epoch 86/1500, Training Loss: 609.0530, Validation Loss: 599.2708\n",
      "Epoch 87/1500, Training Loss: 599.6043, Validation Loss: 590.1732\n",
      "Epoch 88/1500, Training Loss: 590.4833, Validation Loss: 581.3916\n",
      "Epoch 89/1500, Training Loss: 581.6786, Validation Loss: 572.9150\n",
      "Epoch 90/1500, Training Loss: 573.1794, Validation Loss: 564.7327\n",
      "Epoch 91/1500, Training Loss: 564.9749, Validation Loss: 556.8347\n",
      "Epoch 92/1500, Training Loss: 557.0549, Validation Loss: 549.2109\n",
      "Epoch 93/1500, Training Loss: 549.4096, Validation Loss: 541.8519\n",
      "Epoch 94/1500, Training Loss: 542.0296, Validation Loss: 534.7485\n",
      "Epoch 95/1500, Training Loss: 534.9054, Validation Loss: 527.8918\n",
      "Epoch 96/1500, Training Loss: 528.0284, Validation Loss: 521.2734\n",
      "Epoch 97/1500, Training Loss: 521.3898, Validation Loss: 514.8847\n",
      "Epoch 98/1500, Training Loss: 514.9816, Validation Loss: 508.7180\n",
      "Epoch 99/1500, Training Loss: 508.7955, Validation Loss: 502.7656\n",
      "Epoch 100/1500, Training Loss: 502.8241, Validation Loss: 497.0199\n",
      "Epoch 101/1500, Training Loss: 497.0597, Validation Loss: 491.4738\n",
      "Epoch 102/1500, Training Loss: 491.4952, Validation Loss: 486.1203\n",
      "Epoch 103/1500, Training Loss: 486.1237, Validation Loss: 480.9528\n",
      "Epoch 104/1500, Training Loss: 480.9386, Validation Loss: 475.9648\n",
      "Epoch 105/1500, Training Loss: 475.9332, Validation Loss: 471.1502\n",
      "Epoch 106/1500, Training Loss: 471.1014, Validation Loss: 466.5028\n",
      "Epoch 107/1500, Training Loss: 466.4373, Validation Loss: 462.0169\n",
      "Epoch 108/1500, Training Loss: 461.9348, Validation Loss: 457.6868\n",
      "Epoch 109/1500, Training Loss: 457.5885, Validation Loss: 453.5072\n",
      "Epoch 110/1500, Training Loss: 453.3929, Validation Loss: 449.4727\n",
      "Epoch 111/1500, Training Loss: 449.3428, Validation Loss: 445.5786\n",
      "Epoch 112/1500, Training Loss: 445.4333, Validation Loss: 441.8197\n",
      "Epoch 113/1500, Training Loss: 441.6593, Validation Loss: 438.1914\n",
      "Epoch 114/1500, Training Loss: 438.0161, Validation Loss: 434.6892\n",
      "Epoch 115/1500, Training Loss: 434.4993, Validation Loss: 431.3087\n",
      "Epoch 116/1500, Training Loss: 431.1045, Validation Loss: 428.0457\n",
      "Epoch 117/1500, Training Loss: 427.8274, Validation Loss: 424.8961\n",
      "Epoch 118/1500, Training Loss: 424.6639, Validation Loss: 421.8559\n",
      "Epoch 119/1500, Training Loss: 421.6101, Validation Loss: 418.9214\n",
      "Epoch 120/1500, Training Loss: 418.6622, Validation Loss: 416.0889\n",
      "Epoch 121/1500, Training Loss: 415.8166, Validation Loss: 413.3549\n",
      "Epoch 122/1500, Training Loss: 413.0696, Validation Loss: 410.7159\n",
      "Epoch 123/1500, Training Loss: 410.4179, Validation Loss: 408.1685\n",
      "Epoch 124/1500, Training Loss: 407.8582, Validation Loss: 405.7098\n",
      "Epoch 125/1500, Training Loss: 405.3872, Validation Loss: 403.3365\n",
      "Epoch 126/1500, Training Loss: 403.0019, Validation Loss: 401.0457\n",
      "Epoch 127/1500, Training Loss: 400.6992, Validation Loss: 398.8346\n",
      "Epoch 128/1500, Training Loss: 398.4765, Validation Loss: 396.7004\n",
      "Epoch 129/1500, Training Loss: 396.3308, Validation Loss: 394.6403\n",
      "Epoch 130/1500, Training Loss: 394.2595, Validation Loss: 392.6519\n",
      "Epoch 131/1500, Training Loss: 392.2601, Validation Loss: 390.7326\n",
      "Epoch 132/1500, Training Loss: 390.3299, Validation Loss: 388.8800\n",
      "Epoch 133/1500, Training Loss: 388.4667, Validation Loss: 387.0918\n",
      "Epoch 134/1500, Training Loss: 386.6681, Validation Loss: 385.3659\n",
      "Epoch 135/1500, Training Loss: 384.9318, Validation Loss: 383.7000\n",
      "Epoch 136/1500, Training Loss: 383.2558, Validation Loss: 382.0919\n",
      "Epoch 137/1500, Training Loss: 381.6378, Validation Loss: 380.5399\n",
      "Epoch 138/1500, Training Loss: 380.0760, Validation Loss: 379.0418\n",
      "Epoch 139/1500, Training Loss: 378.5683, Validation Loss: 377.5958\n",
      "Epoch 140/1500, Training Loss: 377.1129, Validation Loss: 376.2001\n",
      "Epoch 141/1500, Training Loss: 375.7079, Validation Loss: 374.8529\n",
      "Epoch 142/1500, Training Loss: 374.3517, Validation Loss: 373.5527\n",
      "Epoch 143/1500, Training Loss: 373.0425, Validation Loss: 372.2976\n",
      "Epoch 144/1500, Training Loss: 371.7787, Validation Loss: 371.0862\n",
      "Epoch 145/1500, Training Loss: 370.5586, Validation Loss: 369.9170\n",
      "Epoch 146/1500, Training Loss: 369.3809, Validation Loss: 368.7884\n",
      "Epoch 147/1500, Training Loss: 368.2440, Validation Loss: 367.6991\n",
      "Epoch 148/1500, Training Loss: 367.1465, Validation Loss: 366.6477\n",
      "Epoch 149/1500, Training Loss: 366.0871, Validation Loss: 365.6329\n",
      "Epoch 150/1500, Training Loss: 365.0644, Validation Loss: 364.6534\n",
      "Epoch 151/1500, Training Loss: 364.0771, Validation Loss: 363.7081\n",
      "Epoch 152/1500, Training Loss: 363.1241, Validation Loss: 362.7956\n",
      "Epoch 153/1500, Training Loss: 362.2041, Validation Loss: 361.9149\n",
      "Epoch 154/1500, Training Loss: 361.3160, Validation Loss: 361.0648\n",
      "Epoch 155/1500, Training Loss: 360.4587, Validation Loss: 360.2443\n",
      "Epoch 156/1500, Training Loss: 359.6311, Validation Loss: 359.4523\n",
      "Epoch 157/1500, Training Loss: 358.8322, Validation Loss: 358.6880\n",
      "Epoch 158/1500, Training Loss: 358.0609, Validation Loss: 357.9503\n",
      "Epoch 159/1500, Training Loss: 357.3164, Validation Loss: 357.2382\n",
      "Epoch 160/1500, Training Loss: 356.5977, Validation Loss: 356.5509\n",
      "Epoch 161/1500, Training Loss: 355.9039, Validation Loss: 355.8876\n",
      "Epoch 162/1500, Training Loss: 355.2342, Validation Loss: 355.2474\n",
      "Epoch 163/1500, Training Loss: 354.5877, Validation Loss: 354.6295\n",
      "Epoch 164/1500, Training Loss: 353.9636, Validation Loss: 354.0331\n",
      "Epoch 165/1500, Training Loss: 353.3611, Validation Loss: 353.4575\n",
      "Epoch 166/1500, Training Loss: 352.7795, Validation Loss: 352.9019\n",
      "Epoch 167/1500, Training Loss: 352.2180, Validation Loss: 352.3657\n",
      "Epoch 168/1500, Training Loss: 351.6761, Validation Loss: 351.8482\n",
      "Epoch 169/1500, Training Loss: 351.1529, Validation Loss: 351.3487\n",
      "Epoch 170/1500, Training Loss: 350.6478, Validation Loss: 350.8665\n",
      "Epoch 171/1500, Training Loss: 350.1602, Validation Loss: 350.4013\n",
      "Epoch 172/1500, Training Loss: 349.6895, Validation Loss: 349.9522\n",
      "Epoch 173/1500, Training Loss: 349.2352, Validation Loss: 349.5188\n",
      "Epoch 174/1500, Training Loss: 348.7965, Validation Loss: 349.1004\n",
      "Epoch 175/1500, Training Loss: 348.3731, Validation Loss: 348.6967\n",
      "Epoch 176/1500, Training Loss: 347.9644, Validation Loss: 348.3071\n",
      "Epoch 177/1500, Training Loss: 347.5698, Validation Loss: 347.9310\n",
      "Epoch 178/1500, Training Loss: 347.1889, Validation Loss: 347.5681\n",
      "Epoch 179/1500, Training Loss: 346.8211, Validation Loss: 347.2177\n",
      "Epoch 180/1500, Training Loss: 346.4662, Validation Loss: 346.8796\n",
      "Epoch 181/1500, Training Loss: 346.1234, Validation Loss: 346.5534\n",
      "Epoch 182/1500, Training Loss: 345.7926, Validation Loss: 346.2384\n",
      "Epoch 183/1500, Training Loss: 345.4733, Validation Loss: 345.9345\n",
      "Epoch 184/1500, Training Loss: 345.1650, Validation Loss: 345.6412\n",
      "Epoch 185/1500, Training Loss: 344.8674, Validation Loss: 345.3581\n",
      "Epoch 186/1500, Training Loss: 344.5801, Validation Loss: 345.0849\n",
      "Epoch 187/1500, Training Loss: 344.3028, Validation Loss: 344.8213\n",
      "Epoch 188/1500, Training Loss: 344.0350, Validation Loss: 344.5668\n",
      "Epoch 189/1500, Training Loss: 343.7765, Validation Loss: 344.3212\n",
      "Epoch 190/1500, Training Loss: 343.5270, Validation Loss: 344.0842\n",
      "Epoch 191/1500, Training Loss: 343.2862, Validation Loss: 343.8554\n",
      "Epoch 192/1500, Training Loss: 343.0536, Validation Loss: 343.6347\n",
      "Epoch 193/1500, Training Loss: 342.8291, Validation Loss: 343.4217\n",
      "Epoch 194/1500, Training Loss: 342.6124, Validation Loss: 343.2161\n",
      "Epoch 195/1500, Training Loss: 342.4033, Validation Loss: 343.0177\n",
      "Epoch 196/1500, Training Loss: 342.2013, Validation Loss: 342.8262\n",
      "Epoch 197/1500, Training Loss: 342.0063, Validation Loss: 342.6414\n",
      "Epoch 198/1500, Training Loss: 341.8181, Validation Loss: 342.4630\n",
      "Epoch 199/1500, Training Loss: 341.6364, Validation Loss: 342.2910\n",
      "Epoch 200/1500, Training Loss: 341.4610, Validation Loss: 342.1249\n",
      "Epoch 201/1500, Training Loss: 341.2917, Validation Loss: 341.9646\n",
      "Epoch 202/1500, Training Loss: 341.1282, Validation Loss: 341.8099\n",
      "Epoch 203/1500, Training Loss: 340.9704, Validation Loss: 341.6606\n",
      "Epoch 204/1500, Training Loss: 340.8181, Validation Loss: 341.5166\n",
      "Epoch 205/1500, Training Loss: 340.6710, Validation Loss: 341.3776\n",
      "Epoch 206/1500, Training Loss: 340.5290, Validation Loss: 341.2434\n",
      "Epoch 207/1500, Training Loss: 340.3919, Validation Loss: 341.1140\n",
      "Epoch 208/1500, Training Loss: 340.2596, Validation Loss: 340.9891\n",
      "Epoch 209/1500, Training Loss: 340.1318, Validation Loss: 340.8685\n",
      "Epoch 210/1500, Training Loss: 340.0085, Validation Loss: 340.7522\n",
      "Epoch 211/1500, Training Loss: 339.8895, Validation Loss: 340.6399\n",
      "Epoch 212/1500, Training Loss: 339.7745, Validation Loss: 340.5316\n",
      "Epoch 213/1500, Training Loss: 339.6636, Validation Loss: 340.4271\n",
      "Epoch 214/1500, Training Loss: 339.5564, Validation Loss: 340.3262\n",
      "Epoch 215/1500, Training Loss: 339.4530, Validation Loss: 340.2289\n",
      "Epoch 216/1500, Training Loss: 339.3532, Validation Loss: 340.1349\n",
      "Epoch 217/1500, Training Loss: 339.2568, Validation Loss: 340.0443\n",
      "Epoch 218/1500, Training Loss: 339.1637, Validation Loss: 339.9568\n",
      "Epoch 219/1500, Training Loss: 339.0739, Validation Loss: 339.8724\n",
      "Epoch 220/1500, Training Loss: 338.9872, Validation Loss: 339.7910\n",
      "Epoch 221/1500, Training Loss: 338.9034, Validation Loss: 339.7124\n",
      "Epoch 222/1500, Training Loss: 338.8226, Validation Loss: 339.6366\n",
      "Epoch 223/1500, Training Loss: 338.7446, Validation Loss: 339.5634\n",
      "Epoch 224/1500, Training Loss: 338.6692, Validation Loss: 339.4929\n",
      "Epoch 225/1500, Training Loss: 338.5965, Validation Loss: 339.4248\n",
      "Epoch 226/1500, Training Loss: 338.5263, Validation Loss: 339.3590\n",
      "Epoch 227/1500, Training Loss: 338.4585, Validation Loss: 339.2956\n",
      "Epoch 228/1500, Training Loss: 338.3931, Validation Loss: 339.2344\n",
      "Epoch 229/1500, Training Loss: 338.3299, Validation Loss: 339.1754\n",
      "Epoch 230/1500, Training Loss: 338.2689, Validation Loss: 339.1184\n",
      "Epoch 231/1500, Training Loss: 338.2100, Validation Loss: 339.0634\n",
      "Epoch 232/1500, Training Loss: 338.1531, Validation Loss: 339.0104\n",
      "Epoch 233/1500, Training Loss: 338.0982, Validation Loss: 338.9592\n",
      "Epoch 234/1500, Training Loss: 338.0452, Validation Loss: 338.9099\n",
      "Epoch 235/1500, Training Loss: 337.9940, Validation Loss: 338.8622\n",
      "Epoch 236/1500, Training Loss: 337.9446, Validation Loss: 338.8163\n",
      "Epoch 237/1500, Training Loss: 337.8970, Validation Loss: 338.7720\n",
      "Epoch 238/1500, Training Loss: 337.8509, Validation Loss: 338.7292\n",
      "Epoch 239/1500, Training Loss: 337.8065, Validation Loss: 338.6879\n",
      "Epoch 240/1500, Training Loss: 337.7635, Validation Loss: 338.6481\n",
      "Epoch 241/1500, Training Loss: 337.7221, Validation Loss: 338.6097\n",
      "Epoch 242/1500, Training Loss: 337.6821, Validation Loss: 338.5726\n",
      "Epoch 243/1500, Training Loss: 337.6435, Validation Loss: 338.5369\n",
      "Epoch 244/1500, Training Loss: 337.6062, Validation Loss: 338.5024\n",
      "Epoch 245/1500, Training Loss: 337.5702, Validation Loss: 338.4691\n",
      "Epoch 246/1500, Training Loss: 337.5354, Validation Loss: 338.4370\n",
      "Epoch 247/1500, Training Loss: 337.5019, Validation Loss: 338.4060\n",
      "Epoch 248/1500, Training Loss: 337.4694, Validation Loss: 338.3762\n",
      "Epoch 249/1500, Training Loss: 337.4381, Validation Loss: 338.3474\n",
      "Epoch 250/1500, Training Loss: 337.4080, Validation Loss: 338.3196\n",
      "Epoch 251/1500, Training Loss: 337.3788, Validation Loss: 338.2927\n",
      "Epoch 252/1500, Training Loss: 337.3506, Validation Loss: 338.2668\n",
      "Epoch 253/1500, Training Loss: 337.3234, Validation Loss: 338.2419\n",
      "Epoch 254/1500, Training Loss: 337.2971, Validation Loss: 338.2178\n",
      "Epoch 255/1500, Training Loss: 337.2719, Validation Loss: 338.1946\n",
      "Epoch 256/1500, Training Loss: 337.2473, Validation Loss: 338.1722\n",
      "Epoch 257/1500, Training Loss: 337.2237, Validation Loss: 338.1506\n",
      "Epoch 258/1500, Training Loss: 337.2009, Validation Loss: 338.1297\n",
      "Epoch 259/1500, Training Loss: 337.1788, Validation Loss: 338.1096\n",
      "Epoch 260/1500, Training Loss: 337.1576, Validation Loss: 338.0903\n",
      "Epoch 261/1500, Training Loss: 337.1371, Validation Loss: 338.0716\n",
      "Epoch 262/1500, Training Loss: 337.1172, Validation Loss: 338.0535\n",
      "Epoch 263/1500, Training Loss: 337.0981, Validation Loss: 338.0361\n",
      "Epoch 264/1500, Training Loss: 337.0795, Validation Loss: 338.0193\n",
      "Epoch 265/1500, Training Loss: 337.0617, Validation Loss: 338.0031\n",
      "Epoch 266/1500, Training Loss: 337.0445, Validation Loss: 337.9875\n",
      "Epoch 267/1500, Training Loss: 337.0278, Validation Loss: 337.9725\n",
      "Epoch 268/1500, Training Loss: 337.0117, Validation Loss: 337.9579\n",
      "Epoch 269/1500, Training Loss: 336.9962, Validation Loss: 337.9439\n",
      "Epoch 270/1500, Training Loss: 336.9812, Validation Loss: 337.9304\n",
      "Epoch 271/1500, Training Loss: 336.9667, Validation Loss: 337.9174\n",
      "Epoch 272/1500, Training Loss: 336.9528, Validation Loss: 337.9048\n",
      "Epoch 273/1500, Training Loss: 336.9393, Validation Loss: 337.8927\n",
      "Epoch 274/1500, Training Loss: 336.9262, Validation Loss: 337.8810\n",
      "Epoch 275/1500, Training Loss: 336.9136, Validation Loss: 337.8698\n",
      "Epoch 276/1500, Training Loss: 336.9015, Validation Loss: 337.8589\n",
      "Epoch 277/1500, Training Loss: 336.8897, Validation Loss: 337.8484\n",
      "Epoch 278/1500, Training Loss: 336.8784, Validation Loss: 337.8383\n",
      "Epoch 279/1500, Training Loss: 336.8674, Validation Loss: 337.8286\n",
      "Epoch 280/1500, Training Loss: 336.8569, Validation Loss: 337.8192\n",
      "Epoch 281/1500, Training Loss: 336.8467, Validation Loss: 337.8102\n",
      "Epoch 282/1500, Training Loss: 336.8368, Validation Loss: 337.8014\n",
      "Epoch 283/1500, Training Loss: 336.8273, Validation Loss: 337.7930\n",
      "Epoch 284/1500, Training Loss: 336.8181, Validation Loss: 337.7849\n",
      "Epoch 285/1500, Training Loss: 336.8092, Validation Loss: 337.7770\n",
      "Epoch 286/1500, Training Loss: 336.8006, Validation Loss: 337.7695\n",
      "Epoch 287/1500, Training Loss: 336.7924, Validation Loss: 337.7622\n",
      "Epoch 288/1500, Training Loss: 336.7844, Validation Loss: 337.7552\n",
      "Epoch 289/1500, Training Loss: 336.7766, Validation Loss: 337.7484\n",
      "Epoch 290/1500, Training Loss: 336.7692, Validation Loss: 337.7419\n",
      "Epoch 291/1500, Training Loss: 336.7620, Validation Loss: 337.7357\n",
      "Epoch 292/1500, Training Loss: 336.7550, Validation Loss: 337.7296\n",
      "Epoch 293/1500, Training Loss: 336.7483, Validation Loss: 337.7237\n",
      "Epoch 294/1500, Training Loss: 336.7418, Validation Loss: 337.7181\n",
      "Epoch 295/1500, Training Loss: 336.7355, Validation Loss: 337.7127\n",
      "Epoch 296/1500, Training Loss: 336.7294, Validation Loss: 337.7074\n",
      "Epoch 297/1500, Training Loss: 336.7236, Validation Loss: 337.7024\n",
      "Epoch 298/1500, Training Loss: 336.7180, Validation Loss: 337.6976\n",
      "Epoch 299/1500, Training Loss: 336.7125, Validation Loss: 337.6929\n",
      "Epoch 300/1500, Training Loss: 336.7072, Validation Loss: 337.6883\n",
      "Epoch 301/1500, Training Loss: 336.7021, Validation Loss: 337.6840\n",
      "Epoch 302/1500, Training Loss: 336.6972, Validation Loss: 337.6798\n",
      "Epoch 303/1500, Training Loss: 336.6924, Validation Loss: 337.6758\n",
      "Epoch 304/1500, Training Loss: 336.6878, Validation Loss: 337.6719\n",
      "Epoch 305/1500, Training Loss: 336.6834, Validation Loss: 337.6681\n",
      "Epoch 306/1500, Training Loss: 336.6791, Validation Loss: 337.6645\n",
      "Epoch 307/1500, Training Loss: 336.6750, Validation Loss: 337.6610\n",
      "Epoch 308/1500, Training Loss: 336.6710, Validation Loss: 337.6577\n",
      "Epoch 309/1500, Training Loss: 336.6671, Validation Loss: 337.6544\n",
      "Epoch 310/1500, Training Loss: 336.6634, Validation Loss: 337.6513\n",
      "Epoch 311/1500, Training Loss: 336.6598, Validation Loss: 337.6483\n",
      "Epoch 312/1500, Training Loss: 336.6563, Validation Loss: 337.6454\n",
      "Epoch 313/1500, Training Loss: 336.6529, Validation Loss: 337.6426\n",
      "Epoch 314/1500, Training Loss: 336.6497, Validation Loss: 337.6399\n",
      "Epoch 315/1500, Training Loss: 336.6465, Validation Loss: 337.6373\n",
      "Epoch 316/1500, Training Loss: 336.6434, Validation Loss: 337.6349\n",
      "Epoch 317/1500, Training Loss: 336.6405, Validation Loss: 337.6324\n",
      "Epoch 318/1500, Training Loss: 336.6377, Validation Loss: 337.6301\n",
      "Epoch 319/1500, Training Loss: 336.6349, Validation Loss: 337.6279\n",
      "Epoch 320/1500, Training Loss: 336.6323, Validation Loss: 337.6258\n",
      "Epoch 321/1500, Training Loss: 336.6297, Validation Loss: 337.6237\n",
      "Epoch 322/1500, Training Loss: 336.6273, Validation Loss: 337.6217\n",
      "Epoch 323/1500, Training Loss: 336.6248, Validation Loss: 337.6198\n",
      "Epoch 324/1500, Training Loss: 336.6226, Validation Loss: 337.6179\n",
      "Epoch 325/1500, Training Loss: 336.6203, Validation Loss: 337.6161\n",
      "Epoch 326/1500, Training Loss: 336.6181, Validation Loss: 337.6144\n",
      "Epoch 327/1500, Training Loss: 336.6161, Validation Loss: 337.6128\n",
      "Epoch 328/1500, Training Loss: 336.6140, Validation Loss: 337.6112\n",
      "Epoch 329/1500, Training Loss: 336.6121, Validation Loss: 337.6096\n",
      "Epoch 330/1500, Training Loss: 336.6102, Validation Loss: 337.6082\n",
      "Epoch 331/1500, Training Loss: 336.6083, Validation Loss: 337.6068\n",
      "Epoch 332/1500, Training Loss: 336.6066, Validation Loss: 337.6054\n",
      "Epoch 333/1500, Training Loss: 336.6049, Validation Loss: 337.6041\n",
      "Epoch 334/1500, Training Loss: 336.6032, Validation Loss: 337.6028\n",
      "Epoch 335/1500, Training Loss: 336.6017, Validation Loss: 337.6016\n",
      "Epoch 336/1500, Training Loss: 336.6001, Validation Loss: 337.6005\n",
      "Epoch 337/1500, Training Loss: 336.5986, Validation Loss: 337.5993\n",
      "Epoch 338/1500, Training Loss: 336.5972, Validation Loss: 337.5982\n",
      "Epoch 339/1500, Training Loss: 336.5957, Validation Loss: 337.5972\n",
      "Epoch 340/1500, Training Loss: 336.5944, Validation Loss: 337.5962\n",
      "Epoch 341/1500, Training Loss: 336.5931, Validation Loss: 337.5952\n",
      "Epoch 342/1500, Training Loss: 336.5919, Validation Loss: 337.5943\n",
      "Epoch 343/1500, Training Loss: 336.5906, Validation Loss: 337.5934\n",
      "Epoch 344/1500, Training Loss: 336.5895, Validation Loss: 337.5925\n",
      "Epoch 345/1500, Training Loss: 336.5883, Validation Loss: 337.5917\n",
      "Epoch 346/1500, Training Loss: 336.5872, Validation Loss: 337.5909\n",
      "Epoch 347/1500, Training Loss: 336.5862, Validation Loss: 337.5901\n",
      "Epoch 348/1500, Training Loss: 336.5851, Validation Loss: 337.5894\n",
      "Epoch 349/1500, Training Loss: 336.5841, Validation Loss: 337.5887\n",
      "Epoch 350/1500, Training Loss: 336.5832, Validation Loss: 337.5880\n",
      "Epoch 351/1500, Training Loss: 336.5822, Validation Loss: 337.5873\n",
      "Epoch 352/1500, Training Loss: 336.5813, Validation Loss: 337.5867\n",
      "Epoch 353/1500, Training Loss: 336.5804, Validation Loss: 337.5861\n",
      "Epoch 354/1500, Training Loss: 336.5796, Validation Loss: 337.5855\n",
      "Epoch 355/1500, Training Loss: 336.5788, Validation Loss: 337.5850\n",
      "Epoch 356/1500, Training Loss: 336.5780, Validation Loss: 337.5844\n",
      "Epoch 357/1500, Training Loss: 336.5772, Validation Loss: 337.5839\n",
      "Epoch 358/1500, Training Loss: 336.5764, Validation Loss: 337.5834\n",
      "Epoch 359/1500, Training Loss: 336.5757, Validation Loss: 337.5829\n",
      "Epoch 360/1500, Training Loss: 336.5750, Validation Loss: 337.5825\n",
      "Epoch 361/1500, Training Loss: 336.5743, Validation Loss: 337.5820\n",
      "Epoch 362/1500, Training Loss: 336.5737, Validation Loss: 337.5816\n",
      "Epoch 363/1500, Training Loss: 336.5731, Validation Loss: 337.5812\n",
      "Epoch 364/1500, Training Loss: 336.5724, Validation Loss: 337.5808\n",
      "Epoch 365/1500, Training Loss: 336.5719, Validation Loss: 337.5804\n",
      "Epoch 366/1500, Training Loss: 336.5713, Validation Loss: 337.5800\n",
      "Epoch 367/1500, Training Loss: 336.5707, Validation Loss: 337.5797\n",
      "Epoch 368/1500, Training Loss: 336.5702, Validation Loss: 337.5794\n",
      "Epoch 369/1500, Training Loss: 336.5697, Validation Loss: 337.5790\n",
      "Epoch 370/1500, Training Loss: 336.5691, Validation Loss: 337.5787\n",
      "Epoch 371/1500, Training Loss: 336.5686, Validation Loss: 337.5785\n",
      "Epoch 372/1500, Training Loss: 336.5682, Validation Loss: 337.5782\n",
      "Epoch 373/1500, Training Loss: 336.5677, Validation Loss: 337.5779\n",
      "Epoch 374/1500, Training Loss: 336.5673, Validation Loss: 337.5776\n",
      "Epoch 375/1500, Training Loss: 336.5668, Validation Loss: 337.5774\n",
      "Epoch 376/1500, Training Loss: 336.5664, Validation Loss: 337.5771\n",
      "Epoch 377/1500, Training Loss: 336.5660, Validation Loss: 337.5769\n",
      "Epoch 378/1500, Training Loss: 336.5656, Validation Loss: 337.5767\n",
      "Epoch 379/1500, Training Loss: 336.5652, Validation Loss: 337.5765\n",
      "Epoch 380/1500, Training Loss: 336.5648, Validation Loss: 337.5763\n",
      "Epoch 381/1500, Training Loss: 336.5645, Validation Loss: 337.5761\n",
      "Epoch 382/1500, Training Loss: 336.5641, Validation Loss: 337.5759\n",
      "Epoch 383/1500, Training Loss: 336.5638, Validation Loss: 337.5757\n",
      "Epoch 384/1500, Training Loss: 336.5634, Validation Loss: 337.5755\n",
      "Epoch 385/1500, Training Loss: 336.5631, Validation Loss: 337.5753\n",
      "Epoch 386/1500, Training Loss: 336.5628, Validation Loss: 337.5752\n",
      "Epoch 387/1500, Training Loss: 336.5625, Validation Loss: 337.5751\n",
      "Epoch 388/1500, Training Loss: 336.5622, Validation Loss: 337.5749\n",
      "Epoch 389/1500, Training Loss: 336.5619, Validation Loss: 337.5748\n",
      "Epoch 390/1500, Training Loss: 336.5616, Validation Loss: 337.5746\n",
      "Epoch 391/1500, Training Loss: 336.5614, Validation Loss: 337.5745\n",
      "Epoch 392/1500, Training Loss: 336.5611, Validation Loss: 337.5744\n",
      "Epoch 393/1500, Training Loss: 336.5609, Validation Loss: 337.5742\n",
      "Epoch 394/1500, Training Loss: 336.5606, Validation Loss: 337.5741\n",
      "Epoch 395/1500, Training Loss: 336.5604, Validation Loss: 337.5740\n",
      "Epoch 396/1500, Training Loss: 336.5601, Validation Loss: 337.5739\n",
      "Epoch 397/1500, Training Loss: 336.5599, Validation Loss: 337.5738\n",
      "Epoch 398/1500, Training Loss: 336.5597, Validation Loss: 337.5737\n",
      "Epoch 399/1500, Training Loss: 336.5595, Validation Loss: 337.5736\n",
      "Epoch 400/1500, Training Loss: 336.5592, Validation Loss: 337.5735\n",
      "Epoch 401/1500, Training Loss: 336.5591, Validation Loss: 337.5734\n",
      "Epoch 402/1500, Training Loss: 336.5588, Validation Loss: 337.5734\n",
      "Epoch 403/1500, Training Loss: 336.5587, Validation Loss: 337.5733\n",
      "Epoch 404/1500, Training Loss: 336.5585, Validation Loss: 337.5732\n",
      "Epoch 405/1500, Training Loss: 336.5583, Validation Loss: 337.5732\n",
      "Epoch 406/1500, Training Loss: 336.5581, Validation Loss: 337.5731\n",
      "Epoch 407/1500, Training Loss: 336.5579, Validation Loss: 337.5730\n",
      "Epoch 408/1500, Training Loss: 336.5578, Validation Loss: 337.5730\n",
      "Epoch 409/1500, Training Loss: 336.5576, Validation Loss: 337.5729\n",
      "Epoch 410/1500, Training Loss: 336.5574, Validation Loss: 337.5728\n",
      "Epoch 411/1500, Training Loss: 336.5573, Validation Loss: 337.5728\n",
      "Epoch 412/1500, Training Loss: 336.5571, Validation Loss: 337.5727\n",
      "Epoch 413/1500, Training Loss: 336.5569, Validation Loss: 337.5727\n",
      "Epoch 414/1500, Training Loss: 336.5568, Validation Loss: 337.5727\n",
      "Epoch 415/1500, Training Loss: 336.5567, Validation Loss: 337.5726\n",
      "Epoch 416/1500, Training Loss: 336.5565, Validation Loss: 337.5725\n",
      "Epoch 417/1500, Training Loss: 336.5564, Validation Loss: 337.5725\n",
      "Epoch 418/1500, Training Loss: 336.5563, Validation Loss: 337.5724\n",
      "Epoch 419/1500, Training Loss: 336.5562, Validation Loss: 337.5724\n",
      "Epoch 420/1500, Training Loss: 336.5560, Validation Loss: 337.5724\n",
      "Epoch 421/1500, Training Loss: 336.5559, Validation Loss: 337.5724\n",
      "Epoch 422/1500, Training Loss: 336.5558, Validation Loss: 337.5723\n",
      "Epoch 423/1500, Training Loss: 336.5557, Validation Loss: 337.5723\n",
      "Epoch 424/1500, Training Loss: 336.5556, Validation Loss: 337.5723\n",
      "Epoch 425/1500, Training Loss: 336.5555, Validation Loss: 337.5722\n",
      "Epoch 426/1500, Training Loss: 336.5553, Validation Loss: 337.5722\n",
      "Epoch 427/1500, Training Loss: 336.5552, Validation Loss: 337.5722\n",
      "Epoch 428/1500, Training Loss: 336.5551, Validation Loss: 337.5722\n",
      "Epoch 429/1500, Training Loss: 336.5550, Validation Loss: 337.5721\n",
      "Epoch 430/1500, Training Loss: 336.5549, Validation Loss: 337.5721\n",
      "Epoch 431/1500, Training Loss: 336.5548, Validation Loss: 337.5721\n",
      "Epoch 432/1500, Training Loss: 336.5547, Validation Loss: 337.5721\n",
      "Epoch 433/1500, Training Loss: 336.5546, Validation Loss: 337.5720\n",
      "Epoch 434/1500, Training Loss: 336.5545, Validation Loss: 337.5720\n",
      "Epoch 435/1500, Training Loss: 336.5544, Validation Loss: 337.5720\n",
      "Epoch 436/1500, Training Loss: 336.5544, Validation Loss: 337.5720\n",
      "Epoch 437/1500, Training Loss: 336.5543, Validation Loss: 337.5720\n",
      "Epoch 438/1500, Training Loss: 336.5542, Validation Loss: 337.5719\n",
      "Epoch 439/1500, Training Loss: 336.5541, Validation Loss: 337.5719\n",
      "Epoch 440/1500, Training Loss: 336.5540, Validation Loss: 337.5719\n",
      "Epoch 441/1500, Training Loss: 336.5540, Validation Loss: 337.5719\n",
      "Epoch 442/1500, Training Loss: 336.5539, Validation Loss: 337.5719\n",
      "Epoch 443/1500, Training Loss: 336.5538, Validation Loss: 337.5719\n",
      "Epoch 444/1500, Training Loss: 336.5537, Validation Loss: 337.5719\n",
      "Epoch 445/1500, Training Loss: 336.5536, Validation Loss: 337.5718\n",
      "Epoch 446/1500, Training Loss: 336.5536, Validation Loss: 337.5718\n",
      "Epoch 447/1500, Training Loss: 336.5535, Validation Loss: 337.5718\n",
      "Epoch 448/1500, Training Loss: 336.5534, Validation Loss: 337.5718\n",
      "Epoch 449/1500, Training Loss: 336.5533, Validation Loss: 337.5718\n",
      "Epoch 450/1500, Training Loss: 336.5533, Validation Loss: 337.5718\n",
      "Epoch 451/1500, Training Loss: 336.5532, Validation Loss: 337.5718\n",
      "Epoch 452/1500, Training Loss: 336.5532, Validation Loss: 337.5718\n",
      "Epoch 453/1500, Training Loss: 336.5531, Validation Loss: 337.5718\n",
      "Epoch 454/1500, Training Loss: 336.5530, Validation Loss: 337.5717\n",
      "Epoch 455/1500, Training Loss: 336.5529, Validation Loss: 337.5717\n",
      "Epoch 456/1500, Training Loss: 336.5529, Validation Loss: 337.5717\n",
      "Epoch 457/1500, Training Loss: 336.5528, Validation Loss: 337.5717\n",
      "Epoch 458/1500, Training Loss: 336.5528, Validation Loss: 337.5717\n",
      "Epoch 459/1500, Training Loss: 336.5527, Validation Loss: 337.5717\n",
      "Epoch 460/1500, Training Loss: 336.5526, Validation Loss: 337.5717\n",
      "Epoch 461/1500, Training Loss: 336.5526, Validation Loss: 337.5717\n",
      "Epoch 462/1500, Training Loss: 336.5526, Validation Loss: 337.5717\n",
      "Epoch 463/1500, Training Loss: 336.5525, Validation Loss: 337.5717\n",
      "Epoch 464/1500, Training Loss: 336.5525, Validation Loss: 337.5717\n",
      "Epoch 465/1500, Training Loss: 336.5524, Validation Loss: 337.5717\n",
      "Epoch 466/1500, Training Loss: 336.5524, Validation Loss: 337.5717\n",
      "Epoch 467/1500, Training Loss: 336.5523, Validation Loss: 337.5717\n",
      "Epoch 468/1500, Training Loss: 336.5522, Validation Loss: 337.5717\n",
      "Epoch 469/1500, Training Loss: 336.5522, Validation Loss: 337.5717\n",
      "Epoch 470/1500, Training Loss: 336.5522, Validation Loss: 337.5717\n",
      "Epoch 471/1500, Training Loss: 336.5521, Validation Loss: 337.5717\n",
      "Epoch 472/1500, Training Loss: 336.5520, Validation Loss: 337.5717\n",
      "Epoch 473/1500, Training Loss: 336.5520, Validation Loss: 337.5717\n",
      "Epoch 474/1500, Training Loss: 336.5519, Validation Loss: 337.5717\n",
      "Epoch 475/1500, Training Loss: 336.5519, Validation Loss: 337.5717\n",
      "Epoch 476/1500, Training Loss: 336.5518, Validation Loss: 337.5717\n",
      "Epoch 477/1500, Training Loss: 336.5518, Validation Loss: 337.5717\n",
      "Epoch 478/1500, Training Loss: 336.5518, Validation Loss: 337.5717\n",
      "Epoch 479/1500, Training Loss: 336.5517, Validation Loss: 337.5717\n",
      "Epoch 480/1500, Training Loss: 336.5517, Validation Loss: 337.5716\n",
      "Epoch 481/1500, Training Loss: 336.5516, Validation Loss: 337.5716\n",
      "Epoch 482/1500, Training Loss: 336.5516, Validation Loss: 337.5716\n",
      "Epoch 483/1500, Training Loss: 336.5515, Validation Loss: 337.5716\n",
      "Epoch 484/1500, Training Loss: 336.5515, Validation Loss: 337.5716\n",
      "Epoch 485/1500, Training Loss: 336.5515, Validation Loss: 337.5716\n",
      "Epoch 486/1500, Training Loss: 336.5514, Validation Loss: 337.5716\n",
      "Epoch 487/1500, Training Loss: 336.5514, Validation Loss: 337.5716\n",
      "Epoch 488/1500, Training Loss: 336.5513, Validation Loss: 337.5716\n",
      "Epoch 489/1500, Training Loss: 336.5513, Validation Loss: 337.5716\n",
      "Epoch 490/1500, Training Loss: 336.5512, Validation Loss: 337.5716\n",
      "Epoch 491/1500, Training Loss: 336.5512, Validation Loss: 337.5716\n",
      "Epoch 492/1500, Training Loss: 336.5512, Validation Loss: 337.5716\n",
      "Epoch 493/1500, Training Loss: 336.5511, Validation Loss: 337.5716\n",
      "Epoch 494/1500, Training Loss: 336.5511, Validation Loss: 337.5716\n",
      "Epoch 495/1500, Training Loss: 336.5511, Validation Loss: 337.5716\n",
      "Epoch 496/1500, Training Loss: 336.5510, Validation Loss: 337.5716\n",
      "Epoch 497/1500, Training Loss: 336.5510, Validation Loss: 337.5715\n",
      "Epoch 498/1500, Training Loss: 336.5510, Validation Loss: 337.5716\n",
      "Epoch 499/1500, Training Loss: 336.5509, Validation Loss: 337.5715\n",
      "Epoch 500/1500, Training Loss: 336.5509, Validation Loss: 337.5715\n",
      "Epoch 501/1500, Training Loss: 336.5508, Validation Loss: 337.5715\n",
      "Epoch 502/1500, Training Loss: 336.5508, Validation Loss: 337.5715\n",
      "Epoch 503/1500, Training Loss: 336.5508, Validation Loss: 337.5715\n",
      "Epoch 504/1500, Training Loss: 336.5507, Validation Loss: 337.5715\n",
      "Epoch 505/1500, Training Loss: 336.5507, Validation Loss: 337.5715\n",
      "Epoch 506/1500, Training Loss: 336.5507, Validation Loss: 337.5715\n",
      "Epoch 507/1500, Training Loss: 336.5506, Validation Loss: 337.5715\n",
      "Epoch 508/1500, Training Loss: 336.5506, Validation Loss: 337.5715\n",
      "Epoch 509/1500, Training Loss: 336.5506, Validation Loss: 337.5715\n",
      "Epoch 510/1500, Training Loss: 336.5505, Validation Loss: 337.5715\n",
      "Epoch 511/1500, Training Loss: 336.5505, Validation Loss: 337.5715\n",
      "Epoch 512/1500, Training Loss: 336.5505, Validation Loss: 337.5715\n",
      "Epoch 513/1500, Training Loss: 336.5504, Validation Loss: 337.5715\n",
      "Epoch 514/1500, Training Loss: 336.5504, Validation Loss: 337.5715\n",
      "Epoch 515/1500, Training Loss: 336.5504, Validation Loss: 337.5715\n",
      "Epoch 516/1500, Training Loss: 336.5504, Validation Loss: 337.5715\n",
      "Epoch 517/1500, Training Loss: 336.5503, Validation Loss: 337.5715\n",
      "Epoch 518/1500, Training Loss: 336.5503, Validation Loss: 337.5714\n",
      "Epoch 519/1500, Training Loss: 336.5503, Validation Loss: 337.5714\n",
      "Epoch 520/1500, Training Loss: 336.5502, Validation Loss: 337.5714\n",
      "Epoch 521/1500, Training Loss: 336.5502, Validation Loss: 337.5714\n",
      "Epoch 522/1500, Training Loss: 336.5502, Validation Loss: 337.5714\n",
      "Epoch 523/1500, Training Loss: 336.5501, Validation Loss: 337.5714\n",
      "Epoch 524/1500, Training Loss: 336.5501, Validation Loss: 337.5714\n",
      "Epoch 525/1500, Training Loss: 336.5500, Validation Loss: 337.5714\n",
      "Epoch 526/1500, Training Loss: 336.5500, Validation Loss: 337.5714\n",
      "Epoch 527/1500, Training Loss: 336.5500, Validation Loss: 337.5714\n",
      "Epoch 528/1500, Training Loss: 336.5500, Validation Loss: 337.5714\n",
      "Epoch 529/1500, Training Loss: 336.5500, Validation Loss: 337.5714\n",
      "Epoch 530/1500, Training Loss: 336.5499, Validation Loss: 337.5714\n",
      "Epoch 531/1500, Training Loss: 336.5499, Validation Loss: 337.5714\n",
      "Epoch 532/1500, Training Loss: 336.5499, Validation Loss: 337.5714\n",
      "Epoch 533/1500, Training Loss: 336.5498, Validation Loss: 337.5714\n",
      "Epoch 534/1500, Training Loss: 336.5498, Validation Loss: 337.5714\n",
      "Epoch 535/1500, Training Loss: 336.5498, Validation Loss: 337.5714\n",
      "Epoch 536/1500, Training Loss: 336.5497, Validation Loss: 337.5714\n",
      "Epoch 537/1500, Training Loss: 336.5497, Validation Loss: 337.5714\n",
      "Epoch 538/1500, Training Loss: 336.5497, Validation Loss: 337.5714\n",
      "Epoch 539/1500, Training Loss: 336.5497, Validation Loss: 337.5714\n",
      "Epoch 540/1500, Training Loss: 336.5496, Validation Loss: 337.5714\n",
      "Epoch 541/1500, Training Loss: 336.5496, Validation Loss: 337.5713\n",
      "Epoch 542/1500, Training Loss: 336.5496, Validation Loss: 337.5713\n",
      "Epoch 543/1500, Training Loss: 336.5496, Validation Loss: 337.5713\n",
      "Epoch 544/1500, Training Loss: 336.5496, Validation Loss: 337.5713\n",
      "Epoch 545/1500, Training Loss: 336.5495, Validation Loss: 337.5713\n",
      "Epoch 546/1500, Training Loss: 336.5495, Validation Loss: 337.5713\n",
      "Epoch 547/1500, Training Loss: 336.5495, Validation Loss: 337.5713\n",
      "Epoch 548/1500, Training Loss: 336.5494, Validation Loss: 337.5713\n",
      "Epoch 549/1500, Training Loss: 336.5494, Validation Loss: 337.5713\n",
      "Epoch 550/1500, Training Loss: 336.5494, Validation Loss: 337.5713\n",
      "Epoch 551/1500, Training Loss: 336.5494, Validation Loss: 337.5713\n",
      "Epoch 552/1500, Training Loss: 336.5494, Validation Loss: 337.5713\n",
      "Epoch 553/1500, Training Loss: 336.5493, Validation Loss: 337.5713\n",
      "Epoch 554/1500, Training Loss: 336.5493, Validation Loss: 337.5713\n",
      "Epoch 555/1500, Training Loss: 336.5493, Validation Loss: 337.5713\n",
      "Epoch 556/1500, Training Loss: 336.5493, Validation Loss: 337.5712\n",
      "Epoch 557/1500, Training Loss: 336.5492, Validation Loss: 337.5712\n",
      "Epoch 558/1500, Training Loss: 336.5492, Validation Loss: 337.5712\n",
      "Epoch 559/1500, Training Loss: 336.5492, Validation Loss: 337.5712\n",
      "Epoch 560/1500, Training Loss: 336.5492, Validation Loss: 337.5712\n",
      "Epoch 561/1500, Training Loss: 336.5492, Validation Loss: 337.5712\n",
      "Epoch 562/1500, Training Loss: 336.5491, Validation Loss: 337.5712\n",
      "Epoch 563/1500, Training Loss: 336.5491, Validation Loss: 337.5712\n",
      "Epoch 564/1500, Training Loss: 336.5491, Validation Loss: 337.5712\n",
      "Epoch 565/1500, Training Loss: 336.5491, Validation Loss: 337.5712\n",
      "Epoch 566/1500, Training Loss: 336.5490, Validation Loss: 337.5712\n",
      "Epoch 567/1500, Training Loss: 336.5490, Validation Loss: 337.5712\n",
      "Epoch 568/1500, Training Loss: 336.5490, Validation Loss: 337.5712\n",
      "Epoch 569/1500, Training Loss: 336.5490, Validation Loss: 337.5712\n",
      "Epoch 570/1500, Training Loss: 336.5489, Validation Loss: 337.5712\n",
      "Epoch 571/1500, Training Loss: 336.5489, Validation Loss: 337.5712\n",
      "Epoch 572/1500, Training Loss: 336.5489, Validation Loss: 337.5712\n",
      "Epoch 573/1500, Training Loss: 336.5489, Validation Loss: 337.5711\n",
      "Epoch 574/1500, Training Loss: 336.5489, Validation Loss: 337.5711\n",
      "Epoch 575/1500, Training Loss: 336.5488, Validation Loss: 337.5711\n",
      "Epoch 576/1500, Training Loss: 336.5488, Validation Loss: 337.5711\n",
      "Epoch 577/1500, Training Loss: 336.5488, Validation Loss: 337.5711\n",
      "Epoch 578/1500, Training Loss: 336.5488, Validation Loss: 337.5711\n",
      "Epoch 579/1500, Training Loss: 336.5487, Validation Loss: 337.5711\n",
      "Epoch 580/1500, Training Loss: 336.5487, Validation Loss: 337.5711\n",
      "Epoch 581/1500, Training Loss: 336.5487, Validation Loss: 337.5711\n",
      "Epoch 582/1500, Training Loss: 336.5487, Validation Loss: 337.5711\n",
      "Epoch 583/1500, Training Loss: 336.5487, Validation Loss: 337.5711\n",
      "Epoch 584/1500, Training Loss: 336.5486, Validation Loss: 337.5711\n",
      "Epoch 585/1500, Training Loss: 336.5486, Validation Loss: 337.5711\n",
      "Epoch 586/1500, Training Loss: 336.5486, Validation Loss: 337.5711\n",
      "Epoch 587/1500, Training Loss: 336.5486, Validation Loss: 337.5711\n",
      "Epoch 588/1500, Training Loss: 336.5486, Validation Loss: 337.5710\n",
      "Epoch 589/1500, Training Loss: 336.5486, Validation Loss: 337.5710\n",
      "Epoch 590/1500, Training Loss: 336.5485, Validation Loss: 337.5710\n",
      "Epoch 591/1500, Training Loss: 336.5485, Validation Loss: 337.5710\n",
      "Epoch 592/1500, Training Loss: 336.5485, Validation Loss: 337.5710\n",
      "Epoch 593/1500, Training Loss: 336.5484, Validation Loss: 337.5710\n",
      "Epoch 594/1500, Training Loss: 336.5484, Validation Loss: 337.5710\n",
      "Epoch 595/1500, Training Loss: 336.5484, Validation Loss: 337.5710\n",
      "Epoch 596/1500, Training Loss: 336.5484, Validation Loss: 337.5710\n",
      "Epoch 597/1500, Training Loss: 336.5484, Validation Loss: 337.5710\n",
      "Epoch 598/1500, Training Loss: 336.5484, Validation Loss: 337.5710\n",
      "Epoch 599/1500, Training Loss: 336.5483, Validation Loss: 337.5710\n",
      "Epoch 600/1500, Training Loss: 336.5483, Validation Loss: 337.5710\n",
      "Epoch 601/1500, Training Loss: 336.5483, Validation Loss: 337.5710\n",
      "Epoch 602/1500, Training Loss: 336.5483, Validation Loss: 337.5710\n",
      "Epoch 603/1500, Training Loss: 336.5482, Validation Loss: 337.5710\n",
      "Epoch 604/1500, Training Loss: 336.5482, Validation Loss: 337.5710\n",
      "Epoch 605/1500, Training Loss: 336.5482, Validation Loss: 337.5710\n",
      "Epoch 606/1500, Training Loss: 336.5482, Validation Loss: 337.5710\n",
      "Epoch 607/1500, Training Loss: 336.5482, Validation Loss: 337.5709\n",
      "Epoch 608/1500, Training Loss: 336.5482, Validation Loss: 337.5709\n",
      "Epoch 609/1500, Training Loss: 336.5482, Validation Loss: 337.5709\n",
      "Epoch 610/1500, Training Loss: 336.5482, Validation Loss: 337.5709\n",
      "Epoch 611/1500, Training Loss: 336.5481, Validation Loss: 337.5709\n",
      "Epoch 612/1500, Training Loss: 336.5481, Validation Loss: 337.5709\n",
      "Epoch 613/1500, Training Loss: 336.5481, Validation Loss: 337.5709\n",
      "Epoch 614/1500, Training Loss: 336.5481, Validation Loss: 337.5709\n",
      "Epoch 615/1500, Training Loss: 336.5481, Validation Loss: 337.5709\n",
      "Epoch 616/1500, Training Loss: 336.5480, Validation Loss: 337.5709\n",
      "Epoch 617/1500, Training Loss: 336.5480, Validation Loss: 337.5709\n",
      "Epoch 618/1500, Training Loss: 336.5480, Validation Loss: 337.5709\n",
      "Epoch 619/1500, Training Loss: 336.5480, Validation Loss: 337.5709\n",
      "Epoch 620/1500, Training Loss: 336.5479, Validation Loss: 337.5709\n",
      "Epoch 621/1500, Training Loss: 336.5479, Validation Loss: 337.5709\n",
      "Epoch 622/1500, Training Loss: 336.5479, Validation Loss: 337.5709\n",
      "Epoch 623/1500, Training Loss: 336.5479, Validation Loss: 337.5709\n",
      "Epoch 624/1500, Training Loss: 336.5479, Validation Loss: 337.5709\n",
      "Epoch 625/1500, Training Loss: 336.5479, Validation Loss: 337.5709\n",
      "Epoch 626/1500, Training Loss: 336.5479, Validation Loss: 337.5708\n",
      "Epoch 627/1500, Training Loss: 336.5478, Validation Loss: 337.5708\n",
      "Epoch 628/1500, Training Loss: 336.5478, Validation Loss: 337.5708\n",
      "Epoch 629/1500, Training Loss: 336.5478, Validation Loss: 337.5708\n",
      "Epoch 630/1500, Training Loss: 336.5478, Validation Loss: 337.5709\n",
      "Epoch 631/1500, Training Loss: 336.5478, Validation Loss: 337.5708\n",
      "Epoch 632/1500, Training Loss: 336.5478, Validation Loss: 337.5708\n",
      "Epoch 633/1500, Training Loss: 336.5478, Validation Loss: 337.5708\n",
      "Epoch 634/1500, Training Loss: 336.5477, Validation Loss: 337.5708\n",
      "Epoch 635/1500, Training Loss: 336.5477, Validation Loss: 337.5708\n",
      "Epoch 636/1500, Training Loss: 336.5477, Validation Loss: 337.5708\n",
      "Epoch 637/1500, Training Loss: 336.5477, Validation Loss: 337.5708\n",
      "Epoch 638/1500, Training Loss: 336.5477, Validation Loss: 337.5708\n",
      "Epoch 639/1500, Training Loss: 336.5477, Validation Loss: 337.5708\n",
      "Epoch 640/1500, Training Loss: 336.5477, Validation Loss: 337.5708\n",
      "Epoch 641/1500, Training Loss: 336.5477, Validation Loss: 337.5707\n",
      "Epoch 642/1500, Training Loss: 336.5476, Validation Loss: 337.5707\n",
      "Epoch 643/1500, Training Loss: 336.5476, Validation Loss: 337.5708\n",
      "Epoch 644/1500, Training Loss: 336.5476, Validation Loss: 337.5708\n",
      "Epoch 645/1500, Training Loss: 336.5476, Validation Loss: 337.5707\n",
      "Epoch 646/1500, Training Loss: 336.5476, Validation Loss: 337.5707\n",
      "Epoch 647/1500, Training Loss: 336.5475, Validation Loss: 337.5707\n",
      "Epoch 648/1500, Training Loss: 336.5475, Validation Loss: 337.5707\n",
      "Epoch 649/1500, Training Loss: 336.5475, Validation Loss: 337.5707\n",
      "Epoch 650/1500, Training Loss: 336.5475, Validation Loss: 337.5707\n",
      "Epoch 651/1500, Training Loss: 336.5475, Validation Loss: 337.5707\n",
      "Epoch 652/1500, Training Loss: 336.5475, Validation Loss: 337.5707\n",
      "Epoch 653/1500, Training Loss: 336.5475, Validation Loss: 337.5707\n",
      "Epoch 654/1500, Training Loss: 336.5475, Validation Loss: 337.5707\n",
      "Epoch 655/1500, Training Loss: 336.5474, Validation Loss: 337.5707\n",
      "Epoch 656/1500, Training Loss: 336.5474, Validation Loss: 337.5707\n",
      "Epoch 657/1500, Training Loss: 336.5474, Validation Loss: 337.5707\n",
      "Epoch 658/1500, Training Loss: 336.5474, Validation Loss: 337.5707\n",
      "Epoch 659/1500, Training Loss: 336.5474, Validation Loss: 337.5707\n",
      "Epoch 660/1500, Training Loss: 336.5474, Validation Loss: 337.5707\n",
      "Epoch 661/1500, Training Loss: 336.5473, Validation Loss: 337.5707\n",
      "Epoch 662/1500, Training Loss: 336.5473, Validation Loss: 337.5707\n",
      "Epoch 663/1500, Training Loss: 336.5473, Validation Loss: 337.5706\n",
      "Epoch 664/1500, Training Loss: 336.5473, Validation Loss: 337.5706\n",
      "Epoch 665/1500, Training Loss: 336.5473, Validation Loss: 337.5706\n",
      "Epoch 666/1500, Training Loss: 336.5473, Validation Loss: 337.5706\n",
      "Epoch 667/1500, Training Loss: 336.5473, Validation Loss: 337.5706\n",
      "Epoch 668/1500, Training Loss: 336.5472, Validation Loss: 337.5706\n",
      "Epoch 669/1500, Training Loss: 336.5472, Validation Loss: 337.5706\n",
      "Epoch 670/1500, Training Loss: 336.5472, Validation Loss: 337.5706\n",
      "Epoch 671/1500, Training Loss: 336.5472, Validation Loss: 337.5706\n",
      "Epoch 672/1500, Training Loss: 336.5472, Validation Loss: 337.5706\n",
      "Epoch 673/1500, Training Loss: 336.5472, Validation Loss: 337.5706\n",
      "Epoch 674/1500, Training Loss: 336.5471, Validation Loss: 337.5706\n",
      "Epoch 675/1500, Training Loss: 336.5471, Validation Loss: 337.5706\n",
      "Epoch 676/1500, Training Loss: 336.5471, Validation Loss: 337.5706\n",
      "Epoch 677/1500, Training Loss: 336.5471, Validation Loss: 337.5706\n",
      "Epoch 678/1500, Training Loss: 336.5471, Validation Loss: 337.5706\n",
      "Epoch 679/1500, Training Loss: 336.5471, Validation Loss: 337.5706\n",
      "Epoch 680/1500, Training Loss: 336.5471, Validation Loss: 337.5706\n",
      "Epoch 681/1500, Training Loss: 336.5471, Validation Loss: 337.5706\n",
      "Epoch 682/1500, Training Loss: 336.5471, Validation Loss: 337.5706\n",
      "Epoch 683/1500, Training Loss: 336.5471, Validation Loss: 337.5706\n",
      "Epoch 684/1500, Training Loss: 336.5471, Validation Loss: 337.5706\n",
      "Epoch 685/1500, Training Loss: 336.5470, Validation Loss: 337.5706\n",
      "Epoch 686/1500, Training Loss: 336.5470, Validation Loss: 337.5706\n",
      "Epoch 687/1500, Training Loss: 336.5470, Validation Loss: 337.5705\n",
      "Epoch 688/1500, Training Loss: 336.5470, Validation Loss: 337.5706\n",
      "Epoch 689/1500, Training Loss: 336.5470, Validation Loss: 337.5706\n",
      "Epoch 690/1500, Training Loss: 336.5470, Validation Loss: 337.5705\n",
      "Epoch 691/1500, Training Loss: 336.5469, Validation Loss: 337.5705\n",
      "Epoch 692/1500, Training Loss: 336.5469, Validation Loss: 337.5705\n",
      "Epoch 693/1500, Training Loss: 336.5469, Validation Loss: 337.5705\n",
      "Epoch 694/1500, Training Loss: 336.5469, Validation Loss: 337.5705\n",
      "Epoch 695/1500, Training Loss: 336.5469, Validation Loss: 337.5705\n",
      "Epoch 696/1500, Training Loss: 336.5469, Validation Loss: 337.5705\n",
      "Epoch 697/1500, Training Loss: 336.5469, Validation Loss: 337.5705\n",
      "Epoch 698/1500, Training Loss: 336.5469, Validation Loss: 337.5705\n",
      "Epoch 699/1500, Training Loss: 336.5469, Validation Loss: 337.5705\n",
      "Epoch 700/1500, Training Loss: 336.5468, Validation Loss: 337.5705\n",
      "Epoch 701/1500, Training Loss: 336.5468, Validation Loss: 337.5705\n",
      "Epoch 702/1500, Training Loss: 336.5468, Validation Loss: 337.5705\n",
      "Epoch 703/1500, Training Loss: 336.5468, Validation Loss: 337.5705\n",
      "Epoch 704/1500, Training Loss: 336.5468, Validation Loss: 337.5705\n",
      "Epoch 705/1500, Training Loss: 336.5468, Validation Loss: 337.5705\n",
      "Epoch 706/1500, Training Loss: 336.5468, Validation Loss: 337.5705\n",
      "Epoch 707/1500, Training Loss: 336.5468, Validation Loss: 337.5705\n",
      "Epoch 708/1500, Training Loss: 336.5468, Validation Loss: 337.5704\n",
      "Epoch 709/1500, Training Loss: 336.5468, Validation Loss: 337.5704\n",
      "Epoch 710/1500, Training Loss: 336.5467, Validation Loss: 337.5704\n",
      "Epoch 711/1500, Training Loss: 336.5467, Validation Loss: 337.5704\n",
      "Epoch 712/1500, Training Loss: 336.5467, Validation Loss: 337.5704\n",
      "Epoch 713/1500, Training Loss: 336.5467, Validation Loss: 337.5704\n",
      "Epoch 714/1500, Training Loss: 336.5467, Validation Loss: 337.5704\n",
      "Epoch 715/1500, Training Loss: 336.5467, Validation Loss: 337.5704\n",
      "Epoch 716/1500, Training Loss: 336.5467, Validation Loss: 337.5704\n",
      "Epoch 717/1500, Training Loss: 336.5466, Validation Loss: 337.5704\n",
      "Epoch 718/1500, Training Loss: 336.5466, Validation Loss: 337.5704\n",
      "Epoch 719/1500, Training Loss: 336.5466, Validation Loss: 337.5704\n",
      "Epoch 720/1500, Training Loss: 336.5466, Validation Loss: 337.5704\n",
      "Epoch 721/1500, Training Loss: 336.5466, Validation Loss: 337.5704\n",
      "Epoch 722/1500, Training Loss: 336.5466, Validation Loss: 337.5704\n",
      "Epoch 723/1500, Training Loss: 336.5466, Validation Loss: 337.5704\n",
      "Epoch 724/1500, Training Loss: 336.5466, Validation Loss: 337.5704\n",
      "Epoch 725/1500, Training Loss: 336.5466, Validation Loss: 337.5704\n",
      "Epoch 726/1500, Training Loss: 336.5465, Validation Loss: 337.5704\n",
      "Epoch 727/1500, Training Loss: 336.5465, Validation Loss: 337.5704\n",
      "Epoch 728/1500, Training Loss: 336.5465, Validation Loss: 337.5704\n",
      "Epoch 729/1500, Training Loss: 336.5465, Validation Loss: 337.5704\n",
      "Epoch 730/1500, Training Loss: 336.5465, Validation Loss: 337.5704\n",
      "Epoch 731/1500, Training Loss: 336.5465, Validation Loss: 337.5704\n",
      "Epoch 732/1500, Training Loss: 336.5465, Validation Loss: 337.5704\n",
      "Epoch 733/1500, Training Loss: 336.5465, Validation Loss: 337.5704\n",
      "Epoch 734/1500, Training Loss: 336.5465, Validation Loss: 337.5703\n",
      "Epoch 735/1500, Training Loss: 336.5464, Validation Loss: 337.5704\n",
      "Epoch 736/1500, Training Loss: 336.5464, Validation Loss: 337.5704\n",
      "Epoch 737/1500, Training Loss: 336.5464, Validation Loss: 337.5704\n",
      "Epoch 738/1500, Training Loss: 336.5464, Validation Loss: 337.5703\n",
      "Epoch 739/1500, Training Loss: 336.5464, Validation Loss: 337.5703\n",
      "Epoch 740/1500, Training Loss: 336.5464, Validation Loss: 337.5703\n",
      "Epoch 741/1500, Training Loss: 336.5464, Validation Loss: 337.5703\n",
      "Epoch 742/1500, Training Loss: 336.5464, Validation Loss: 337.5703\n",
      "Epoch 743/1500, Training Loss: 336.5464, Validation Loss: 337.5703\n",
      "Epoch 744/1500, Training Loss: 336.5464, Validation Loss: 337.5703\n",
      "Epoch 745/1500, Training Loss: 336.5464, Validation Loss: 337.5703\n",
      "Epoch 746/1500, Training Loss: 336.5463, Validation Loss: 337.5703\n",
      "Epoch 747/1500, Training Loss: 336.5463, Validation Loss: 337.5703\n",
      "Epoch 748/1500, Training Loss: 336.5463, Validation Loss: 337.5703\n",
      "Epoch 749/1500, Training Loss: 336.5463, Validation Loss: 337.5703\n",
      "Epoch 750/1500, Training Loss: 336.5463, Validation Loss: 337.5703\n",
      "Epoch 751/1500, Training Loss: 336.5463, Validation Loss: 337.5703\n",
      "Epoch 752/1500, Training Loss: 336.5463, Validation Loss: 337.5703\n",
      "Epoch 753/1500, Training Loss: 336.5463, Validation Loss: 337.5703\n",
      "Epoch 754/1500, Training Loss: 336.5463, Validation Loss: 337.5703\n",
      "Epoch 755/1500, Training Loss: 336.5463, Validation Loss: 337.5703\n",
      "Epoch 756/1500, Training Loss: 336.5463, Validation Loss: 337.5703\n",
      "Epoch 757/1500, Training Loss: 336.5462, Validation Loss: 337.5703\n",
      "Epoch 758/1500, Training Loss: 336.5462, Validation Loss: 337.5703\n",
      "Epoch 759/1500, Training Loss: 336.5462, Validation Loss: 337.5703\n",
      "Epoch 760/1500, Training Loss: 336.5462, Validation Loss: 337.5703\n",
      "Epoch 761/1500, Training Loss: 336.5462, Validation Loss: 337.5703\n",
      "Epoch 762/1500, Training Loss: 336.5462, Validation Loss: 337.5703\n",
      "Epoch 763/1500, Training Loss: 336.5462, Validation Loss: 337.5703\n",
      "Epoch 764/1500, Training Loss: 336.5462, Validation Loss: 337.5703\n",
      "Epoch 765/1500, Training Loss: 336.5462, Validation Loss: 337.5703\n",
      "Epoch 766/1500, Training Loss: 336.5462, Validation Loss: 337.5703\n",
      "Epoch 767/1500, Training Loss: 336.5462, Validation Loss: 337.5703\n",
      "Epoch 768/1500, Training Loss: 336.5461, Validation Loss: 337.5703\n",
      "Epoch 769/1500, Training Loss: 336.5461, Validation Loss: 337.5703\n",
      "Epoch 770/1500, Training Loss: 336.5461, Validation Loss: 337.5703\n",
      "Epoch 771/1500, Training Loss: 336.5461, Validation Loss: 337.5703\n",
      "Epoch 772/1500, Training Loss: 336.5461, Validation Loss: 337.5702\n",
      "Epoch 773/1500, Training Loss: 336.5461, Validation Loss: 337.5702\n",
      "Epoch 774/1500, Training Loss: 336.5461, Validation Loss: 337.5702\n",
      "Epoch 775/1500, Training Loss: 336.5461, Validation Loss: 337.5702\n",
      "Epoch 776/1500, Training Loss: 336.5461, Validation Loss: 337.5702\n",
      "Epoch 777/1500, Training Loss: 336.5461, Validation Loss: 337.5702\n",
      "Epoch 778/1500, Training Loss: 336.5461, Validation Loss: 337.5702\n",
      "Epoch 779/1500, Training Loss: 336.5461, Validation Loss: 337.5702\n",
      "Epoch 780/1500, Training Loss: 336.5461, Validation Loss: 337.5702\n",
      "Epoch 781/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 782/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 783/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 784/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 785/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 786/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 787/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 788/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 789/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 790/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 791/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 792/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 793/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 794/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 795/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 796/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 797/1500, Training Loss: 336.5460, Validation Loss: 337.5702\n",
      "Epoch 798/1500, Training Loss: 336.5459, Validation Loss: 337.5702\n",
      "Epoch 799/1500, Training Loss: 336.5459, Validation Loss: 337.5702\n",
      "Epoch 800/1500, Training Loss: 336.5459, Validation Loss: 337.5702\n",
      "Epoch 801/1500, Training Loss: 336.5459, Validation Loss: 337.5701\n",
      "Epoch 802/1500, Training Loss: 336.5459, Validation Loss: 337.5701\n",
      "Epoch 803/1500, Training Loss: 336.5459, Validation Loss: 337.5701\n",
      "Epoch 804/1500, Training Loss: 336.5459, Validation Loss: 337.5701\n",
      "Epoch 805/1500, Training Loss: 336.5459, Validation Loss: 337.5701\n",
      "Epoch 806/1500, Training Loss: 336.5459, Validation Loss: 337.5701\n",
      "Epoch 807/1500, Training Loss: 336.5459, Validation Loss: 337.5701\n",
      "Epoch 808/1500, Training Loss: 336.5459, Validation Loss: 337.5701\n",
      "Epoch 809/1500, Training Loss: 336.5459, Validation Loss: 337.5701\n",
      "Epoch 810/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 811/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 812/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 813/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 814/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 815/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 816/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 817/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 818/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 819/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 820/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 821/1500, Training Loss: 336.5458, Validation Loss: 337.5701\n",
      "Epoch 822/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 823/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 824/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 825/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 826/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 827/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 828/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 829/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 830/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 831/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 832/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 833/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 834/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 835/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 836/1500, Training Loss: 336.5457, Validation Loss: 337.5700\n",
      "Epoch 837/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 838/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 839/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 840/1500, Training Loss: 336.5457, Validation Loss: 337.5701\n",
      "Epoch 841/1500, Training Loss: 336.5456, Validation Loss: 337.5701\n",
      "Epoch 842/1500, Training Loss: 336.5456, Validation Loss: 337.5701\n",
      "Epoch 843/1500, Training Loss: 336.5456, Validation Loss: 337.5701\n",
      "Epoch 844/1500, Training Loss: 336.5456, Validation Loss: 337.5700\n",
      "Epoch 845/1500, Training Loss: 336.5456, Validation Loss: 337.5701\n",
      "Epoch 846/1500, Training Loss: 336.5456, Validation Loss: 337.5701\n",
      "Epoch 847/1500, Training Loss: 336.5456, Validation Loss: 337.5701\n",
      "Epoch 848/1500, Training Loss: 336.5456, Validation Loss: 337.5700\n",
      "Epoch 849/1500, Training Loss: 336.5456, Validation Loss: 337.5701\n",
      "Epoch 850/1500, Training Loss: 336.5456, Validation Loss: 337.5700\n",
      "Epoch 851/1500, Training Loss: 336.5456, Validation Loss: 337.5700\n",
      "Epoch 852/1500, Training Loss: 336.5456, Validation Loss: 337.5700\n",
      "Epoch 853/1500, Training Loss: 336.5456, Validation Loss: 337.5700\n",
      "Epoch 854/1500, Training Loss: 336.5456, Validation Loss: 337.5700\n",
      "Epoch 855/1500, Training Loss: 336.5456, Validation Loss: 337.5700\n",
      "Epoch 856/1500, Training Loss: 336.5456, Validation Loss: 337.5700\n",
      "Epoch 857/1500, Training Loss: 336.5456, Validation Loss: 337.5700\n",
      "Epoch 858/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 859/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 860/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 861/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 862/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 863/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 864/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 865/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 866/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 867/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 868/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 869/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 870/1500, Training Loss: 336.5455, Validation Loss: 337.5700\n",
      "Epoch 871/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 872/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 873/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 874/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 875/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 876/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 877/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 878/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 879/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 880/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 881/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 882/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 883/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 884/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 885/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 886/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 887/1500, Training Loss: 336.5454, Validation Loss: 337.5700\n",
      "Epoch 888/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 889/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 890/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 891/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 892/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 893/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 894/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 895/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 896/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 897/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 898/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 899/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 900/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 901/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 902/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 903/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 904/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 905/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 906/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 907/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 908/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 909/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 910/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 911/1500, Training Loss: 336.5453, Validation Loss: 337.5700\n",
      "Epoch 912/1500, Training Loss: 336.5453, Validation Loss: 337.5699\n",
      "Epoch 913/1500, Training Loss: 336.5452, Validation Loss: 337.5699\n",
      "Epoch 914/1500, Training Loss: 336.5452, Validation Loss: 337.5700\n",
      "Epoch 915/1500, Training Loss: 336.5452, Validation Loss: 337.5700\n",
      "Epoch 916/1500, Training Loss: 336.5452, Validation Loss: 337.5700\n",
      "Epoch 917/1500, Training Loss: 336.5452, Validation Loss: 337.5700\n",
      "Epoch 918/1500, Training Loss: 336.5452, Validation Loss: 337.5700\n",
      "Epoch 919/1500, Training Loss: 336.5452, Validation Loss: 337.5700\n",
      "Epoch 920/1500, Training Loss: 336.5452, Validation Loss: 337.5699\n",
      "Epoch 921/1500, Training Loss: 336.5452, Validation Loss: 337.5699\n",
      "Epoch 922/1500, Training Loss: 336.5452, Validation Loss: 337.5699\n",
      "Epoch 923/1500, Training Loss: 336.5452, Validation Loss: 337.5699\n",
      "Epoch 924/1500, Training Loss: 336.5452, Validation Loss: 337.5699\n",
      "Epoch 925/1500, Training Loss: 336.5452, Validation Loss: 337.5699\n",
      "Epoch 926/1500, Training Loss: 336.5452, Validation Loss: 337.5699\n",
      "Epoch 927/1500, Training Loss: 336.5452, Validation Loss: 337.5699\n",
      "Epoch 928/1500, Training Loss: 336.5452, Validation Loss: 337.5699\n",
      "Epoch 929/1500, Training Loss: 336.5452, Validation Loss: 337.5699\n",
      "Epoch 930/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 931/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 932/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 933/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 934/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 935/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 936/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 937/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 938/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 939/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 940/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 941/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 942/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 943/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 944/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 945/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 946/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 947/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 948/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 949/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 950/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 951/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 952/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 953/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 954/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 955/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 956/1500, Training Loss: 336.5451, Validation Loss: 337.5699\n",
      "Epoch 957/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 958/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 959/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 960/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 961/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 962/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 963/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 964/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 965/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 966/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 967/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 968/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 969/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 970/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 971/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 972/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 973/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 974/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 975/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 976/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 977/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 978/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 979/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 980/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 981/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 982/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 983/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 984/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 985/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 986/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 987/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 988/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 989/1500, Training Loss: 336.5450, Validation Loss: 337.5699\n",
      "Epoch 990/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 991/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 992/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 993/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 994/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 995/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 996/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 997/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 998/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 999/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1000/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1001/1500, Training Loss: 336.5449, Validation Loss: 337.5698\n",
      "Epoch 1002/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1003/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1004/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1005/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1006/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1007/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1008/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1009/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1010/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1011/1500, Training Loss: 336.5449, Validation Loss: 337.5698\n",
      "Epoch 1012/1500, Training Loss: 336.5449, Validation Loss: 337.5698\n",
      "Epoch 1013/1500, Training Loss: 336.5449, Validation Loss: 337.5698\n",
      "Epoch 1014/1500, Training Loss: 336.5449, Validation Loss: 337.5698\n",
      "Epoch 1015/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1016/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1017/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1018/1500, Training Loss: 336.5449, Validation Loss: 337.5699\n",
      "Epoch 1019/1500, Training Loss: 336.5448, Validation Loss: 337.5699\n",
      "Epoch 1020/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1021/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1022/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1023/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1024/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1025/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1026/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1027/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1028/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1029/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1030/1500, Training Loss: 336.5448, Validation Loss: 337.5699\n",
      "Epoch 1031/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1032/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1033/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1034/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1035/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1036/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1037/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1038/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1039/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1040/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1041/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1042/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1043/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1044/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1045/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1046/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1047/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1048/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1049/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1050/1500, Training Loss: 336.5448, Validation Loss: 337.5698\n",
      "Epoch 1051/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1052/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1053/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1054/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1055/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1056/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1057/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1058/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1059/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1060/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1061/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1062/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1063/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1064/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1065/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1066/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1067/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1068/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1069/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1070/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1071/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1072/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1073/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1074/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1075/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1076/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1077/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1078/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1079/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1080/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1081/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1082/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1083/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1084/1500, Training Loss: 336.5447, Validation Loss: 337.5698\n",
      "Epoch 1085/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1086/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1087/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1088/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1089/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1090/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1091/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1092/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1093/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1094/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1095/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1096/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1097/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1098/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1099/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1100/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1101/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1102/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1103/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1104/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1105/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1106/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1107/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1108/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1109/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1110/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1111/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1112/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1113/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1114/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1115/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1116/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1117/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1118/1500, Training Loss: 336.5446, Validation Loss: 337.5698\n",
      "Epoch 1119/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1120/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1121/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1122/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1123/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1124/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1125/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1126/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1127/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1128/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1129/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1130/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1131/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1132/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1133/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1134/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1135/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1136/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1137/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1138/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1139/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1140/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1141/1500, Training Loss: 336.5446, Validation Loss: 337.5697\n",
      "Epoch 1142/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1143/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1144/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1145/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1146/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1147/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1148/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1149/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1150/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1151/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1152/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1153/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1154/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1155/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1156/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1157/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1158/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1159/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1160/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1161/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1162/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1163/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1164/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1165/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1166/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1167/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1168/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1169/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1170/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1171/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1172/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1173/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1174/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1175/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1176/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1177/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1178/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1179/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1180/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1181/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1182/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1183/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1184/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1185/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1186/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1187/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1188/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1189/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1190/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1191/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1192/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1193/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1194/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1195/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1196/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1197/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1198/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1199/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1200/1500, Training Loss: 336.5445, Validation Loss: 337.5697\n",
      "Epoch 1201/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1202/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1203/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1204/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1205/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1206/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1207/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1208/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1209/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1210/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1211/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1212/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1213/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1214/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1215/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1216/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1217/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1218/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1219/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1220/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1221/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1222/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1223/1500, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1224/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1225/1500, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1226/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1227/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1228/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1229/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1230/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1231/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1232/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1233/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1234/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1235/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1236/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1237/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1238/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1239/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1240/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1241/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1242/1500, Training Loss: 336.5444, Validation Loss: 337.5697\n",
      "Epoch 1243/1500, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1244/1500, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1245/1500, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1246/1500, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1247/1500, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1248/1500, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1249/1500, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1250/1500, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1251/1500, Training Loss: 336.5444, Validation Loss: 337.5696\n",
      "Epoch 1252/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1253/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1254/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1255/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1256/1500, Training Loss: 336.5443, Validation Loss: 337.5697\n",
      "Epoch 1257/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1258/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1259/1500, Training Loss: 336.5443, Validation Loss: 337.5697\n",
      "Epoch 1260/1500, Training Loss: 336.5443, Validation Loss: 337.5697\n",
      "Epoch 1261/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1262/1500, Training Loss: 336.5443, Validation Loss: 337.5697\n",
      "Epoch 1263/1500, Training Loss: 336.5443, Validation Loss: 337.5697\n",
      "Epoch 1264/1500, Training Loss: 336.5443, Validation Loss: 337.5697\n",
      "Epoch 1265/1500, Training Loss: 336.5443, Validation Loss: 337.5697\n",
      "Epoch 1266/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1267/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1268/1500, Training Loss: 336.5443, Validation Loss: 337.5697\n",
      "Epoch 1269/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1270/1500, Training Loss: 336.5443, Validation Loss: 337.5697\n",
      "Epoch 1271/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1272/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1273/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1274/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1275/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1276/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1277/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1278/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1279/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1280/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1281/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1282/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1283/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1284/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1285/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1286/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1287/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1288/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1289/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1290/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1291/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1292/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1293/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1294/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1295/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1296/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1297/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1298/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1299/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1300/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1301/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1302/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1303/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1304/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1305/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1306/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1307/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1308/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1309/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1310/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1311/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1312/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1313/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1314/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1315/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1316/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1317/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1318/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1319/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1320/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1321/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1322/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1323/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1324/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1325/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1326/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1327/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1328/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1329/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1330/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1331/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1332/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1333/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1334/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1335/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1336/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1337/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1338/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1339/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1340/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1341/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1342/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1343/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1344/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1345/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1346/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1347/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1348/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1349/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1350/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1351/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1352/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1353/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1354/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1355/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1356/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1357/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1358/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1359/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1360/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1361/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1362/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1363/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1364/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1365/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1366/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1367/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1368/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1369/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1370/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1371/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1372/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1373/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1374/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1375/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1376/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1377/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1378/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1379/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1380/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1381/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1382/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1383/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1384/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1385/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1386/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1387/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1388/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1389/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1390/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1391/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1392/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1393/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1394/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1395/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1396/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1397/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1398/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1399/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1400/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1401/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1402/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1403/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1404/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1405/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1406/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1407/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1408/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1409/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1410/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1411/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1412/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1413/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1414/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1415/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1416/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1417/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1418/1500, Training Loss: 336.5443, Validation Loss: 337.5696\n",
      "Epoch 1419/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1420/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1421/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1422/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1423/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1424/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1425/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1426/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1427/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1428/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1429/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1430/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1431/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1432/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1433/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1434/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1435/1500, Training Loss: 336.5443, Validation Loss: 337.5695\n",
      "Epoch 1436/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1437/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1438/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1439/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1440/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1441/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1442/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1443/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1444/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1445/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1446/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1447/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1448/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1449/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1450/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1451/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1452/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1453/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1454/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1455/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1456/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1457/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1458/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1459/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1460/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1461/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1462/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1463/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1464/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1465/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1466/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1467/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1468/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1469/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1470/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1471/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1472/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1473/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1474/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1475/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1476/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1477/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1478/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1479/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1480/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1481/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1482/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1483/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1484/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1485/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1486/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1487/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1488/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1489/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1490/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1491/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1492/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1493/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1494/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1495/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1496/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1497/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1498/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1499/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n",
      "Epoch 1500/1500, Training Loss: 336.5442, Validation Loss: 337.5695\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.25%\n",
      "Mean Percent Accuracy (Yield): 89.19%\n",
      "Median Percent Accuracy (Days to Harvest): 79.59%\n",
      "Median Percent Accuracy (Yield): 92.56%\n",
      "Accuracy Within 10% (Days to Harvest): 23.72%\n",
      "Accuracy Within 10% (Yield): 62.48%\n",
      "MAE (Days to Harvest): 22.45\n",
      "MAE (Yield): 0.40\n",
      "RMSE (Days to Harvest): 25.94\n",
      "RMSE (Yield): 0.50\n",
      "\n",
      "Validation Metrics:\n",
      "Mean Percent Accuracy (Days to Harvest): 76.17%\n",
      "Mean Percent Accuracy (Yield): 88.99%\n",
      "Median Percent Accuracy (Days to Harvest): 79.54%\n",
      "Median Percent Accuracy (Yield): 92.52%\n",
      "Accuracy Within 10% (Days to Harvest): 23.60%\n",
      "Accuracy Within 10% (Yield): 62.36%\n",
      "MAE (Days to Harvest): 22.49\n",
      "MAE (Yield): 0.40\n",
      "RMSE (Days to Harvest): 25.98\n",
      "RMSE (Yield): 0.50\n",
      "\n",
      "Training completed in 2.72 seconds\n",
      "Final Training Loss: 336.5442\n",
      "Final Validation Loss: 337.5695\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y.values, test_size=0.2, random_state=88)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Move data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor, X_val_tensor = X_train_tensor.to(device), X_val_tensor.to(device)\n",
    "y_train_tensor, y_val_tensor = y_train_tensor.to(device), y_val_tensor.to(device)\n",
    "\n",
    "# Define Linear Regression model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 2)  # Two outputs: Days to Harvest and Yield\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = LinearRegressionModel(input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 1500\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_train_tensor)\n",
    "    train_loss = criterion(predictions, y_train_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_val_tensor)\n",
    "        val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "    # Store losses\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Final training and validation loss\n",
    "final_train_loss = train_losses[-1]\n",
    "final_val_loss = val_losses[-1]\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate metrics\n",
    "def evaluate_model_with_metrics(y_actual, y_pred, tolerance=0.1):\n",
    "    # Percent accuracy\n",
    "    percent_accuracies = 100 * (1 - np.abs(y_actual - y_pred) / y_actual)\n",
    "    mean_percent_accuracy = np.mean(percent_accuracies, axis=0)\n",
    "    median_percent_accuracy = np.median(percent_accuracies, axis=0)\n",
    "\n",
    "    # Accuracy within tolerance\n",
    "    within_tolerance = np.abs(y_actual - y_pred) <= (tolerance * y_actual)\n",
    "    percent_within_tolerance = np.mean(within_tolerance, axis=0) * 100\n",
    "\n",
    "    # MAE\n",
    "    mae = np.mean(np.abs(y_actual - y_pred), axis=0)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(np.mean((y_actual - y_pred) ** 2, axis=0))\n",
    "\n",
    "    return {\n",
    "        \"mean_percent_accuracy\": mean_percent_accuracy,\n",
    "        \"median_percent_accuracy\": median_percent_accuracy,\n",
    "        \"percent_within_tolerance\": percent_within_tolerance,\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "    }\n",
    "\n",
    "# Evaluate metrics\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_preds = model(X_train_tensor).cpu().numpy()\n",
    "    val_preds = model(X_val_tensor).cpu().numpy()\n",
    "    y_train_np = y_train_tensor.cpu().numpy()\n",
    "    y_val_np = y_val_tensor.cpu().numpy()\n",
    "\n",
    "    # Training metrics\n",
    "    train_metrics = evaluate_model_with_metrics(y_train_np, train_preds, tolerance=0.1)\n",
    "    \n",
    "    # Validation metrics\n",
    "    val_metrics = evaluate_model_with_metrics(y_val_np, val_preds, tolerance=0.1)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {train_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {train_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {train_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {train_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {train_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {train_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {train_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {train_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {train_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {train_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"Mean Percent Accuracy (Days to Harvest): {val_metrics['mean_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Mean Percent Accuracy (Yield): {val_metrics['mean_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Days to Harvest): {val_metrics['median_percent_accuracy'][0]:.2f}%\")\n",
    "print(f\"Median Percent Accuracy (Yield): {val_metrics['median_percent_accuracy'][1]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Days to Harvest): {val_metrics['percent_within_tolerance'][0]:.2f}%\")\n",
    "print(f\"Accuracy Within 10% (Yield): {val_metrics['percent_within_tolerance'][1]:.2f}%\")\n",
    "print(f\"MAE (Days to Harvest): {val_metrics['mae'][0]:.2f}\")\n",
    "print(f\"MAE (Yield): {val_metrics['mae'][1]:.2f}\")\n",
    "print(f\"RMSE (Days to Harvest): {val_metrics['rmse'][0]:.2f}\")\n",
    "print(f\"RMSE (Yield): {val_metrics['rmse'][1]:.2f}\")\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
